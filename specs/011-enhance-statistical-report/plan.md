# Implementation Plan: Enhanced Statistical Report Generation

**Branch**: `011-enhance-statistical-report` | **Date**: 2025-10-29 | **Spec**: [spec.md](spec.md)
**Input**: Feature specification from `/specs/011-enhance-statistical-report/spec.md`

**Note**: This plan was generated by the `/speckit.plan` command following the workflow in `.github/prompts/speckit.plan.prompt.md`.

## Summary

Enhance the existing `statistical_report.md` generation in `ExperimentAnalyzer` to include comprehensive statistical analysis with normality tests, effect sizes, power analysis, and publication-quality visualizations. The enhanced reports will use didactic "What/Why/How" explanations to make statistics accessible to researchers at all expertise levels. Statistical artifacts will be actively integrated into the paper generation pipeline, embedding visualizations and statistical content into the paper's Methodology, Results, and Discussion sections. This transforms the paper from basic comparative analysis to comprehensive statistical evaluation suitable for peer-reviewed journals.

## Technical Context

**Language/Version**: Python 3.11+ (matching existing genai-devbench codebase)  
**Primary Dependencies**: 
- Core: PyYAML 6.0.1, requests 2.31.0, pytest 7.4.3 (existing)
- **NEW**: scipy ≥1.9.0 (statistical tests), statsmodels ≥0.14.0 (power analysis), seaborn ≥0.12.0 (visualizations), numpy ≥1.24.0 (numerical operations)
- Visualization: matplotlib ≥3.8.0 (already present, will be used for plots)

**Storage**: File-based (YAML configs, JSON run data, Markdown reports, SVG/PNG figures)  
**Testing**: pytest 7.4.3 with unit tests for statistical methods, integration tests for report generation  
**Target Platform**: Linux/macOS development environments, CI/CD pipelines  
**Project Type**: Single project (monolithic Python codebase with src/ structure)  
**Performance Goals**: 
- Generate complete statistical analysis in <60 seconds for typical experiments (n=10 runs, 3 frameworks, 10 metrics)
- Bootstrap resampling (10,000 iterations) should complete in <2 seconds per metric-framework combination
- Visualization generation should add <5 seconds to total analysis time

**Constraints**: 
- Must not interfere with existing paper generation pipeline (`FigureExporter`, comparative charts)
- Educational content must be at 8th grade reading level while maintaining scientific accuracy
- Reports must be markdown-compatible with embedded SVG images
- **CRITICAL**: Must work with existing experiment at `~/projects/uece/baes/baes_benchmarking_20251028_0713`
- **Testing requirement**: All development tested against real experiment, output to `tmp/test_paper2/`
- **No unit tests**: Validation done by generating reports from target experiment only

**Scale/Scope**: 
- Handle 2-50 runs per framework (typical experimental scale)
- Support 2-10 frameworks per experiment
- Analyze 5-20 metrics per run
- Generate 4+ visualization types (box plots, violin plots, forest plots, Q-Q plots)
- Produce dual reports (summary ~300 lines, full ~800-1200 lines)

## Constitution Check

*GATE: Must pass before Phase 0 research. Re-check after Phase 1 design.*

Verify compliance with BAEs Experiment Constitution v1.2.0:

- [x] **Scientific Reproducibility**: ✅ Fixed random seeds for bootstrap resampling (numpy.random.seed), deterministic statistical calculations
- [x] **Clarity & Transparency**: ✅ All statistical methods documented with "What/Why/How" explanations, formulas explained in code comments
- [x] **Open Science**: ✅ No proprietary dependencies (scipy, statsmodels, matplotlib all open-source), reports published with experiments
- [x] **Minimal Dependencies**: ⚠️ **VIOLATION** - Adding scipy, statsmodels, seaborn, numpy beyond standard PyYAML/requests/pytest
  - **Justification**: Statistical analysis requires specialized libraries (scipy for tests, statsmodels for power analysis). These are industry-standard open-source tools with stable APIs. Alternative (implementing statistical tests from scratch) would introduce bugs and reduce scientific validity.
- [x] **Deterministic HITL**: ✅ N/A - Statistical analysis is fully automated, no human-in-the-loop required
- [x] **Reproducible Metrics**: ✅ Statistical tests produce deterministic results given same input data, bootstrap uses fixed seeds
- [x] **Version Control Integrity**: ✅ Dependency versions pinned in requirements.txt (scipy==1.11.0, statsmodels==0.14.0, etc.)
- [x] **Automation-First**: ✅ All statistical analysis automated in `ExperimentAnalyzer.analyze()`, no manual steps
- [x] **Failure Isolation**: ✅ Each analysis run operates on isolated experiment directory, temporary visualization files in run-specific `figures/statistical/`
- [x] **Educational Accessibility**: ✅ **CORE FEATURE** - Didactic explanations at 8th grade reading level, practical analogies, Quick Start Guide
- [x] **DRY Principle**: ✅ Helper methods for reusable logic (`_format_didactic_explanation()`, `_generate_analogy()`), base statistical test selection logic shared
- [x] **No Backward Compatibility Burden**: ⚠️ **SPECIAL CASE** - This feature must work with existing experiment at `~/projects/uece/baes/baes_benchmarking_20251028_0713`. No graceful fallback - implementation must handle real-world data correctly from day one.
- [x] **Fail-Fast Philosophy**: ✅ Invalid data (zero variance, n<2) raises explicit errors/warnings, no silent fallbacks that mask issues

## Project Structure

### Documentation (this feature)

```
specs/011-enhance-statistical-report/
├── plan.md              # This file (/speckit.plan command output)
├── research.md          # Phase 0 output (best practices for scipy, statsmodels, matplotlib)
├── data-model.md        # Phase 1 output (StatisticalFindings, MetricDistribution, etc.)
├── quickstart.md        # Phase 1 output (how to use enhanced reports)
├── contracts/           # Phase 1 output (statistical method signatures, report schemas)
├── checklists/          # Existing validation checklists
│   └── requirements.md  # Requirements checklist (already complete)
├── spec.md              # Feature specification (already complete)
└── full-report-sample.md # Example comprehensive report (reference material)
```

### Source Code (repository root)

```
src/
├── paper_generation/
│   ├── experiment_analyzer.py           # MODIFIED: Enhanced with statistical analysis methods
│   ├── paper_generator.py               # MODIFIED: Integration with statistical reports
│   ├── figure_exporter.py               # UNCHANGED: Existing comparative charts
│   ├── statistical_analyzer.py          # NEW: Core statistical analysis logic
│   ├── educational_content.py           # NEW: Didactic explanation generators
│   ├── statistical_visualizations.py    # NEW: Box plots, violin plots, forest plots, Q-Q plots
│   └── exceptions.py                    # MODIFIED: Add StatisticalAnalysisError
└── utils/
    └── statistical_helpers.py           # NEW: Shared utilities for effect sizes, power analysis

# NO TEST FILES - Testing done via real experiment generation only
```

**Target Experiment** (for validation):
```
~/projects/uece/baes/baes_benchmarking_20251028_0713/
├── runs/                                # Existing run data
├── config.yaml                          # Existing configuration
└── ...                                  # Other experiment files
```

**Test Output Directory**:
```
tmp/test_paper2/
├── statistical_report_summary.md        # NEW: Generated during development
├── statistical_report_full.md           # NEW: Generated during development
├── paper.md                             # MODIFIED: With statistical content
└── figures/
    └── statistical/                     # NEW: Statistical visualizations
        ├── *_boxplot.svg
        ├── *_violinplot.svg
        ├── effect_sizes_forestplot.svg
        └── *_*_qqplot.svg
```

**Structure Decision**: Single project structure (Option 1) is appropriate. This is an enhancement to existing paper generation components, not a new independent service. All statistical analysis logic is added to the existing `src/paper_generation/` module with new helper classes/functions. 

**Testing Strategy**: No traditional unit/integration tests. All validation done by running the enhanced analyzer against the real experiment at `~/projects/uece/baes/baes_benchmarking_20251028_0713` and inspecting generated reports in `tmp/test_paper2/`. This ensures the implementation handles real-world data correctly from the start.

## Complexity Tracking

*Fill ONLY if Constitution Check has violations that must be justified*

| Violation | Why Needed | Simpler Alternative Rejected Because |
|-----------|------------|-------------------------------------|
| **Minimal Dependencies** (scipy, statsmodels, seaborn, numpy) | Statistical analysis requires specialized libraries for: (1) scipy: Shapiro-Wilk, Mann-Whitney U, Kruskal-Wallis, t-tests, ANOVA, Levene's test - industry-standard implementations with peer-reviewed algorithms, (2) statsmodels: Statistical power analysis with validated formulas, (3) seaborn/matplotlib: Publication-quality visualizations with kernel density estimation for violin plots, (4) numpy: Bootstrap resampling with vectorized operations | Implementing statistical tests from scratch would: (1) Introduce mathematical bugs (many tests have edge cases), (2) Lack peer review and validation, (3) Take months of development time, (4) Reduce scientific credibility (reviewers expect scipy/statsmodels), (5) Create maintenance burden for complex algorithms. Using established libraries ensures correctness and reproducibility. |
| **No Backward Compatibility Burden** (must work with specific experiment) | This feature is being developed for immediate use with existing experiment at `~/projects/uece/baes/baes_benchmarking_20251028_0713`. Cannot have graceful fallbacks. | Testing with synthetic/mock data would not reveal real-world edge cases in the actual experiment data (outliers, zero variance, small samples, etc.). The target experiment IS the test suite. |

**Special Testing Requirement**: 
- ❌ No pytest unit/integration tests
- ✅ All validation via real experiment generation
- ✅ Output to `tmp/test_paper2/` for manual inspection
- ✅ Must handle actual data correctly from day one

**Note**: These are the only constitution violations. All other principles are fully complied with, including the critical Educational Accessibility principle which is a core feature of this implementation.

---

## Phase 0: Research & Best Practices ✅ COMPLETE

**Status**: Research documentation generated → `research.md` (381 lines)

**Resolved Unknowns**:
1. ✅ Statistical library selection: scipy.stats for tests, statsmodels for power analysis
2. ✅ Visualization approach: matplotlib + seaborn for publication-quality SVG output
3. ✅ Bootstrap strategy: 10,000 iterations with numpy.random.default_rng, percentile CIs
4. ✅ Educational content generation: Template-based with analogy database, Flesch-Kincaid validation
5. ✅ Test selection logic: Decision tree based on Shapiro-Wilk normality + Levene variance tests
6. ✅ Effect size measures: Cohen's d (parametric), Cliff's delta (non-parametric) with standard thresholds
7. ✅ Markdown generation: f-string templates with helper functions, emoji icons for navigation
8. ✅ Paper integration: Two-phase (analysis generates reports → paper parses and embeds content)
9. ✅ Plot specifications: Box (median/quartiles/outliers), violin (KDE+quartiles), forest (effect sizes+CIs), Q-Q (normality check)
10. ✅ Dependencies finalized: scipy==1.11.0, statsmodels==0.14.0, seaborn==0.12.2, numpy==1.24.3

**Key Decisions Documented**:
- Statistical tests: Shapiro-Wilk → t-test/ANOVA (normal) or Mann-Whitney/Kruskal-Wallis (non-normal)
- Power analysis: Target 80%, recommend sample sizes using statsmodels.stats.power.TTestIndPower
- Visualizations: SVG format for scalability, seaborn 'colorblind' palette for accessibility
- Readability: 8th grade target using template library with analogies
- Performance: <60s total analysis time, <2s per bootstrap, <5s for all visualizations

---

## Phase 1: Design & Contracts ✅ COMPLETE

**Status**: All design artifacts generated

**Generated Artifacts**:
1. ✅ **data-model.md** (548 lines) - Defines 7 core entities:
   - `MetricDistribution`: Descriptive stats + outlier detection
   - `AssumptionCheck`: Shapiro-Wilk and Levene test results
   - `StatisticalTest`: Hypothesis test results with interpretation
   - `EffectSize`: Cohen's d / Cliff's delta with bootstrap CIs
   - `PowerAnalysis`: Achieved power + sample size recommendations
   - `Visualization`: Plot metadata (type, paths, captions)
   - `StatisticalFindings`: Top-level container aggregating all results

2. ✅ **contracts/statistical_analyzer.md** (315 lines) - Core analysis API:
   - `analyze_experiment()`: Main entry point
   - `compute_metric_distribution()`: Descriptive statistics
   - `run_normality_test()`: Shapiro-Wilk
   - `run_variance_test()`: Levene's test
   - `run_statistical_test()`: Auto-select appropriate test
   - `compute_effect_size()`: Cohen's d or Cliff's delta with bootstrap CI
   - `perform_power_analysis()`: statsmodels power calculations
   - `generate_visualization()`: SVG plots

3. ✅ **contracts/educational_content.md** (298 lines) - Didactic explanation API:
   - `explain_statistical_test()`: What/Why/How templates
   - `explain_effect_size()`: Plain-language + analogy
   - `explain_power_analysis()`: Researcher-friendly recommendations
   - `explain_visualization()`: Reading guides for plots
   - `generate_quick_start_guide()`: Beginner-friendly navigation
   - `generate_glossary()`: Statistical terms dictionary
   - `generate_analogy()`: Real-world comparisons

4. ✅ **contracts/statistical_visualizations.md** (283 lines) - Plotting API:
   - `generate_box_plot()`: Median + quartiles + outliers
   - `generate_violin_plot()`: KDE + quartile lines
   - `generate_forest_plot()`: Effect sizes with CIs (horizontal)
   - `generate_qq_plot()`: Normality assessment
   - `generate_all_visualizations()`: Batch generation for all metrics
   - Publication styling: seaborn colorblind palette, SVG output

5. ✅ **quickstart.md** (392 lines) - User guide:
   - For complete beginners: 4-step process (summary → visualizations → power warnings → paper integration)
   - Key concepts explained: p-value, effect size, confidence interval, statistical power
   - Visualization reading guides: Box, violin, forest, Q-Q plot interpretations
   - Common questions: Interpreting contradictory results, low power, reporting guidelines
   - Checklist for correct usage

6. ✅ **Agent context updated**: Python 3.11+ + scipy/statsmodels/seaborn added to `.github/copilot-instructions.md`

**Data Flow Documented**:
```
ExperimentAnalyzer.analyze()
  → StatisticalAnalyzer.analyze_experiment(run_data)
  → StatisticalFindings object
  → Serialize to statistical_report_summary.md + statistical_report_full.md
PaperGenerator._load_analyzed_data()
  → Parse statistical_report_summary.md
  → Extract key_findings, visualizations, methodology_text
  → Inject into Methodology/Results/Discussion sections
```

---

## Development Workflow (Special Requirements)

### Testing Approach

**NO UNIT TESTS**: This feature uses real-world validation instead of synthetic test cases.

**Target Experiment**: `~/projects/uece/baes/baes_benchmarking_20251028_0713`
- Existing experiment with real run data
- Must work correctly with this data from day one
- No backward compatibility fallbacks allowed

**Output Directory**: `tmp/test_paper2/`
- All development testing outputs here
- Gitignored (not committed)
- Cleared before each test run

### Validation Command

After implementing each component, run:

```bash
# Clear previous test outputs
rm -rf tmp/test_paper2/

# Generate reports with new statistical analysis
python scripts/generate_paper.py \
  ~/projects/uece/baes/baes_benchmarking_20251028_0713 \
  --output-dir tmp/test_paper2/

# Verify outputs
ls -la tmp/test_paper2/
ls -la tmp/test_paper2/figures/statistical/
```

### Manual Inspection Checklist

After each run, verify:

1. **Reports Generated**:
   - [ ] `tmp/test_paper2/statistical_report_summary.md` exists
   - [ ] `tmp/test_paper2/statistical_report_full.md` exists
   - [ ] Both are valid markdown with no template placeholders

2. **Visualizations Created**:
   - [ ] `tmp/test_paper2/figures/statistical/*.svg` files exist
   - [ ] SVG files render correctly in browser
   - [ ] Plots show actual data from experiment (not empty/error)

3. **Statistical Content Correct**:
   - [ ] p-values are between 0 and 1
   - [ ] Effect sizes have magnitude labels (small/medium/large)
   - [ ] Confidence intervals are formatted correctly
   - [ ] Power analysis shows recommended sample sizes

4. **Educational Content Present**:
   - [ ] "What/Why/How" sections for each test
   - [ ] Emoji icons used (📚, 💡, 📊, ✅, ⚠️)
   - [ ] Plain-language explanations (no unexplained jargon)
   - [ ] Quick Start Guide at beginning of summary report

5. **Paper Integration**:
   - [ ] `tmp/test_paper2/paper.md` includes statistical methodology section
   - [ ] Results section has effect sizes (not just means)
   - [ ] Visualizations embedded with `![](figures/statistical/...)`
   - [ ] Discussion mentions power limitations

### Iterative Development

For each new module (e.g., `statistical_analyzer.py`):

1. **Implement** core functionality
2. **Run validation command** (above)
3. **Inspect outputs** manually
4. **Fix issues** revealed by real data
5. **Re-run** until output is correct
6. **Move to next module**

**No pytest required** - the real experiment data is the test suite.

---

## Phase 2: Task Breakdown (Pending)

**Status**: ⏳ Not started - requires separate `/speckit.tasks` command

**Next Command**: `/speckit.tasks` to generate `tasks.md` with:
- Implementation tasks broken down from functional requirements
- Priority ordering (1a: statistical analysis, 1b: educational helpers, 2: paper integration)
- Acceptance criteria per task
- Estimated complexity/effort

---

## Implementation Readiness Summary

### ✅ Planning Phase Complete

**Artifacts Generated**:
- ✅ Implementation plan (`plan.md`) - 130 lines
- ✅ Research documentation (`research.md`) - 381 lines
- ✅ Data model specification (`data-model.md`) - 548 lines
- ✅ API contracts (`contracts/*.md`) - 896 lines total (3 files)
- ✅ User guide (`quickstart.md`) - 392 lines
- ✅ Agent context updated (scipy, statsmodels, seaborn added)

**Total Documentation**: ~2,347 lines across 6 files

**Constitution Check**: ✅ PASSED (1 justified violation for statistical libraries)

**Branch**: `011-enhance-statistical-report` (created by setup-plan.sh)

**Feature Spec**: `spec.md` (883 lines, 58 FRs, 30 SCs, 6 user stories)

### 📋 Next Steps

1. **Run `/speckit.tasks`** to generate task breakdown (`tasks.md`)
2. **Review generated plan** - verify all design decisions align with functional requirements
3. **Add dependencies to `requirements.txt`**:
   ```
   scipy==1.11.0
   statsmodels==0.14.0
   seaborn==0.12.2
   numpy==1.24.3
   ```
4. **Begin implementation** following priority order:
   - Priority 1a: `statistical_analyzer.py` (core analysis logic)
   - Priority 1b: `educational_content.py` (didactic explanations)
   - Priority 2: Paper integration (`paper_generator.py` enhancements)
5. **Validate after each component** by running:
   ```bash
   python scripts/generate_paper.py \
     ~/projects/uece/baes/baes_benchmarking_20251028_0713 \
     --output-dir tmp/test_paper2/
   ```
   Then inspect `tmp/test_paper2/statistical_report_*.md` and `tmp/test_paper2/figures/statistical/`

### 🎯 Success Criteria for Implementation

From FR/SC requirements:
- Statistical tests produce correct p-values (verified against scipy documentation examples)
- Effect sizes match manual calculations (Cohen's d, Cliff's delta formulas)
- Power analysis recommends sample sizes matching statsmodels.stats.power
- Visualizations render as SVG with correct data representation
- Educational content passes 8th grade readability (Flesch-Kincaid ≤10)
- Reports generate in <60 seconds for target experiment
- Paper sections automatically include statistical content
- 90%+ comprehension by non-statisticians (user testing)

**Validation Method**:
- ✅ Run against `~/projects/uece/baes/baes_benchmarking_20251028_0713`
- ✅ Output to `tmp/test_paper2/`
- ✅ Manually inspect generated reports and visualizations
- ✅ Verify paper includes statistical content
- ✅ Check reading level of explanations
- ❌ NO automated pytest tests required

### ⚠️ Risk Mitigation

**Risk 1: Bootstrap resampling too slow**
- Mitigation: Vectorize with numpy, profile early, reduce to 5,000 iterations if needed

**Risk 2: Educational content too technical**
- Mitigation: Flesch-Kincaid validation during development, user testing with non-statisticians

**Risk 3: Visualization rendering issues in PDF**
- Mitigation: Test LaTeX compilation early, provide PNG fallback option

---

**Plan Status**: ✅ COMPLETE - Ready for `/speckit.tasks` command

````
