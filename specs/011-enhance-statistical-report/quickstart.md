# Quick Start Guide: Enhanced Statistical Reports

**Version**: 1.0  
**Last Updated**: 2025-10-29

## Overview

This guide helps you understand and use the enhanced statistical reports generated by genai-devbench. Whether you're a statistician or have no statistics background, you'll learn how to interpret the results and use them effectively in your research.

---

## üìã What Gets Generated

After running `python scripts/generate_paper.py my_experiment`, you'll find:

### 1. Statistical Reports
- **`statistical_report_summary.md`** - Executive summary (5-10 min read)
  - Key findings with effect sizes
  - Visualizations
  - Power recommendations
  - **Start here!**

- **`statistical_report_full.md`** - Complete analysis (20-30 min read)
  - Detailed methodology
  - All assumption tests
  - Complete statistical tables
  - Glossary of terms
  - **Reference when you need details**

### 2. Visualizations (in `figures/statistical/`)
- Box plots - Show median and spread
- Violin plots - Show distribution shapes
- Forest plots - Show effect sizes with uncertainty
- Q-Q plots - Check if data is normally distributed

### 3. Enhanced Paper
- **`paper.md`** or **`paper.pdf`** - Automatically includes:
  - Statistical methodology section
  - Effect sizes in results
  - Visualizations embedded
  - Power limitations in discussion

---

## üéØ For Complete Beginners: Read This First

### Step 1: Check the Executive Summary

Open `statistical_report_summary.md` and look for the **Key Findings** section. This tells you:

‚úÖ **Which frameworks differ significantly** (not just random chance)
‚úÖ **How MUCH they differ** (effect sizes - is it meaningful?)
‚úÖ **How confident we are** (confidence intervals - margin of error)

**Example**:
```
Key Findings:
- ChatDev significantly faster than MetaGPT (p=0.032, d=0.72, medium effect)
  ‚Üí ChatDev saves ~33 seconds on average (95% CI: [18s, 48s])
- No significant difference between MetaGPT and CrewAI (p=0.15)
- ‚ö†Ô∏è Power analysis: Increase to 13 runs for reliable detection (currently 5)
```

**What this means**:
1. ChatDev is genuinely faster (p=0.032 = 3.2% chance it's random luck)
2. The difference is medium-sized (d=0.72 - noticeable in practice)
3. ChatDev saves 18-48 seconds on average (95% confident)
4. MetaGPT and CrewAI are too similar to call a winner
5. You need more runs to be confident in all comparisons

### Step 2: Look at the Pictures

Scroll to the **Visualizations** section. Each plot has a "How to read this" guide.

**Violin Plot** (most useful for beginners):
- **Width** = How common that value is (wider = more data points)
- **White dot** = Median (middle value)
- **Thick bar** = Middle 50% of data
- If violins don't overlap much ‚Üí groups are different

**Forest Plot** (shows effect sizes):
- **Dot** = Effect size estimate
- **Horizontal line** = Uncertainty range (95% CI)
- **Vertical line at 0** = No effect line
- If the horizontal line crosses 0 ‚Üí difference uncertain

### Step 3: Check Power Warnings

Look for ‚ö†Ô∏è warnings about statistical power. If power < 80%:

‚ùå **Don't ignore**: Your results might be unreliable
‚úÖ **Action**: Run more experiments as recommended
üìä **Why**: Low power = high risk of missing real differences

### Step 4: Use Results in Your Paper

The statistical analysis is **automatically included** in your generated paper:
- **Methodology section**: Explains which tests were used and why
- **Results section**: Includes effect sizes and visualizations
- **Discussion section**: Notes power limitations

---

## üéì Understanding Key Concepts

### p-value (Statistical Significance)

**What it is**: Probability the difference is just random luck

**Rules of thumb**:
- p < 0.05 ‚Üí "Statistically significant" (reliable difference)
- p > 0.05 ‚Üí "Not significant" (could be chance)

**Common mistake**: p < 0.05 doesn't mean the difference is IMPORTANT, just that it's not random.

**Analogy**: p=0.03 means "3% chance this is a fluke" - like rolling a 20-sided die and getting a 1. Unlikely, but possible.

### Effect Size (Practical Significance)

**What it is**: HOW MUCH groups differ (more important than p-value!)

**Cohen's d thresholds**:
- 0.2 = **Small** (hard to notice in practice)
- 0.5 = **Medium** (noticeable difference)
- 0.8 = **Large** (obvious difference)

**Cliff's delta thresholds**:
- 0.147 = **Small**
- 0.33 = **Medium**
- 0.474 = **Large**

**Why it matters**: 
- p-value tells you IF there's a difference
- Effect size tells you if you should CARE

**Analogy**: d=0.8 is like the height difference between a 13-year-old and an 18-year-old. It's not just statistically detectable - it's obvious.

### Confidence Interval (CI)

**What it is**: Range where the true value likely falls

**Example**: "ChatDev saves 33s on average (95% CI: [18s, 48s])"
- Best estimate: 33 seconds faster
- 95% confident the true saving is between 18-48 seconds
- Could be as little as 18s or as much as 48s

**Narrow CI** = Precise estimate (good!)
**Wide CI** = Lots of uncertainty (need more data)

### Statistical Power

**What it is**: Probability your experiment will detect a real difference if it exists

**Think of it like**: Sensitivity of a medical test
- 80% power = Detects 80% of real effects (standard scientific threshold)
- 50% power = Coin flip - too unreliable
- 95% power = Very sensitive - excellent

**What affects power**:
1. Sample size (more runs = more power)
2. Effect size (bigger differences easier to detect)
3. Variability (more consistent data = easier to detect differences)

**If your power is low (<80%)**:
- ‚ö†Ô∏è Risk: Might miss real differences (false negative)
- ‚úÖ Solution: Add more runs as recommended in report

---

## üìä Reading the Visualizations

### Box Plot

```
    ‚îÇ
 ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ  ‚Üê Max (within 1.5*IQR)
    ‚îÇ
 ‚îå‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îê
 ‚îÇ  ‚îÇ  ‚îÇ  ‚Üê Box (middle 50% of data)
 ‚îÇ ‚îÄ‚îº‚îÄ ‚îÇ  ‚Üê Median (middle value)
 ‚îî‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îò
    ‚îÇ
 ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ  ‚Üê Min (within 1.5*IQR)
    ‚îÇ
    ‚Ä¢     ‚Üê Outliers
```

**How to compare**:
- Higher median ‚Üí Slower/more of that metric
- Bigger box ‚Üí More variable (less consistent)
- Overlapping boxes ‚Üí Groups might be similar

### Violin Plot

```
    ‚ï±‚ï≤      ‚Üê Wider = more data points at this value
   ‚ï±  ‚ï≤
  ‚ï± ‚îÅ‚îÅ ‚ï≤   ‚Üê White dot = median
 ‚ï±  ‚îÉ   ‚ï≤  ‚Üê Thick bar = middle 50%
‚ï±   ‚îÉ    ‚ï≤
‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ  ‚Üê Thin line = data range
```

**How to compare**:
- Shape shows distribution (symmetric vs skewed)
- Non-overlapping violins ‚Üí Clearly different groups
- Similar shapes ‚Üí Similar distributions

### Forest Plot

```
Framework A vs B   ‚Ä¢‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  [Effect size with CI]
Framework A vs C     ‚Ä¢‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Framework B vs C      ‚Ä¢‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                 ‚îÇ
                 0  ‚Üê No effect line
```

**How to interpret**:
- **Dot** = Effect size estimate
- **Line** = 95% confidence interval (uncertainty)
- **Crosses 0** ‚Üí Difference uncertain
- **Doesn't cross 0** ‚Üí Reliable difference
- **Color**: Green (small), Orange (medium), Red (large)

### Q-Q Plot

```
     ‚îÇ  ‚Ä¢
     ‚îÇ ‚Ä¢
   S ‚îÇ‚Ä¢     ‚Üê Points on line = normal
   a ‚îÇ‚Ä¢     ‚Üê Points off line = not normal
   m ‚îÇ ‚Ä¢
   p ‚îÇ  ‚Ä¢
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
     Theoretical
```

**What it tells you**:
- Points on 45¬∞ line ‚Üí Data is normally distributed
- Points curve off ‚Üí Data is skewed or has outliers
- **Why it matters**: Determines which statistical tests are valid

---

## üîç Common Questions

### Q1: What if p-value is significant but effect size is small?

**A**: Statistical significance ‚â† practical importance.

**Example**: p=0.01, d=0.15
- **Means**: The difference is real (not random), but tiny
- **Action**: Report it, but don't overstate importance
- **Quote**: "Statistically significant but negligible effect (d=0.15)"

### Q2: What if effect size is large but p-value is not significant?

**A**: You might have a real effect, but not enough data to be confident.

**Example**: p=0.08, d=0.75
- **Means**: Looks like a medium-large difference, but could be chance (8% probability)
- **Action**: Check power analysis - likely underpowered
- **Next step**: Add more runs and retest

### Q3: How do I report results in my paper?

**Summary format**:
```
ChatDev was significantly faster than MetaGPT (t(8)=2.45, p=0.032, 
d=0.72, 95% CI [0.28, 1.16]), representing a medium effect size. 
On average, ChatDev completed tasks 33 seconds faster (95% CI [18s, 48s]).
```

**What to include**:
1. Direction of difference ("faster", "higher quality")
2. Test statistic and p-value
3. Effect size with interpretation (small/medium/large)
4. 95% CI for effect size
5. Practical meaning (actual time/cost difference)

### Q4: My power is only 54%. Are my results invalid?

**A**: No, but be cautious about interpretation.

**What low power means**:
- ‚úÖ If you found a significant result (p<0.05), it's likely real
- ‚ö†Ô∏è If you didn't find significance (p>0.05), you might have missed a real effect
- üìä You have 54% chance of detecting this size effect (not great odds)

**Action**:
1. Report the limitation in your Discussion section
2. Note the recommended sample size for future work
3. Be conservative about claiming "no difference"

### Q5: Which metric should I focus on?

**A**: Focus on your **primary metric** (listed at top of report).

**Secondary metrics**:
- Check if patterns are consistent
- Explore unexpected findings
- Don't cherry-pick metrics that show significance

**If metrics contradict**:
- This is normal! Different metrics measure different things
- Discuss in your paper why metrics might differ
- Rely on the metric most relevant to your research question

### Q6: What if my data isn't normally distributed?

**A**: The report automatically handles this!

**Process**:
1. Shapiro-Wilk test checks normality
2. If normal ‚Üí Uses t-test/ANOVA
3. If not normal ‚Üí Uses Mann-Whitney/Kruskal-Wallis (non-parametric tests)

**You don't need to do anything** - the system selects appropriate tests.

### Q7: How do I use the visualizations in my paper?

**A**: They're automatically embedded! But you can also:

1. **In presentations**: Copy SVG files to slides (scalable, no pixelation)
2. **In papers**: Reference figure numbers ("see Figure 3")
3. **In supplementary materials**: Include all plots for transparency

**Citation format**:
```markdown
![Execution time comparison](figures/statistical/violin_plot_execution_time.svg)

**Figure 3**: Distribution of execution times across frameworks. 
ChatDev shows consistently lower execution times (median 45s) compared 
to MetaGPT (median 78s, p=0.032, d=0.72).
```

---

## ‚úÖ Checklist: Have I Used the Report Correctly?

Before publishing or presenting:

- [ ] Read executive summary for key findings
- [ ] Checked p-values AND effect sizes (not just p-values)
- [ ] Reviewed power analysis warnings
- [ ] Understood confidence intervals (margin of error)
- [ ] Looked at visualizations to confirm patterns
- [ ] Reported limitations from Discussion section
- [ ] Included recommended sample sizes for future work
- [ ] Used primary metric as main outcome
- [ ] Cited statistical methods from Methodology section
- [ ] Embedded visualizations in paper/presentation

---

## üéØ Next Steps

### For Your Current Experiment

1. **If power is adequate (‚â•80%)**:
   - ‚úÖ Use results confidently
   - ‚úÖ Report findings in paper
   - ‚úÖ Share visualizations

2. **If power is low (<80%)**:
   - ‚ö†Ô∏è Note limitation in Discussion
   - üìä Plan follow-up with recommended sample size
   - üéì Report results as "preliminary" or "exploratory"

### For Future Experiments

1. **Use recommended sample sizes** from power analysis
2. **Pre-register hypotheses** (decide primary metric before running)
3. **Report all metrics** (not just significant ones - avoid p-hacking)
4. **Share statistical reports** in supplementary materials for transparency

---

## üìö Learning More

### Beginner Resources
- **Khan Academy**: Statistics and Probability (free)
- **Seeing Theory**: Visual introduction to statistics (interactive)
- **Effect Size FAQs**: Understanding Cohen's d and Cliff's delta

### Intermediate Resources
- **Statistical Rethinking** by Richard McElreath (Bayesian perspective)
- **The New Statistics** by Geoff Cumming (confidence intervals)
- **Statistical Power Analysis** by Jacob Cohen (classic reference)

### Glossary

For definitions of all statistical terms, see the **Glossary** section in `statistical_report_full.md`.

---

## üÜò Getting Help

**If you're confused**:
1. Re-read the Quick Start Guide in the summary report
2. Check the Glossary in the full report
3. Look at the visualization reading guides
4. Ask a colleague with statistics background

**If you find an error**:
1. Check your experiment configuration
2. Verify input data quality
3. Review logs for warnings
4. Report issues with reproducible example

---

## üß™ Validation Scenarios & Examples

### Running Against Real Experiments

**Basic Usage**:
```bash
# Run analysis on an existing experiment
python scripts/generate_paper.py ~/projects/uece/baes/baes_benchmarking_20251028_0713 \
    --output-dir tmp/test_paper2/
```

**Expected Output Structure**:
```
tmp/test_paper2/
‚îú‚îÄ‚îÄ metrics.json                          # Aggregated metrics
‚îú‚îÄ‚îÄ statistical_report.md                 # Basic statistical summary (legacy)
‚îú‚îÄ‚îÄ statistical_report_summary.md         # Executive summary (<300 lines)
‚îú‚îÄ‚îÄ statistical_report_full.md            # Complete analysis (800-1200 lines)
‚îú‚îÄ‚îÄ paper.md                              # Generated paper with statistical content
‚îî‚îÄ‚îÄ figures/
    ‚îú‚îÄ‚îÄ comparative/                      # Traditional comparative charts
    ‚îÇ   ‚îú‚îÄ‚îÄ execution_time_comparison.svg
    ‚îÇ   ‚îî‚îÄ‚îÄ ...
    ‚îî‚îÄ‚îÄ statistical/                      # Statistical visualizations
        ‚îú‚îÄ‚îÄ box_plot_execution_time.svg
        ‚îú‚îÄ‚îÄ violin_plot_execution_time.svg
        ‚îú‚îÄ‚îÄ forest_plot_execution_time.svg
        ‚îú‚îÄ‚îÄ qq_plot_execution_time_chatdev.svg
        ‚îú‚îÄ‚îÄ qq_plot_execution_time_metagpt.svg
        ‚îî‚îÄ‚îÄ ...
```

### Interpretation Examples

**Scenario 1: Clear Winner**
```markdown
**Finding**: ChatDev vs MetaGPT on execution_time
- **Test**: Mann-Whitney U = 42, p = 0.008
- **Effect Size**: Cliff's Delta = -0.64 [95% CI: -0.85, -0.38], large effect
- **Interpretation**: ChatDev is significantly faster than MetaGPT with a large, 
  practically meaningful difference. We can be 95% confident the true difference 
  is between 0.38 and 0.85 (on a -1 to +1 scale).
- **Power**: 92% (adequate for reliable conclusions)
```

**What this means for your paper**:
- ‚úÖ State: "ChatDev demonstrated significantly faster execution times than MetaGPT"
- ‚úÖ Include: Effect size and confidence interval
- ‚úÖ Add: Visual comparison (box plot or violin plot)
- ‚úÖ Discuss: Practical implications (time savings in real workflows)

**Scenario 2: No Significant Difference**
```markdown
**Finding**: MetaGPT vs CrewAI on tokens_total
- **Test**: Welch's t-test, t = 1.47, p = 0.164
- **Effect Size**: Cohen's d = 0.31 [95% CI: -0.14, 0.76], small effect
- **Interpretation**: No statistically significant difference detected, though a 
  small effect favoring CrewAI is possible given the CI includes 0.
- **Power**: 45% (INADEQUATE - need 18 runs per group for 80% power)
```

**What this means for your paper**:
- ‚ö†Ô∏è **DO NOT** state: "MetaGPT and CrewAI are equivalent" (low power!)
- ‚úÖ **DO** state: "No significant difference was detected between MetaGPT and 
  CrewAI (p=0.164), though the study was underpowered (45%) to detect small 
  effects reliably"
- ‚úÖ **DO** recommend: "Future work with larger sample sizes (n‚â•18) is needed to 
  conclusively compare these frameworks"

**Scenario 3: Violated Assumptions**
```markdown
**Finding**: Skewed data detected for cost_total
- **Shapiro-Wilk**: ChatDev p = 0.003 (non-normal)
- **Levene's Test**: F = 8.21, p = 0.011 (unequal variances)
- **Test Used**: Mann-Whitney U (non-parametric, robust to violations)
- **Recommendation**: Interpret with caution; consider log-transformation for 
  future analyses
```

**What this means for your paper**:
- ‚úÖ Acknowledge: "Cost data showed non-normal distributions with unequal 
  variances across groups"
- ‚úÖ Justify: "Non-parametric Mann-Whitney U test was used to handle these 
  violations"
- ‚úÖ Transparency: Include Q-Q plots in supplementary materials
- ‚úÖ Note: Mention in limitations section

### Validation Checklist

After generating reports, verify:

- [ ] **Reports Generated**
  - [ ] `statistical_report_summary.md` exists (< 300 lines)
  - [ ] `statistical_report_full.md` exists (800-1200 lines)
  - [ ] Both reports contain actual data (no placeholders)

- [ ] **Statistical Content**
  - [ ] All metrics have normality tests (Shapiro-Wilk)
  - [ ] Appropriate tests selected (parametric for normal, non-parametric otherwise)
  - [ ] Effect sizes present with confidence intervals
  - [ ] p-values between 0 and 1 (not placeholders)
  - [ ] Power analysis with recommendations

- [ ] **Visualizations**
  - [ ] Box plots show actual experiment data (not empty)
  - [ ] Forest plots display effect sizes with CIs
  - [ ] Q-Q plots show normality assessment
  - [ ] All SVG files render in browser

- [ ] **Educational Content**
  - [ ] Quick Start Guide at beginning of summary
  - [ ] "What/Why/How" explanations for all tests
  - [ ] Glossary with plain-language definitions
  - [ ] Emoji icons used for navigation (üìö, üí°, ‚ö†Ô∏è, ‚úÖ)

- [ ] **Paper Integration**
  - [ ] `paper.md` includes "Statistical Analysis" in Methodology
  - [ ] Results section mentions effect sizes
  - [ ] Statistical visualizations embedded in Results
  - [ ] Discussion includes power limitations (if warnings exist)

**Validation Time**: ~10 minutes for complete review

---

## üéâ Final Tips

### Do's ‚úÖ
- **Do** report effect sizes, not just p-values
- **Do** acknowledge power limitations
- **Do** use visualizations to communicate findings
- **Do** provide confidence intervals
- **Do** share full statistical report in supplementary materials

### Don'ts ‚ùå
- **Don't** claim "no difference" based on p>0.05 with low power
- **Don't** cherry-pick metrics that show significance
- **Don't** round p-values to "p<0.05" (report exact values)
- **Don't** interpret statistical significance as practical importance
- **Don't** ignore warnings in the report

---

**Remember**: Statistics is a tool to help you understand your data, not a barrier to publication. The enhanced reports make it easy to do rigorous analysis - use them to make your research more credible and reproducible!

For questions about the statistical methods, see the **Methodology** section in the full report or consult `contracts/statistical_analyzer.md` in the project documentation.
