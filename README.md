# GenAI-DevBench - Experiment Generator

**Generate standalone experiment projects for evaluating GenAI software development frameworks**

GenAI-DevBench is a powerful generator that creates complete, self-contained experiment projects for rigorous benchmarking of AI-powered development frameworks (BAES, ChatDev, GitHub Copilot). Each generated experiment is a fully independent Git repository with everything needed to run sophisticated comparative evaluations.

---

## ğŸ¯ What Does This Do?

This generator creates **standalone experiment projects** that:

- âœ… Are completely independent (no dependencies on this generator after creation)
- âœ… Include only the code and configurations needed for your specific setup
- âœ… Come with one-command setup and execution scripts
- âœ… Are initialized as Git repositories ready for version control
- âœ… Can be distributed, archived, or run on any machine with Python 3.9+

**Think of it as:** A project template generator specifically designed for AI framework evaluation experiments.

---

## âš¡ Quick Start

### Prerequisites

- **Python 3.11+** (required)
- **Git** (required)
- **API Keys**: OpenAI for BAEs/ChatDev, GitHub token for Spec-kit
- **System**: 8GB RAM, 10GB disk space (per multi-framework run)

### Installation

```bash
# Clone repository
git clone https://github.com/nosredna123/genai-devbench.git
cd genai-devbench

# Create virtual environment (recommended)
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Configure API keys
cp .env.example .env
# Edit .env with your API keys:
#   OPENAI_API_KEY_BAES=sk-your-openai-key
#   OPENAI_API_KEY_CHATDEV=sk-your-openai-key
#   OPENAI_API_KEY_GHSPEC=sk-your-openai-key
#
# IMPORTANT: Also add API Key IDs for usage tracking (find in OpenAI Dashboard > Usage):
#   OPENAI_API_KEY_BAES_ID=key_XXXXXXXXXXXX
#   OPENAI_API_KEY_CHATDEV_ID=key_XXXXXXXXXXXX
#   OPENAI_API_KEY_GHSPEC_ID=key_XXXXXXXXXXXX
```

### Your First Experiment (5 Minutes)

The framework supports **config sets** - curated templates for common scenarios:

```bash
# 1. List available config sets
python scripts/new_experiment.py --list-config-sets

# Output:
# ğŸ“¦ Available Config Sets:
#   â€¢ default (6 steps): Traditional 6-step CRUD application
#   â€¢ minimal (1 step): Hello World API for testing

# 2. Generate experiment from config set
python scripts/new_experiment.py \
    --name my_first_experiment \
    --config-set default \
    --model gpt-4o-mini \
    --frameworks baes \
    --runs 10

# 3. Customize (optional)
vim my_first_experiment/config.yaml
# â†’ Disable steps, reorder, adjust timeouts

# 4. Run the experiment
cd my_first_experiment
./run.sh

# 5. View results
cat runs/latest/summary.json
```

**What happened:**
- âœ… Generated experiment from `default` config set (6 CRUD steps)
- âœ… Copied all prompts, HITL files, and configurations
- âœ… Created self-contained, runnable experiment
- âœ… Ready to customize and execute

**See [Config Sets Quick Start](docs/configurable_steps/QUICKSTART_CONFIG_SETS.md) for detailed guide.**

---

## âš ï¸ Breaking Changes - Metrics Configuration (v2.0+)

**If you have existing experiment configs**, you need to migrate your metrics configuration to the new unified format.

### What Changed?

The old 3-subsection metrics format has been replaced with a simpler unified format:

**OLD (deprecated):**
```yaml
metrics:
  reliable_metrics:
    TOK_IN: { name: "Input Tokens", ... }
  derived_metrics:
    COST_USD: { name: "Cost", ... }
  excluded_metrics:
    MC: { name: "Maintainability", ... }
```

**NEW (required):**
```yaml
metrics:
  TOK_IN: 
    name: "Input Tokens"
    status: measured  # or 'derived', 'unmeasured'
    reason: "Direct from OpenAI API"  # optional
    # ... other fields
  COST_USD:
    name: "Cost"
    status: derived
    reason: "Calculated from token counts"
  MC:
    name: "Maintainability"
    status: unmeasured
    reason: "Requires static analysis tool integration"
```

### Migration Required

If you see this error:
```
ConfigMigrationError: Old metrics format detected. Found subsections: reliable_metrics, derived_metrics
```

**â†’ See [CONFIG_MIGRATION_GUIDE.md](docs/CONFIG_MIGRATION_GUIDE.md)** for step-by-step migration instructions.

### Benefits of New Format

- âœ… **Simpler**: One section instead of three
- âœ… **More flexible**: `status` field replaces rigid subsections
- âœ… **Better documentation**: `reason` field explains why metrics are unmeasured
- âœ… **Auto-generated limitations**: Reports automatically document unmeasured metrics

---

### Compare Multiple Experiments

```bash
# Create baseline
python scripts/new_experiment.py \
    --name baseline_gpt4o \
    --model gpt-4o \
    --frameworks baes \
    --runs 10

# Create variant
python scripts/new_experiment.py \
    --name variant_gpt4omini \
    --model gpt-4o-mini \
    --frameworks baes \
    --runs 10

# Run both
python scripts/run_experiment.py baseline_gpt4o
python scripts/run_experiment.py variant_gpt4omini

# Analyze both
./runners/analyze_results.sh baseline_gpt4o
./runners/analyze_results.sh variant_gpt4omini

# Compare
diff experiments/baseline_gpt4o/analysis/report.md \
     experiments/variant_gpt4omini/analysis/report.md
```

**See [Comparison Guide](docs/COMPARISON_GUIDE.md) for statistical comparison techniques.**

### Legacy Single-Run Mode

The framework still supports legacy single-run mode (deprecated):

```bash
# Legacy: Execute single framework run (15-30 minutes)
# DEPRECATED: Use new_experiment.py instead
./runners/run_experiment.sh baes

# Legacy: View metrics from a run
cat runs/baes/<run-id>/metrics.json

# Legacy: Analyze legacy runs
./runners/analyze_results.sh ./analysis_output
```

**Note:** Legacy mode is maintained for backward compatibility but new workflows should use the multi-experiment system.

## ğŸ“¦ Config Sets

**Config Sets** are curated experiment templates that provide pre-configured prompts, steps, and HITL files for common scenarios. They enable rapid experiment creation and standardized testing across different domains.

### Available Config Sets

| Config Set | Steps | Description | Use Case |
|------------|-------|-------------|----------|
| **default** | 6 | Traditional CRUD application (Student/Course/Teacher) | Full-featured API testing, framework comparison |
| **minimal** | 1 | Hello World API | Quick testing, learning, debugging |
| *microservices* | - | Multi-service architecture | *(Coming in V2)* |
| *ml_pipeline* | - | ML model development | *(Coming in V2)* |

### Config Set Workflow

```bash
# 1. List available config sets
python scripts/new_experiment.py --list-config-sets

# 2. Generate experiment from config set
python scripts/new_experiment.py \
    --name my_test \
    --config-set default \
    --model gpt-4o-mini \
    --frameworks baes chatdev \
    --runs 10

# 3. Customize post-generation (optional)
vim my_test/config.yaml
# â†’ Disable steps: set enabled: false
# â†’ Reorder steps: move entries (executes in declaration order)
# â†’ Adjust timeouts and metrics

# 4. Run experiment
cd my_test
./run.sh
```

### Features

- âœ… **Curated Templates**: Pre-configured prompts and steps for common scenarios
- âœ… **Self-Contained**: All files copied (prompts, HITL, configs)
- âœ… **Customizable**: Edit config.yaml post-generation
- âœ… **Declaration Order**: Steps execute in YAML order, not sorted by ID
- âœ… **Fail-Fast Validation**: Catches errors before wasting tokens

### Creating Custom Config Sets

Want to create your own config set? See the [Creating Config Sets Guide](docs/configurable_steps/CREATING_CONFIG_SETS.md).

---

## Documentation

### Getting Started
- **[Quick Start Guide](docs/QUICKSTART.md)** â­ **Start here!** - Create your first experiment in 5 minutes
- **[Config Sets Quick Start](docs/configurable_steps/QUICKSTART_CONFIG_SETS.md)** ğŸ†• **Config sets guide** - Use curated templates
- **[Workflows Guide](docs/WORKFLOWS.md)** - Common usage patterns and real-world scenarios
- **[Comparison Guide](docs/COMPARISON_GUIDE.md)** - Statistical comparison of experiments
- **[Best Practices Guide](docs/BEST_PRACTICES.md)** - Recommendations for effective experimentation

### Config Sets & Customization ğŸ†•
- **[Creating Config Sets](docs/configurable_steps/CREATING_CONFIG_SETS.md)** - Build your own config sets
- **[Implementation Plan](docs/configurable_steps/FINAL-IMPLEMENTATION-PLAN.md)** - Technical design details
- **[Feature Specification](docs/configurable_steps/feature-spec.md)** - Complete requirements

### Multi-Experiment System
- **[Architecture Guide](docs/architecture.md)** - System design and multi-experiment components
- **[Configuration Reference](docs/configuration_reference.md)** - Complete config schema and examples
- **[Validation System](docs/validation_system.md)** - Configuration validation reference
- **[Troubleshooting Guide](docs/troubleshooting.md)** - Common issues and solutions

### Metrics and Analysis
- **[Metrics Guide](docs/metrics.md)** - Complete reference for all 16 metrics
- **[Statistical Power Analysis](docs/statistical_power_analysis.md)** - Sample size calculations
- **[API Usage Reconciliation](docs/reconcile_usage_guide.md)** - Accurate cost tracking

### Legacy Documentation
- [Original Quickstart](docs/quickstart.md) - Legacy single-run quickstart (deprecated)
- [Feature Specification](specs/001-baes-experiment-framework/spec.md) - Requirements and user stories
- [Implementation Plan](specs/001-baes-experiment-framework/plan.md) - Technical design
- [Research Decisions](specs/001-baes-experiment-framework/research.md) - Design rationale

### Testing
- **Test Suite:** Comprehensive unit tests with 100% pass rate
- **Run Tests:** `pytest tests/unit/test_report_generation.py -v`
- **Test Coverage:** 26 tests covering validation, dynamics, and edge cases
- **Execution Time:** < 2 seconds for full suite

## Architecture

```
genai-devbench/
â”œâ”€â”€ experiments/                 # ğŸ†• Multi-experiment storage (isolated)
â”‚   â”œâ”€â”€ <experiment_name>/       # Individual experiment directory
â”‚   â”‚   â”œâ”€â”€ config.yaml          # Experiment configuration (immutable)
â”‚   â”‚   â”œâ”€â”€ README.md            # Auto-generated documentation
â”‚   â”‚   â”œâ”€â”€ runs/                # Run outputs for this experiment
â”‚   â”‚   â”‚   â”œâ”€â”€ manifest.json    # Run tracking and metadata
â”‚   â”‚   â”‚   â”œâ”€â”€ baes/            # Framework-specific runs
â”‚   â”‚   â”‚   â”œâ”€â”€ chatdev/
â”‚   â”‚   â”‚   â””â”€â”€ ghspec/
â”‚   â”‚   â”œâ”€â”€ analysis/            # Analysis outputs
â”‚   â”‚   â”‚   â”œâ”€â”€ report.md        # Statistical report
â”‚   â”‚   â”‚   â””â”€â”€ visualizations/  # Charts and graphs
â”‚   â”‚   â””â”€â”€ .meta/               # Metadata
â”‚   â”‚       â””â”€â”€ config.hash      # Config integrity verification
â”‚   â””â”€â”€ .experiment_registry.json # ğŸ†• Global experiment registry
â”œâ”€â”€ scripts/                     # ğŸ†• Experiment management
â”‚   â”œâ”€â”€ new_experiment.py        # Create experiments (interactive/CLI)
â”‚   â””â”€â”€ run_experiment.py        # Run experiments (high-level wrapper)
â”œâ”€â”€ config/                      # Experiment configuration
â”‚   â”œâ”€â”€ experiment.yaml          # Framework configs, timeouts, seeds
â”‚   â”œâ”€â”€ prompts/                 # 6-step CRUD evolution scenario
â”‚   â””â”€â”€ hitl/                    # HITL clarification templates
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ orchestrator/            # Execution coordination
â”‚   â”‚   â”œâ”€â”€ runner.py            # OrchestratorRunner (single/multi)
â”‚   â”‚   â”œâ”€â”€ metrics_collector.py # 16 metrics computation
â”‚   â”‚   â”œâ”€â”€ validator.py         # CRUD/UI/downtime validation
â”‚   â”‚   â””â”€â”€ archiver.py          # tar.gz with SHA-256
â”‚   â”œâ”€â”€ adapters/                # Framework integrations
â”‚   â”‚   â”œâ”€â”€ base_adapter.py      # Abstract BaseAdapter
â”‚   â”‚   â”œâ”€â”€ baes_adapter.py      # BAEs implementation
â”‚   â”‚   â”œâ”€â”€ chatdev_adapter.py   # ChatDev implementation
â”‚   â”‚   â””â”€â”€ ghspec_adapter.py    # GitHub Spec-kit implementation
â”‚   â”œâ”€â”€ analysis/                # Statistical analysis
â”‚   â”‚   â”œâ”€â”€ statistics.py        # Tests, effect sizes, reports
â”‚   â”‚   â”œâ”€â”€ stopping_rule.py     # Bootstrap CI convergence
â”‚   â”‚   â””â”€â”€ visualizations.py    # Radar, Pareto, timeline charts
â”‚   â””â”€â”€ utils/                   # Utilities
â”‚       â”œâ”€â”€ logger.py            # Structured JSON logging
â”‚       â”œâ”€â”€ config_loader.py     # YAML validation
â”‚       â”œâ”€â”€ isolation.py         # Workspace management
â”‚       â”œâ”€â”€ api_client.py        # OpenAI Usage API verification
â”‚       â”œâ”€â”€ experiment_paths.py  # ğŸ†• Path resolution for experiments
â”‚       â””â”€â”€ experiment_registry.py # ğŸ†• Global experiment tracking
â”œâ”€â”€ runners/                     # Entry point scripts
â”‚   â”œâ”€â”€ run_experiment.sh        # âš ï¸ DEPRECATED (use scripts/run_experiment.py)
â”‚   â”œâ”€â”€ analyze_results.sh       # Analysis pipeline (experiment-aware)
â”‚   â””â”€â”€ reconcile_usage.sh       # ğŸ†• API usage reconciliation
â”œâ”€â”€ runs/                        # ğŸ—„ï¸ Legacy run outputs (gitignored, deprecated)
â”œâ”€â”€ tests/                       # Test suite
â”‚   â”œâ”€â”€ unit/                    # Unit tests
â”‚   â”œâ”€â”€ integration/             # Integration tests
â”‚   â””â”€â”€ contract/                # Contract tests
â””â”€â”€ docs/                        # Documentation
    â”œâ”€â”€ QUICKSTART.md            # ğŸ†• 5-minute getting started guide
    â”œâ”€â”€ WORKFLOWS.md             # ğŸ†• Common usage patterns
    â”œâ”€â”€ COMPARISON_GUIDE.md      # ğŸ†• Statistical comparison guide
    â”œâ”€â”€ BEST_PRACTICES.md        # ğŸ†• Recommendations and tips
    â””â”€â”€ ...                      # Other documentation
```

**Key Changes:**
- ğŸ†• **experiments/** directory: All experiments organized by name
- ğŸ†• **Experiment Registry**: Global tracking of all experiments
- ğŸ†• **Config Hash**: Immutable configuration with integrity verification
- ğŸ†• **Isolated Runs**: Each experiment has its own runs and analysis
- âš ï¸ **Deprecated**: Legacy `runners/run_experiment.sh` (use `scripts/run_experiment.py`)
- ğŸ—„ï¸ **Legacy**: Old `runs/` directory maintained for backward compatibility

---

## ğŸ“„ Camera-Ready Paper Generation

**NEW**: Generate publication-ready papers from experiment results with AI-assisted prose, statistical analysis, and ACM formatting.

### Quick Start

```bash
# After running an experiment with statistical analysis
cd my_first_experiment

# Generate camera-ready paper (requires OpenAI API key)
python ../../scripts/generate_paper.py . --output-dir paper

# Output:
#   paper/main.pdf         # ACM SIGSOFT formatted PDF
#   paper/main.tex         # LaTeX source with AI-generated prose
#   paper/figures/         # Publication-quality figures (PDF + PNG)
#   README.md              # Enhanced with reproduction instructions
```

### Features

**Automated Content Generation:**
- âœ… **AI-Generated Prose**: All sections (Introduction, Related Work, Methodology, Results, Discussion, Conclusion) with â‰¥800 words each
- âœ… **Statistical Tables**: Descriptive statistics, hypothesis tests, effect sizes automatically formatted
- âœ… **Publication Figures**: Vector PDFs (scalable) + PNG (300 DPI) for all metrics
- âœ… **Citation Placeholders**: Bold markers like **[CITE: framework_name]** for manual citation filling
- âœ… **ACM SIGSOFT Format**: Ready for conference submission (sigconf template)

**Customization Options:**
```bash
# Generate only specific sections
python ../../scripts/generate_paper.py . \
    --sections=methodology,results,discussion

# Filter metrics in results section
python ../../scripts/generate_paper.py . \
    --metrics-filter=execution_time,total_cost_usd,quality_score

# Control prose detail level
python ../../scripts/generate_paper.py . \
    --prose-level=minimal  # minimal | standard | comprehensive

# Export only figures (fast, no prose generation)
python ../../scripts/generate_paper.py . --figures-only
# or use dedicated script:
python ../../scripts/export_figures.py . --formats=pdf,png --dpi=300
```

**Reproduction Enhancement:**
- ğŸ“ Automatically enhances experiment README.md with comprehensive reproduction instructions
- â±ï¸ Enables independent researchers to reproduce experiments in â‰¤30 minutes
- ğŸ” Includes environment setup, dependencies, execution steps, and expected outputs

### Output Structure

```
my_experiment/
â”œâ”€â”€ paper/
â”‚   â”œâ”€â”€ main.pdf              # Compiled PDF (if pdflatex available)
â”‚   â”œâ”€â”€ main.tex              # LaTeX source
â”‚   â”œâ”€â”€ main.md               # Markdown intermediate
â”‚   â”œâ”€â”€ figures/
â”‚   â”‚   â”œâ”€â”€ metric_comparison_execution_time.pdf
â”‚   â”‚   â”œâ”€â”€ metric_comparison_execution_time.png
â”‚   â”‚   â”œâ”€â”€ metric_comparison_cost.pdf
â”‚   â”‚   â”œâ”€â”€ metric_comparison_cost.png
â”‚   â”‚   â””â”€â”€ statistical_significance.pdf
â”‚   â””â”€â”€ acm_sigsoft/          # ACM template files
â””â”€â”€ README.md                 # Enhanced with Reproduction Guide
```

### Requirements

**System:**
- Python 3.11+ with matplotlib, numpy, pandas
- Pandoc â‰¥2.0 (for Markdownâ†’LaTeX conversion)
- pdflatex (optional, for PDF compilation)

**API:**
- OpenAI API key (set `OPENAI_API_KEY` environment variable)
- Note: Separate from experiment API keys; used only for prose generation

**Installation:**
```bash
# Install Pandoc (Ubuntu/Debian)
sudo apt-get install pandoc texlive-latex-base texlive-latex-extra

# Install Pandoc (macOS)
brew install pandoc
brew install --cask mactex-no-gui  # optional, for PDF

# Verify
pandoc --version  # Should be â‰¥2.0
```

### Best Practices

âš ï¸ **Important**: AI-generated prose requires manual review:
1. âœ… Verify all claims match experimental data
2. âœ… Fill citation placeholders with proper references
3. âœ… Review interpretations for accuracy (AI may overclaim)
4. âœ… Check statistical reporting (p-values, effect sizes)
5. âœ… Validate figure captions and descriptions

**See [Paper Generation Guide](docs/paper_generation_guide.md) for detailed usage and troubleshooting.**

---

## Metrics Reference

### Quality Metrics (5)
- **AUTR** (Automated User Testing Rate): Autonomy rate = 1 - (HIT/UTT), measuring independence from human intervention [0-1]
- **Q\*** (Quality Star): Composite score = 0.4Â·ESR + 0.3Â·(CRUDe/12) + 0.3Â·MC [0-1]
- **ESR** (Emerging State Rate): Successful incremental evolution steps [0-1]
- **CRUDe** (CRUD Evolution): CRUD operation coverage [0-12]
- **MC** (Model Call Efficiency): Inverse normalized API calls [0-1]

### Efficiency Metrics (6)
- **TOK_IN** (Input Tokens): Total tokens sent to LLM APIs
- **TOK_OUT** (Output Tokens): Total tokens received from LLM APIs
- **T_WALL** (Wall-Clock Time): Total elapsed time (seconds)
- **T_USER** (User Time): CPU time in user mode (seconds)
- **T_CPU** (System Time): CPU time in kernel mode (seconds)
- **AEI** (API Efficiency Index): AUTR / log(1 + TOK_IN)

### Reliability Metrics (3)
- **ZDI** (Zero-Downtime Incidents): Count of availability failures
- **RTE** (Runtime Errors): Proportion of steps with crashes [0-1]
- **MCI** (Model Call Interruptions): Count of API retry events

### Process Metrics (2)
- **ITR** (Iterations): Number of framework invocations
- **DPL** (Deployment Success): Binary deployment outcome {0, 1}

See [Metrics Guide](docs/metrics.md) for detailed definitions, formulas, and interpretation.

## Statistical Analysis

The framework automatically performs:

1. **Aggregate Statistics**: Mean, median, std, 95% bootstrap CI for each metric
2. **Hypothesis Testing**: Kruskal-Wallis H-test for group differences (p < 0.05)
3. **Pairwise Comparisons**: Dunn-Å idÃ¡k corrected Mann-Whitney U tests
4. **Effect Sizes**: Cliff's delta (non-parametric effect magnitude)
5. **Outlier Detection**: Values >3Ïƒ from median
6. **Convergence Detection**: Stop when CI half-width < 10% of mean

Results output as markdown report with publication-ready SVG visualizations.

## Reproducibility

The framework ensures deterministic execution through:

- **Fixed Seeds**: Python random, NumPy seeds set per run
- **Commit Verification**: SHA verification of framework repository state
- **HITL Logging**: SHA-1 hashing of human-in-the-loop responses
- **Byte-for-Byte Validation**: Reproducibility test script for output comparison
- **Temperature Zero**: LLM sampling with temperature=0.0, top_p=1.0

See [Architecture Guide](docs/architecture.md#reproducibility) for details.

## License

This project is licensed under CC BY 4.0 - see [LICENSE](LICENSE) file for details.

## Citation

If you use this framework in your research, please cite:

```bibtex
@software{genai-devbench_2025,
  title={GenAI-DevBench},
  author={GESAD Lab},
  year={2025},
  url={https://github.com/nosredna123/genai-devbench}
}
```

## Contributing

This is a research artifact. For questions or issues, please open a GitHub issue.
