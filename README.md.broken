# GenAI-DevBench - Experiment Generator# GenAI-DevBench - Experiment Generator# GenAI-DevBench



**Generate standalone experiment projects for evaluating GenAI software development frameworks**



GenAI-DevBench is a powerful generator that creates complete, self-contained experiment projects for rigorous benchmarking of AI-powered development frameworks (BAES, ChatDev, GitHub Copilot). Each generated experiment is a fully independent Git repository with everything needed to run sophisticated comparative evaluations.**Generate standalone experiment projects for evaluating GenAI software development frameworks****Rigorous benchmarking for GenAI-powered software development**



---



## ğŸ¯ What Does This Do?GenAI-DevBench is a powerful generator that creates complete, self-contained experiment projects for rigorous benchmarking of AI-powered development frameworks (BAES, ChatDev, GitHub Copilot). Each generated experiment is a fully independent Git repository with everything needed to run sophisticated comparative evaluations.This framework executes reproducible experiments to compare three LLM-driven frameworks (BAEs, ChatDev, GitHub Spec-kit) across a six-step academic CRUD evolution scenario. It provides deterministic orchestration, reproducible metrics collection, and automated statistical analysis.



This generator creates **standalone experiment projects** that:



- âœ… Are completely independent (no dependencies on this generator after creation)---## Features

- âœ… Include only the code and configurations needed for your specific setup

- âœ… Come with one-command setup and execution scripts

- âœ… Are initialized as Git repositories ready for version control

- âœ… Can be distributed, archived, or run on any machine with Python 3.9+## ğŸ¯ What Does This Do?- **Automated Execution**: Single-command orchestration of complete experiment runs



**Think of it as:** A project template generator specifically designed for AI framework evaluation experiments.- **Deterministic HITL**: Fixed clarification responses with SHA-1 verification for reproducible results



---This generator creates **standalone experiment projects** that:- **Comprehensive Metrics**: 16 metrics including autonomy (AUTR), quality (Q*, ESR, CRUDe, MC), efficiency (tokens, time, AEI), and reliability (ZDI, RTE, MCI)



## âš¡ Quick Start- **Statistical Analysis**: Bootstrap confidence intervals, Kruskal-Wallis tests, Dunn-Å idÃ¡k pairwise comparisons, Cliff's Î´ effect sizes



### 1. Install the Generator- âœ… Are completely independent (no dependencies on this generator after creation)- **Convergence Detection**: Automatic stopping when 95% CI half-width < 10% of mean (min 5, max 25 runs)



```bash- âœ… Include only the code and configurations needed for your specific setup- **Publication-Quality Visualizations**: Radar charts, Pareto plots, timeline charts (SVG export)

git clone https://github.com/nosredna123/genai-devbench.git

cd genai-devbench- âœ… Come with one-command setup and execution scripts- **Complete Archival**: Full workspace preservation with SHA-256 verification

pip install -r requirements.txt

```- âœ… Are initialized as Git repositories ready for version control- **Isolation**: Per-run UUID workspaces with deterministic seeding



### 2. Generate Your First Experiment- âœ… Can be distributed, archived, or run on any machine with Python 3.9+



**Interactive Mode (Recommended):**## Quick Start

```bash

python scripts/new_experiment.py**Think of it as:** A project template generator specifically designed for AI framework evaluation experiments.

```

### Prerequisites

Follow the prompts to configure your experiment.

---

**Or use CLI Mode:**

```bash- **Python 3.11+** (required)

python scripts/new_experiment.py \

  --name my_evaluation \## âš¡ Quick Start- **Git** (required)

  --model gpt-4o \

  --frameworks baes,chatdev \- **API Keys**: OpenAI for BAEs/ChatDev, GitHub token for Spec-kit

  --runs 50

```### 1. Install the Generator- **System**: 8GB RAM, 10GB disk space (per multi-framework run)



### 3. Run the Generated Experiment



```bash```bash### Installation

cd experiments/my_evaluation

./setup.shgit clone https://github.com/nosredna123/genai-devbench.git

# Edit .env with your OpenAI API keys

./run.shcd genai-devbench```bash

```

pip install -r requirements.txt# Clone repository

That's it! Results will be in `./runs/`

```git clone https://github.com/nosredna123/genai-devbench.git

---

cd genai-devbench

## ğŸ“¦ What Gets Generated?

### 2. Generate Your First Experiment

Each experiment is a complete, standalone project:

# Create virtual environment (recommended)

```

my_evaluation/                    # Fully independent project**Interactive Mode (Recommended):**python3 -m venv venv

â”œâ”€â”€ .git/                        # Git repository

â”œâ”€â”€ setup.sh                     # One-command setup script```bashsource venv/bin/activate  # On Windows: venv\Scripts\activate

â”œâ”€â”€ run.sh                       # One-command execution script

â”œâ”€â”€ config.yaml                  # Experiment configurationpython scripts/new_experiment.py

â”œâ”€â”€ README.md                    # Standalone documentation

â”œâ”€â”€ requirements.txt             # Python dependencies (minimal)```# Install dependencies

â”œâ”€â”€ .env.example                 # API key template

â”œâ”€â”€ .gitignore                   # Git ignore patternspip install -r requirements.txt

â”œâ”€â”€ src/                         # Complete source code

â”‚   â”œâ”€â”€ adapters/               # Framework adapters (only enabled ones)Follow the prompts to configure your experiment.

â”‚   â”œâ”€â”€ orchestrator/           # Experiment orchestration

â”‚   â”œâ”€â”€ analysis/               # Results analysis & visualization# Configure API keys

â”‚   â”œâ”€â”€ utils/                  # Utilities

â”‚   â”œâ”€â”€ main.py                 # Entry point**Or use CLI Mode:**cp .env.example .env

â”‚   â””â”€â”€ setup_frameworks.py     # Framework repository setup

â”œâ”€â”€ config/                      # Framework configurations```bash# Edit .env with your API keys:

â”‚   â”œâ”€â”€ prompts/                # Framework-specific prompts

â”‚   â””â”€â”€ hitl/                   # Human-in-the-loop specspython scripts/new_experiment.py \#   BAES_API_KEY=sk-your-openai-key

â”œâ”€â”€ runs/                        # Experiment outputs (generated)

â”œâ”€â”€ analysis/                    # Analysis results (generated)  --name my_evaluation \#   CHATDEV_API_KEY=sk-your-openai-key

â””â”€â”€ frameworks/                  # Framework repositories (generated)

```  --model gpt-4o \#   GHSPEC_API_KEY=ghp_your-github-token



**Key Point:** The generated project has ZERO dependencies on this generator. It's completely standalone.  --frameworks baes,chatdev \```



---  --runs 50



## ğŸ”§ Configuration Options```### Your First Experiment (5 Minutes)



When generating an experiment, you configure:



### Models### 3. Run the Generated ExperimentThe framework supports multiple independent experiments with organized, isolated outputs:

- **gpt-4o** - Latest GPT-4 Omni (recommended)

- **gpt-4o-mini** - Smaller, faster GPT-4 Omni

- **gpt-4-turbo** - GPT-4 Turbo

- **gpt-4** - GPT-4 (legacy)```bash```bash

- **gpt-3.5-turbo** - GPT-3.5 Turbo

cd experiments/my_evaluation# 1. Create an experiment

### Frameworks

- **BAES** - Behavior-driven Agile Engineering System./setup.shpython scripts/new_experiment.py \

- **ChatDev** - Communicative agents for software development

- **GitHub Copilot** - AI pair programmer (via spec-kit)# Edit .env with your OpenAI API keys    --name my_first_experiment \



**Select one, two, or all three** - the generator only includes what you need../run.sh    --model gpt-4o \



### Runs```    --frameworks baes \

- **Min/Max runs per framework** - Determines statistical confidence

- **Automatic stopping** - Based on confidence interval convergence    --runs 10

- Default: 5-50 runs per framework

That's it! Results will be in `./runs/`

### Metrics

- **Functional Correctness** - Does the code work?# 2. Run the experiment

- **Design Quality** - Architecture and patterns

- **Code Maintainability** - Readability and structure---python scripts/run_experiment.py my_first_experiment

- **API Usage** - Token consumption and costs



---

## ğŸ“¦ What Gets Generated?# 3. Analyze results

## ğŸ“– Generator Usage

./runners/analyze_results.sh my_first_experiment

### Interactive Mode

Each experiment is a complete, standalone project:

```bash

python scripts/new_experiment.py# 4. View report

```

```cat experiments/my_first_experiment/analysis/report.md

Guided wizard walks you through:

1. Experiment namemy_evaluation/                    # Fully independent project```

2. Model selection

3. Framework selectionâ”œâ”€â”€ .git/                        # Git repository

4. Run configuration

5. Optional template selectionâ”œâ”€â”€ setup.sh                     # One-command setup script**What happened:**



### CLI Modeâ”œâ”€â”€ run.sh                       # One-command execution script- âœ… Created isolated experiment directory



```bashâ”œâ”€â”€ config.yaml                  # Experiment configuration- âœ… Ran 10 independent BAEs executions

python scripts/new_experiment.py \

  --name <experiment_name> \â”œâ”€â”€ README.md                    # Standalone documentation- âœ… Generated statistical analysis with confidence intervals

  --model <model> \

  --frameworks <framework1,framework2> \â”œâ”€â”€ requirements.txt             # Python dependencies (minimal)- âœ… Produced comprehensive report with recommendations

  --runs <max_runs_per_framework> \

  [--experiments-dir <custom_directory>]â”œâ”€â”€ .env.example                 # API key template

```

â”œâ”€â”€ .gitignore                   # Git ignore patterns**See [Quick Start Guide](docs/QUICKSTART.md) for detailed walkthrough.**

**Examples:**

â”œâ”€â”€ src/                         # Complete source code

```bash

# Single framework evaluationâ”‚   â”œâ”€â”€ adapters/               # Framework adapters (only enabled ones)### Compare Multiple Experiments

python scripts/new_experiment.py \

  --name baes_eval \â”‚   â”œâ”€â”€ orchestrator/           # Experiment orchestration

  --model gpt-4o-mini \

  --frameworks baes \â”‚   â”œâ”€â”€ analysis/               # Results analysis & visualization```bash

  --runs 10

â”‚   â”œâ”€â”€ utils/                  # Utilities# Create baseline

# Compare all frameworks

python scripts/new_experiment.py \â”‚   â”œâ”€â”€ main.py                 # Entry pointpython scripts/new_experiment.py \

  --name full_comparison \

  --model gpt-4o \â”‚   â””â”€â”€ setup_frameworks.py     # Framework repository setup    --name baseline_gpt4o \

  --frameworks baes,chatdev,ghspec \

  --runs 50â”œâ”€â”€ config/                      # Framework configurations    --model gpt-4o \



# Custom output directoryâ”‚   â”œâ”€â”€ prompts/                # Framework-specific prompts    --frameworks baes \

python scripts/new_experiment.py \

  --name remote_eval \â”‚   â””â”€â”€ hitl/                   # Human-in-the-loop specs    --runs 10

  --model gpt-4o \

  --frameworks chatdev \â”œâ”€â”€ runs/                        # Experiment outputs (generated)

  --runs 25 \

  --experiments-dir /mnt/experimentsâ”œâ”€â”€ analysis/                    # Analysis results (generated)# Create variant

```

â””â”€â”€ frameworks/                  # Framework repositories (generated)python scripts/new_experiment.py \

---

```    --name variant_gpt4omini \

## ğŸš€ Using Generated Experiments

    --model gpt-4o-mini \

### Setup

**Key Point:** The generated project has ZERO dependencies on this generator. It's completely standalone.    --frameworks baes \

```bash

cd experiments/<experiment_name>    --runs 10

./setup.sh

```---



This creates a virtual environment, installs dependencies, and clones framework repositories.# Run both



### Configure API Keys## ğŸ”§ Configuration Optionspython scripts/run_experiment.py baseline_gpt4o



```bashpython scripts/run_experiment.py variant_gpt4omini

nano .env  # or your preferred editor

```When generating an experiment, you configure:



Add your OpenAI API keys (one per framework for separate usage tracking):# Analyze both



```bash### Models./runners/analyze_results.sh baseline_gpt4o

OPENAI_API_KEY_BAES=sk-...

OPENAI_API_KEY_CHATDEV=sk-...- **gpt-4o** - Latest GPT-4 Omni (recommended)./runners/analyze_results.sh variant_gpt4omini

OPENAI_API_KEY_GHSPEC=sk-...

```- **gpt-4o-mini** - Smaller, faster GPT-4 Omni



### Run- **gpt-4-turbo** - GPT-4 Turbo# Compare



```bash- **gpt-4** - GPT-4 (legacy)diff experiments/baseline_gpt4o/analysis/report.md \

./run.sh

```- **gpt-3.5-turbo** - GPT-3.5 Turbo     experiments/variant_gpt4omini/analysis/report.md



Experiment executes automatically. Results saved to `./runs/<framework>/<run_id>/````



### Analyze### Frameworks



```bash- **BAES** - Behavior-driven Agile Engineering System**See [Comparison Guide](docs/COMPARISON_GUIDE.md) for statistical comparison techniques.**

source venv/bin/activate

python -m src.analysis.report_generator- **ChatDev** - Communicative agents for software development

```

- **GitHub Copilot** - AI pair programmer (via spec-kit)### Legacy Single-Run Mode

Generates comprehensive analysis report with visualizations.



---

**Select one, two, or all three** - the generator only includes what you need.The framework still supports legacy single-run mode (deprecated):

## ğŸ—ï¸ Architecture



### Generator Components

### Runs```bash

1. **Artifact Collector** - Determines which files to include based on config

2. **Import Rewriter** - Adapts Python imports for standalone operation- **Min/Max runs per framework** - Determines statistical confidence# Legacy: Execute single framework run (15-30 minutes)

3. **Script Generator** - Creates setup.sh, run.sh, README.md

4. **Dependency Analyzer** - Generates minimal requirements.txt- **Automatic stopping** - Based on confidence interval convergence# DEPRECATED: Use new_experiment.py instead

5. **Standalone Generator** - Orchestrates the entire generation process

- Default: 5-50 runs per framework./runners/run_experiment.sh baes

### Generated Experiment Flow



```

Config â†’ Artifact Collection â†’ File Copying â†’ Script Generation### Metrics# Legacy: View metrics from a run

    â†’ Git Init â†’ Standalone Project

```- **Functional Correctness** - Does the code work?cat runs/baes/<run-id>/metrics.json



### Experiment Execution Flow- **Design Quality** - Architecture and patterns



```- **Code Maintainability** - Readability and structure# Legacy: Analyze legacy runs

Setup â†’ Framework Cloning â†’ Configuration â†’ Run Loop

    â†’ Per-Framework Execution â†’ Metrics Collection- **API Usage** - Token consumption and costs./runners/analyze_results.sh ./analysis_output

    â†’ Statistical Analysis â†’ Visualization

``````



------



## ğŸ“Š Metrics & Analysis**Note:** Legacy mode is maintained for backward compatibility but new workflows should use the multi-experiment system.



Generated experiments collect comprehensive metrics:## ğŸ“– Generator Usage



### Autonomy Metrics## Documentation

- **AUTR** - Autonomous Task Resolution

- **HITL** - Human-in-the-Loop interactions### Interactive Mode

- **MCI** - Manual Code Interventions

### Getting Started

### Quality Metrics

- **Q*** - Quality star rating```bash- **[Quick Start Guide](docs/QUICKSTART.md)** â­ **Start here!** - Create your first experiment in 5 minutes

- **ESR** - External Service Readiness

- **CRUDe** - CRUD endpoint coveragepython scripts/new_experiment.py- **[Workflows Guide](docs/WORKFLOWS.md)** - Common usage patterns and real-world scenarios

- **MC** - McCabe complexity

```- **[Comparison Guide](docs/COMPARISON_GUIDE.md)** - Statistical comparison of experiments

### Efficiency Metrics

- **TOK_IN/TOK_OUT** - Token consumption- **[Best Practices Guide](docs/BEST_PRACTICES.md)** - Recommendations for effective experimentation

- **T_WALL** - Wall clock time

- **COST_USD** - API costsGuided wizard walks you through:

- **AEI** - Autonomy Efficiency Index

1. Experiment name### Multi-Experiment System

### Reliability Metrics

- **ZDI** - Zero-defect installations2. Model selection- **[Architecture Guide](docs/architecture.md)** - System design and multi-experiment components

- **RTE** - Runtime errors

- **BR** - Build success rate3. Framework selection- **[Configuration Reference](docs/configuration_reference.md)** - Complete config schema and examples



### Analysis Features4. Run configuration- **[Validation System](docs/validation_system.md)** - Configuration validation reference

- Bootstrap confidence intervals (95%)

- Kruskal-Wallis non-parametric tests5. Optional template selection- **[Troubleshooting Guide](docs/troubleshooting.md)** - Common issues and solutions

- Dunn-Å idÃ¡k pairwise comparisons

- Cliff's Î´ effect sizes

- Automatic convergence detection

### CLI Mode### Metrics and Analysis

---

- **[Metrics Guide](docs/metrics.md)** - Complete reference for all 16 metrics

## ğŸ” Advanced Features

```bash- **[Statistical Power Analysis](docs/statistical_power_analysis.md)** - Sample size calculations

### Template-based Generation

python scripts/new_experiment.py \- **[API Usage Reconciliation](docs/reconcile_usage_guide.md)** - Accurate cost tracking

Generate experiments based on existing ones:

  --name <experiment_name> \

```bash

python scripts/new_experiment.py  --model <model> \### Legacy Documentation

# Select "yes" when asked about templates

# Choose an existing experiment as base  --frameworks <framework1,framework2> \- [Original Quickstart](docs/quickstart.md) - Legacy single-run quickstart (deprecated)

```

  --runs <max_runs_per_framework> \- [Feature Specification](specs/001-baes-experiment-framework/spec.md) - Requirements and user stories

### Custom Output Directories

  [--output-dir <custom_directory>]- [Implementation Plan](specs/001-baes-experiment-framework/plan.md) - Technical design

Generate experiments outside the default `experiments/` folder:

```- [Research Decisions](specs/001-baes-experiment-framework/research.md) - Design rationale

```bash

python scripts/new_experiment.py \

  --name external_eval \

  --model gpt-4o \**Examples:**### Testing

  --frameworks baes \

  --runs 20 \- **Test Suite:** Comprehensive unit tests with 100% pass rate

  --experiments-dir /path/to/custom/location

``````bash- **Run Tests:** `pytest tests/unit/test_report_generation.py -v`



### Distributed Experiments# Single framework evaluation- **Test Coverage:** 26 tests covering validation, dynamics, and edge cases



Generated experiments are portable:python scripts/new_experiment.py \- **Execution Time:** < 2 seconds for full suite



```bash  --name baes_eval \

# Generate locally

python scripts/new_experiment.py --name dist_eval --model gpt-4o --frameworks baes --runs 50  --model gpt-4o-mini \## Architecture



# Archive  --frameworks baes \

tar -czf dist_eval.tar.gz experiments/dist_eval/

  --runs 10```

# Transfer to remote machine

scp dist_eval.tar.gz remote:/experiments/genai-devbench/



# Extract and run remotely# Compare all frameworksâ”œâ”€â”€ experiments/                 # ğŸ†• Multi-experiment storage (isolated)

ssh remote

cd /experimentspython scripts/new_experiment.py \â”‚   â”œâ”€â”€ <experiment_name>/       # Individual experiment directory

tar -xzf dist_eval.tar.gz

cd dist_eval  --name full_comparison \â”‚   â”‚   â”œâ”€â”€ config.yaml          # Experiment configuration (immutable)

./setup.sh

# Configure .env  --model gpt-4o \â”‚   â”‚   â”œâ”€â”€ README.md            # Auto-generated documentation

./run.sh

```  --frameworks baes,chatdev,ghspec \â”‚   â”‚   â”œâ”€â”€ runs/                # Run outputs for this experiment



---  --runs 50â”‚   â”‚   â”‚   â”œâ”€â”€ manifest.json    # Run tracking and metadata



## ğŸ“š Documentationâ”‚   â”‚   â”‚   â”œâ”€â”€ baes/            # Framework-specific runs



- **[Generator Architecture](docs/STANDALONE_EXPERIMENT_DESIGN.md)** - Design overview# Custom output directoryâ”‚   â”‚   â”‚   â”œâ”€â”€ chatdev/

- **[Transformation Plan](docs/GENERATOR_TRANSFORMATION_PLAN.md)** - Implementation details

- **[Validation Report](docs/TRANSFORMATION_VALIDATION.md)** - Testing and validationpython scripts/new_experiment.py \â”‚   â”‚   â”‚   â””â”€â”€ ghspec/

- **[Metrics Documentation](docs/metrics.md)** - Comprehensive metric definitions

- **[Configuration Reference](docs/configuration_reference.md)** - All config options  --name remote_eval \â”‚   â”‚   â”œâ”€â”€ analysis/            # Analysis outputs



---  --model gpt-4o \â”‚   â”‚   â”‚   â”œâ”€â”€ report.md        # Statistical report



## ğŸ¤ Contributing  --frameworks chatdev \â”‚   â”‚   â”‚   â””â”€â”€ visualizations/  # Charts and graphs



Contributions welcome! Areas for improvement:  --runs 25 \â”‚   â”‚   â””â”€â”€ .meta/               # Metadata



- Additional framework adapters  --output-dir /mnt/experimentsâ”‚   â”‚       â””â”€â”€ config.hash      # Config integrity verification

- New metrics implementations

- Enhanced analysis visualizations```â”‚   â””â”€â”€ .experiment_registry.json # ğŸ†• Global experiment registry

- Documentation improvements

- Bug fixes and optimizationsâ”œâ”€â”€ scripts/                     # ğŸ†• Experiment management



------â”‚   â”œâ”€â”€ new_experiment.py        # Create experiments (interactive/CLI)



## ğŸ“„ Licenseâ”‚   â””â”€â”€ run_experiment.py        # Run experiments (high-level wrapper)



See [LICENSE](LICENSE) file.## ğŸš€ Using Generated Experimentsâ”œâ”€â”€ config/                      # Experiment configuration



---â”‚   â”œâ”€â”€ experiment.yaml          # Framework configs, timeouts, seeds



## ğŸ†˜ Troubleshooting### Setupâ”‚   â”œâ”€â”€ prompts/                 # 6-step CRUD evolution scenario



### Generator Issuesâ”‚   â””â”€â”€ hitl/                    # HITL clarification templates



**Problem:** Import errors when running generator  ```bashâ”œâ”€â”€ src/

**Solution:** Ensure you're in the project root: `cd genai-devbench`

cd experiments/<experiment_name>â”‚   â”œâ”€â”€ orchestrator/            # Execution coordination

**Problem:** Generation fails  

**Solution:** Check Python version (3.9+ required), verify all dependencies installed./setup.shâ”‚   â”‚   â”œâ”€â”€ runner.py            # OrchestratorRunner (single/multi)



### Generated Experiment Issues```â”‚   â”‚   â”œâ”€â”€ metrics_collector.py # 16 metrics computation



**Problem:** `./setup.sh` fails  â”‚   â”‚   â”œâ”€â”€ validator.py         # CRUD/UI/downtime validation

**Solution:** Check Python 3.9+ installed, verify internet connection for package downloads

This creates a virtual environment, installs dependencies, and clones framework repositories.â”‚   â”‚   â””â”€â”€ archiver.py          # tar.gz with SHA-256

**Problem:** `./run.sh` fails with API key errors  

**Solution:** Verify `.env` file exists and contains valid API keys starting with `sk-`â”‚   â”œâ”€â”€ adapters/                # Framework integrations



**Problem:** Framework cloning fails  ### Configure API Keysâ”‚   â”‚   â”œâ”€â”€ base_adapter.py      # Abstract BaseAdapter

**Solution:** Check internet connection, verify git installed

â”‚   â”‚   â”œâ”€â”€ baes_adapter.py      # BAEs implementation

**Problem:** Import errors in generated experiment  

**Solution:** Run `./setup.sh` again, activate venv: `source venv/bin/activate````bashâ”‚   â”‚   â”œâ”€â”€ chatdev_adapter.py   # ChatDev implementation



---nano .env  # or your preferred editorâ”‚   â”‚   â””â”€â”€ ghspec_adapter.py    # GitHub Spec-kit implementation



## ğŸ—ºï¸ Roadmap```â”‚   â”œâ”€â”€ analysis/                # Statistical analysis



### Planned Featuresâ”‚   â”‚   â”œâ”€â”€ statistics.py        # Tests, effect sizes, reports



- [ ] Docker containerization option for generated experimentsAdd your OpenAI API keys (one per framework for separate usage tracking):â”‚   â”‚   â”œâ”€â”€ stopping_rule.py     # Bootstrap CI convergence

- [ ] CI/CD workflow generation (GitHub Actions, GitLab CI)

- [ ] Additional framework support (AutoGen, CrewAI, etc.)â”‚   â”‚   â””â”€â”€ visualizations.py    # Radar, Pareto, timeline charts

- [ ] Web UI for experiment generation

- [ ] Experiment result aggregation across multiple runs```bashâ”‚   â””â”€â”€ utils/                   # Utilities

- [ ] Cloud deployment templates (AWS, Azure, GCP)

OPENAI_API_KEY_BAES=sk-...â”‚       â”œâ”€â”€ logger.py            # Structured JSON logging

---

OPENAI_API_KEY_CHATDEV=sk-...â”‚       â”œâ”€â”€ config_loader.py     # YAML validation

**Ready to start?** Run `python scripts/new_experiment.py` and create your first experiment! ğŸš€

OPENAI_API_KEY_GHSPEC=sk-...â”‚       â”œâ”€â”€ isolation.py         # Workspace management

```â”‚       â”œâ”€â”€ api_client.py        # OpenAI Usage API verification

â”‚       â”œâ”€â”€ experiment_paths.py  # ğŸ†• Path resolution for experiments

### Runâ”‚       â””â”€â”€ experiment_registry.py # ğŸ†• Global experiment tracking

â”œâ”€â”€ runners/                     # Entry point scripts

```bashâ”‚   â”œâ”€â”€ run_experiment.sh        # âš ï¸ DEPRECATED (use scripts/run_experiment.py)

./run.shâ”‚   â”œâ”€â”€ analyze_results.sh       # Analysis pipeline (experiment-aware)

```â”‚   â””â”€â”€ reconcile_usage.sh       # ğŸ†• API usage reconciliation

â”œâ”€â”€ runs/                        # ğŸ—„ï¸ Legacy run outputs (gitignored, deprecated)

Experiment executes automatically. Results saved to `./runs/<framework>/<run_id>/`â”œâ”€â”€ tests/                       # Test suite

â”‚   â”œâ”€â”€ unit/                    # Unit tests

### Analyzeâ”‚   â”œâ”€â”€ integration/             # Integration tests

â”‚   â””â”€â”€ contract/                # Contract tests

```bashâ””â”€â”€ docs/                        # Documentation

source venv/bin/activate    â”œâ”€â”€ QUICKSTART.md            # ğŸ†• 5-minute getting started guide

python -m src.analysis.report_generator    â”œâ”€â”€ WORKFLOWS.md             # ğŸ†• Common usage patterns

```    â”œâ”€â”€ COMPARISON_GUIDE.md      # ğŸ†• Statistical comparison guide

    â”œâ”€â”€ BEST_PRACTICES.md        # ğŸ†• Recommendations and tips

Generates comprehensive analysis report with visualizations.    â””â”€â”€ ...                      # Other documentation

```

---

**Key Changes:**

## ğŸ—ï¸ Architecture- ğŸ†• **experiments/** directory: All experiments organized by name

- ğŸ†• **Experiment Registry**: Global tracking of all experiments

### Generator Components- ğŸ†• **Config Hash**: Immutable configuration with integrity verification

- ğŸ†• **Isolated Runs**: Each experiment has its own runs and analysis

1. **Artifact Collector** - Determines which files to include based on config- âš ï¸ **Deprecated**: Legacy `runners/run_experiment.sh` (use `scripts/run_experiment.py`)

2. **Import Rewriter** - Adapts Python imports for standalone operation- ğŸ—„ï¸ **Legacy**: Old `runs/` directory maintained for backward compatibility

3. **Script Generator** - Creates setup.sh, run.sh, README.md

4. **Dependency Analyzer** - Generates minimal requirements.txt## Metrics Reference

5. **Standalone Generator** - Orchestrates the entire generation process

### Quality Metrics (5)

### Generated Experiment Flow- **AUTR** (Automated User Testing Rate): Autonomy rate = 1 - (HIT/UTT), measuring independence from human intervention [0-1]

- **Q\*** (Quality Star): Composite score = 0.4Â·ESR + 0.3Â·(CRUDe/12) + 0.3Â·MC [0-1]

```- **ESR** (Emerging State Rate): Successful incremental evolution steps [0-1]

Config â†’ Artifact Collection â†’ File Copying â†’ Script Generation- **CRUDe** (CRUD Evolution): CRUD operation coverage [0-12]

    â†’ Git Init â†’ Standalone Project- **MC** (Model Call Efficiency): Inverse normalized API calls [0-1]

```

### Efficiency Metrics (6)

### Experiment Execution Flow- **TOK_IN** (Input Tokens): Total tokens sent to LLM APIs

- **TOK_OUT** (Output Tokens): Total tokens received from LLM APIs

```- **T_WALL** (Wall-Clock Time): Total elapsed time (seconds)

Setup â†’ Framework Cloning â†’ Configuration â†’ Run Loop- **T_USER** (User Time): CPU time in user mode (seconds)

    â†’ Per-Framework Execution â†’ Metrics Collection- **T_CPU** (System Time): CPU time in kernel mode (seconds)

    â†’ Statistical Analysis â†’ Visualization- **AEI** (API Efficiency Index): AUTR / log(1 + TOK_IN)

```

### Reliability Metrics (3)

---- **ZDI** (Zero-Downtime Incidents): Count of availability failures

- **RTE** (Runtime Errors): Proportion of steps with crashes [0-1]

## ğŸ“Š Metrics & Analysis- **MCI** (Model Call Interruptions): Count of API retry events



Generated experiments collect comprehensive metrics:### Process Metrics (2)

- **ITR** (Iterations): Number of framework invocations

### Autonomy Metrics- **DPL** (Deployment Success): Binary deployment outcome {0, 1}

- **AUTR** - Autonomous Task Resolution

- **HITL** - Human-in-the-Loop interactionsSee [Metrics Guide](docs/metrics.md) for detailed definitions, formulas, and interpretation.

- **MCI** - Manual Code Interventions

## Statistical Analysis

### Quality Metrics

- **Q*** - Quality star ratingThe framework automatically performs:

- **ESR** - External Service Readiness

- **CRUDe** - CRUD endpoint coverage1. **Aggregate Statistics**: Mean, median, std, 95% bootstrap CI for each metric

- **MC** - McCabe complexity2. **Hypothesis Testing**: Kruskal-Wallis H-test for group differences (p < 0.05)

3. **Pairwise Comparisons**: Dunn-Å idÃ¡k corrected Mann-Whitney U tests

### Efficiency Metrics4. **Effect Sizes**: Cliff's delta (non-parametric effect magnitude)

- **TOK_IN/TOK_OUT** - Token consumption5. **Outlier Detection**: Values >3Ïƒ from median

- **T_WALL** - Wall clock time6. **Convergence Detection**: Stop when CI half-width < 10% of mean

- **COST_USD** - API costs

- **AEI** - Autonomy Efficiency IndexResults output as markdown report with publication-ready SVG visualizations.



### Reliability Metrics## Reproducibility

- **ZDI** - Zero-defect installations

- **RTE** - Runtime errorsThe framework ensures deterministic execution through:

- **BR** - Build success rate

- **Fixed Seeds**: Python random, NumPy seeds set per run

### Analysis Features- **Commit Verification**: SHA verification of framework repository state

- Bootstrap confidence intervals (95%)- **HITL Logging**: SHA-1 hashing of human-in-the-loop responses

- Kruskal-Wallis non-parametric tests- **Byte-for-Byte Validation**: Reproducibility test script for output comparison

- Dunn-Å idÃ¡k pairwise comparisons- **Temperature Zero**: LLM sampling with temperature=0.0, top_p=1.0

- Cliff's Î´ effect sizes

- Automatic convergence detectionSee [Architecture Guide](docs/architecture.md#reproducibility) for details.



---## License



## ğŸ” Advanced FeaturesThis project is licensed under CC BY 4.0 - see [LICENSE](LICENSE) file for details.



### Template-based Generation## Citation



Generate experiments based on existing ones:If you use this framework in your research, please cite:



```bash```bibtex

python scripts/new_experiment.py@software{genai-devbench_2025,

# Select "yes" when asked about templates  title={GenAI-DevBench},

# Choose an existing experiment as base  author={GESAD Lab},

```  year={2025},

  url={https://github.com/nosredna123/genai-devbench}

### Custom Output Directories}

```

Generate experiments outside the default `experiments/` folder:

## Contributing

```bash

python scripts/new_experiment.py \This is a research artifact. For questions or issues, please open a GitHub issue.

  --name external_eval \
  --model gpt-4o \
  --frameworks baes \
  --runs 20 \
  --output-dir /path/to/custom/location
```

### Distributed Experiments

Generated experiments are portable:

```bash
# Generate locally
python scripts/new_experiment.py --name dist_eval --model gpt-4o --frameworks baes --runs 50

# Archive
tar -czf dist_eval.tar.gz experiments/dist_eval/

# Transfer to remote machine
scp dist_eval.tar.gz remote:/experiments/

# Extract and run remotely
ssh remote
cd /experiments
tar -xzf dist_eval.tar.gz
cd dist_eval
./setup.sh
# Configure .env
./run.sh
```

---

## ğŸ“š Documentation

- **[Generator Architecture](docs/STANDALONE_EXPERIMENT_DESIGN.md)** - Design overview
- **[Transformation Plan](docs/GENERATOR_TRANSFORMATION_PLAN.md)** - Implementation details
- **[Metrics Documentation](docs/metrics.md)** - Comprehensive metric definitions
- **[Configuration Reference](docs/configuration_reference.md)** - All config options

---

## ğŸ¤ Contributing

Contributions welcome! Areas for improvement:

- Additional framework adapters
- New metrics implementations
- Enhanced analysis visualizations
- Documentation improvements
- Bug fixes and optimizations

---

## ğŸ“„ License

See [LICENSE](LICENSE) file.

---

## ğŸ†˜ Troubleshooting

### Generator Issues

**Problem:** Import errors when running generator  
**Solution:** Ensure you're in the project root: `cd genai-devbench`

**Problem:** Generation fails  
**Solution:** Check Python version (3.9+ required), verify all dependencies installed

### Generated Experiment Issues

**Problem:** `./setup.sh` fails  
**Solution:** Check Python 3.9+ installed, verify internet connection for package downloads

**Problem:** `./run.sh` fails with API key errors  
**Solution:** Verify `.env` file exists and contains valid API keys starting with `sk-`

**Problem:** Framework cloning fails  
**Solution:** Check internet connection, verify git installed

**Problem:** Import errors in generated experiment  
**Solution:** Run `./setup.sh` again, activate venv: `source venv/bin/activate`

---

## ğŸ—ºï¸ Roadmap

### Planned Features

- [ ] Docker containerization option for generated experiments
- [ ] CI/CD workflow generation (GitHub Actions, GitLab CI)
- [ ] Additional framework support (AutoGen, CrewAI, etc.)
- [ ] Web UI for experiment generation
- [ ] Experiment result aggregation across multiple runs
- [ ] Cloud deployment templates (AWS, Azure, GCP)

---

**Ready to start?** Run `python scripts/new_experiment.py` and create your first experiment! ğŸš€
