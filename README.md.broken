# GenAI-DevBench - Experiment Generator# GenAI-DevBench - Experiment Generator# GenAI-DevBench



**Generate standalone experiment projects for evaluating GenAI software development frameworks**



GenAI-DevBench is a powerful generator that creates complete, self-contained experiment projects for rigorous benchmarking of AI-powered development frameworks (BAES, ChatDev, GitHub Copilot). Each generated experiment is a fully independent Git repository with everything needed to run sophisticated comparative evaluations.**Generate standalone experiment projects for evaluating GenAI software development frameworks****Rigorous benchmarking for GenAI-powered software development**



---



## 🎯 What Does This Do?GenAI-DevBench is a powerful generator that creates complete, self-contained experiment projects for rigorous benchmarking of AI-powered development frameworks (BAES, ChatDev, GitHub Copilot). Each generated experiment is a fully independent Git repository with everything needed to run sophisticated comparative evaluations.This framework executes reproducible experiments to compare three LLM-driven frameworks (BAEs, ChatDev, GitHub Spec-kit) across a six-step academic CRUD evolution scenario. It provides deterministic orchestration, reproducible metrics collection, and automated statistical analysis.



This generator creates **standalone experiment projects** that:



- ✅ Are completely independent (no dependencies on this generator after creation)---## Features

- ✅ Include only the code and configurations needed for your specific setup

- ✅ Come with one-command setup and execution scripts

- ✅ Are initialized as Git repositories ready for version control

- ✅ Can be distributed, archived, or run on any machine with Python 3.9+## 🎯 What Does This Do?- **Automated Execution**: Single-command orchestration of complete experiment runs



**Think of it as:** A project template generator specifically designed for AI framework evaluation experiments.- **Deterministic HITL**: Fixed clarification responses with SHA-1 verification for reproducible results



---This generator creates **standalone experiment projects** that:- **Comprehensive Metrics**: 16 metrics including autonomy (AUTR), quality (Q*, ESR, CRUDe, MC), efficiency (tokens, time, AEI), and reliability (ZDI, RTE, MCI)



## ⚡ Quick Start- **Statistical Analysis**: Bootstrap confidence intervals, Kruskal-Wallis tests, Dunn-Šidák pairwise comparisons, Cliff's δ effect sizes



### 1. Install the Generator- ✅ Are completely independent (no dependencies on this generator after creation)- **Convergence Detection**: Automatic stopping when 95% CI half-width < 10% of mean (min 5, max 25 runs)



```bash- ✅ Include only the code and configurations needed for your specific setup- **Publication-Quality Visualizations**: Radar charts, Pareto plots, timeline charts (SVG export)

git clone https://github.com/nosredna123/genai-devbench.git

cd genai-devbench- ✅ Come with one-command setup and execution scripts- **Complete Archival**: Full workspace preservation with SHA-256 verification

pip install -r requirements.txt

```- ✅ Are initialized as Git repositories ready for version control- **Isolation**: Per-run UUID workspaces with deterministic seeding



### 2. Generate Your First Experiment- ✅ Can be distributed, archived, or run on any machine with Python 3.9+



**Interactive Mode (Recommended):**## Quick Start

```bash

python scripts/new_experiment.py**Think of it as:** A project template generator specifically designed for AI framework evaluation experiments.

```

### Prerequisites

Follow the prompts to configure your experiment.

---

**Or use CLI Mode:**

```bash- **Python 3.11+** (required)

python scripts/new_experiment.py \

  --name my_evaluation \## ⚡ Quick Start- **Git** (required)

  --model gpt-4o \

  --frameworks baes,chatdev \- **API Keys**: OpenAI for BAEs/ChatDev, GitHub token for Spec-kit

  --runs 50

```### 1. Install the Generator- **System**: 8GB RAM, 10GB disk space (per multi-framework run)



### 3. Run the Generated Experiment



```bash```bash### Installation

cd experiments/my_evaluation

./setup.shgit clone https://github.com/nosredna123/genai-devbench.git

# Edit .env with your OpenAI API keys

./run.shcd genai-devbench```bash

```

pip install -r requirements.txt# Clone repository

That's it! Results will be in `./runs/`

```git clone https://github.com/nosredna123/genai-devbench.git

---

cd genai-devbench

## 📦 What Gets Generated?

### 2. Generate Your First Experiment

Each experiment is a complete, standalone project:

# Create virtual environment (recommended)

```

my_evaluation/                    # Fully independent project**Interactive Mode (Recommended):**python3 -m venv venv

├── .git/                        # Git repository

├── setup.sh                     # One-command setup script```bashsource venv/bin/activate  # On Windows: venv\Scripts\activate

├── run.sh                       # One-command execution script

├── config.yaml                  # Experiment configurationpython scripts/new_experiment.py

├── README.md                    # Standalone documentation

├── requirements.txt             # Python dependencies (minimal)```# Install dependencies

├── .env.example                 # API key template

├── .gitignore                   # Git ignore patternspip install -r requirements.txt

├── src/                         # Complete source code

│   ├── adapters/               # Framework adapters (only enabled ones)Follow the prompts to configure your experiment.

│   ├── orchestrator/           # Experiment orchestration

│   ├── analysis/               # Results analysis & visualization# Configure API keys

│   ├── utils/                  # Utilities

│   ├── main.py                 # Entry point**Or use CLI Mode:**cp .env.example .env

│   └── setup_frameworks.py     # Framework repository setup

├── config/                      # Framework configurations```bash# Edit .env with your API keys:

│   ├── prompts/                # Framework-specific prompts

│   └── hitl/                   # Human-in-the-loop specspython scripts/new_experiment.py \#   BAES_API_KEY=sk-your-openai-key

├── runs/                        # Experiment outputs (generated)

├── analysis/                    # Analysis results (generated)  --name my_evaluation \#   CHATDEV_API_KEY=sk-your-openai-key

└── frameworks/                  # Framework repositories (generated)

```  --model gpt-4o \#   GHSPEC_API_KEY=ghp_your-github-token



**Key Point:** The generated project has ZERO dependencies on this generator. It's completely standalone.  --frameworks baes,chatdev \```



---  --runs 50



## 🔧 Configuration Options```### Your First Experiment (5 Minutes)



When generating an experiment, you configure:



### Models### 3. Run the Generated ExperimentThe framework supports multiple independent experiments with organized, isolated outputs:

- **gpt-4o** - Latest GPT-4 Omni (recommended)

- **gpt-4o-mini** - Smaller, faster GPT-4 Omni

- **gpt-4-turbo** - GPT-4 Turbo

- **gpt-4** - GPT-4 (legacy)```bash```bash

- **gpt-3.5-turbo** - GPT-3.5 Turbo

cd experiments/my_evaluation# 1. Create an experiment

### Frameworks

- **BAES** - Behavior-driven Agile Engineering System./setup.shpython scripts/new_experiment.py \

- **ChatDev** - Communicative agents for software development

- **GitHub Copilot** - AI pair programmer (via spec-kit)# Edit .env with your OpenAI API keys    --name my_first_experiment \



**Select one, two, or all three** - the generator only includes what you need../run.sh    --model gpt-4o \



### Runs```    --frameworks baes \

- **Min/Max runs per framework** - Determines statistical confidence

- **Automatic stopping** - Based on confidence interval convergence    --runs 10

- Default: 5-50 runs per framework

That's it! Results will be in `./runs/`

### Metrics

- **Functional Correctness** - Does the code work?# 2. Run the experiment

- **Design Quality** - Architecture and patterns

- **Code Maintainability** - Readability and structure---python scripts/run_experiment.py my_first_experiment

- **API Usage** - Token consumption and costs



---

## 📦 What Gets Generated?# 3. Analyze results

## 📖 Generator Usage

./runners/analyze_results.sh my_first_experiment

### Interactive Mode

Each experiment is a complete, standalone project:

```bash

python scripts/new_experiment.py# 4. View report

```

```cat experiments/my_first_experiment/analysis/report.md

Guided wizard walks you through:

1. Experiment namemy_evaluation/                    # Fully independent project```

2. Model selection

3. Framework selection├── .git/                        # Git repository

4. Run configuration

5. Optional template selection├── setup.sh                     # One-command setup script**What happened:**



### CLI Mode├── run.sh                       # One-command execution script- ✅ Created isolated experiment directory



```bash├── config.yaml                  # Experiment configuration- ✅ Ran 10 independent BAEs executions

python scripts/new_experiment.py \

  --name <experiment_name> \├── README.md                    # Standalone documentation- ✅ Generated statistical analysis with confidence intervals

  --model <model> \

  --frameworks <framework1,framework2> \├── requirements.txt             # Python dependencies (minimal)- ✅ Produced comprehensive report with recommendations

  --runs <max_runs_per_framework> \

  [--experiments-dir <custom_directory>]├── .env.example                 # API key template

```

├── .gitignore                   # Git ignore patterns**See [Quick Start Guide](docs/QUICKSTART.md) for detailed walkthrough.**

**Examples:**

├── src/                         # Complete source code

```bash

# Single framework evaluation│   ├── adapters/               # Framework adapters (only enabled ones)### Compare Multiple Experiments

python scripts/new_experiment.py \

  --name baes_eval \│   ├── orchestrator/           # Experiment orchestration

  --model gpt-4o-mini \

  --frameworks baes \│   ├── analysis/               # Results analysis & visualization```bash

  --runs 10

│   ├── utils/                  # Utilities# Create baseline

# Compare all frameworks

python scripts/new_experiment.py \│   ├── main.py                 # Entry pointpython scripts/new_experiment.py \

  --name full_comparison \

  --model gpt-4o \│   └── setup_frameworks.py     # Framework repository setup    --name baseline_gpt4o \

  --frameworks baes,chatdev,ghspec \

  --runs 50├── config/                      # Framework configurations    --model gpt-4o \



# Custom output directory│   ├── prompts/                # Framework-specific prompts    --frameworks baes \

python scripts/new_experiment.py \

  --name remote_eval \│   └── hitl/                   # Human-in-the-loop specs    --runs 10

  --model gpt-4o \

  --frameworks chatdev \├── runs/                        # Experiment outputs (generated)

  --runs 25 \

  --experiments-dir /mnt/experiments├── analysis/                    # Analysis results (generated)# Create variant

```

└── frameworks/                  # Framework repositories (generated)python scripts/new_experiment.py \

---

```    --name variant_gpt4omini \

## 🚀 Using Generated Experiments

    --model gpt-4o-mini \

### Setup

**Key Point:** The generated project has ZERO dependencies on this generator. It's completely standalone.    --frameworks baes \

```bash

cd experiments/<experiment_name>    --runs 10

./setup.sh

```---



This creates a virtual environment, installs dependencies, and clones framework repositories.# Run both



### Configure API Keys## 🔧 Configuration Optionspython scripts/run_experiment.py baseline_gpt4o



```bashpython scripts/run_experiment.py variant_gpt4omini

nano .env  # or your preferred editor

```When generating an experiment, you configure:



Add your OpenAI API keys (one per framework for separate usage tracking):# Analyze both



```bash### Models./runners/analyze_results.sh baseline_gpt4o

OPENAI_API_KEY_BAES=sk-...

OPENAI_API_KEY_CHATDEV=sk-...- **gpt-4o** - Latest GPT-4 Omni (recommended)./runners/analyze_results.sh variant_gpt4omini

OPENAI_API_KEY_GHSPEC=sk-...

```- **gpt-4o-mini** - Smaller, faster GPT-4 Omni



### Run- **gpt-4-turbo** - GPT-4 Turbo# Compare



```bash- **gpt-4** - GPT-4 (legacy)diff experiments/baseline_gpt4o/analysis/report.md \

./run.sh

```- **gpt-3.5-turbo** - GPT-3.5 Turbo     experiments/variant_gpt4omini/analysis/report.md



Experiment executes automatically. Results saved to `./runs/<framework>/<run_id>/````



### Analyze### Frameworks



```bash- **BAES** - Behavior-driven Agile Engineering System**See [Comparison Guide](docs/COMPARISON_GUIDE.md) for statistical comparison techniques.**

source venv/bin/activate

python -m src.analysis.report_generator- **ChatDev** - Communicative agents for software development

```

- **GitHub Copilot** - AI pair programmer (via spec-kit)### Legacy Single-Run Mode

Generates comprehensive analysis report with visualizations.



---

**Select one, two, or all three** - the generator only includes what you need.The framework still supports legacy single-run mode (deprecated):

## 🏗️ Architecture



### Generator Components

### Runs```bash

1. **Artifact Collector** - Determines which files to include based on config

2. **Import Rewriter** - Adapts Python imports for standalone operation- **Min/Max runs per framework** - Determines statistical confidence# Legacy: Execute single framework run (15-30 minutes)

3. **Script Generator** - Creates setup.sh, run.sh, README.md

4. **Dependency Analyzer** - Generates minimal requirements.txt- **Automatic stopping** - Based on confidence interval convergence# DEPRECATED: Use new_experiment.py instead

5. **Standalone Generator** - Orchestrates the entire generation process

- Default: 5-50 runs per framework./runners/run_experiment.sh baes

### Generated Experiment Flow



```

Config → Artifact Collection → File Copying → Script Generation### Metrics# Legacy: View metrics from a run

    → Git Init → Standalone Project

```- **Functional Correctness** - Does the code work?cat runs/baes/<run-id>/metrics.json



### Experiment Execution Flow- **Design Quality** - Architecture and patterns



```- **Code Maintainability** - Readability and structure# Legacy: Analyze legacy runs

Setup → Framework Cloning → Configuration → Run Loop

    → Per-Framework Execution → Metrics Collection- **API Usage** - Token consumption and costs./runners/analyze_results.sh ./analysis_output

    → Statistical Analysis → Visualization

``````



------



## 📊 Metrics & Analysis**Note:** Legacy mode is maintained for backward compatibility but new workflows should use the multi-experiment system.



Generated experiments collect comprehensive metrics:## 📖 Generator Usage



### Autonomy Metrics## Documentation

- **AUTR** - Autonomous Task Resolution

- **HITL** - Human-in-the-Loop interactions### Interactive Mode

- **MCI** - Manual Code Interventions

### Getting Started

### Quality Metrics

- **Q*** - Quality star rating```bash- **[Quick Start Guide](docs/QUICKSTART.md)** ⭐ **Start here!** - Create your first experiment in 5 minutes

- **ESR** - External Service Readiness

- **CRUDe** - CRUD endpoint coveragepython scripts/new_experiment.py- **[Workflows Guide](docs/WORKFLOWS.md)** - Common usage patterns and real-world scenarios

- **MC** - McCabe complexity

```- **[Comparison Guide](docs/COMPARISON_GUIDE.md)** - Statistical comparison of experiments

### Efficiency Metrics

- **TOK_IN/TOK_OUT** - Token consumption- **[Best Practices Guide](docs/BEST_PRACTICES.md)** - Recommendations for effective experimentation

- **T_WALL** - Wall clock time

- **COST_USD** - API costsGuided wizard walks you through:

- **AEI** - Autonomy Efficiency Index

1. Experiment name### Multi-Experiment System

### Reliability Metrics

- **ZDI** - Zero-defect installations2. Model selection- **[Architecture Guide](docs/architecture.md)** - System design and multi-experiment components

- **RTE** - Runtime errors

- **BR** - Build success rate3. Framework selection- **[Configuration Reference](docs/configuration_reference.md)** - Complete config schema and examples



### Analysis Features4. Run configuration- **[Validation System](docs/validation_system.md)** - Configuration validation reference

- Bootstrap confidence intervals (95%)

- Kruskal-Wallis non-parametric tests5. Optional template selection- **[Troubleshooting Guide](docs/troubleshooting.md)** - Common issues and solutions

- Dunn-Šidák pairwise comparisons

- Cliff's δ effect sizes

- Automatic convergence detection

### CLI Mode### Metrics and Analysis

---

- **[Metrics Guide](docs/metrics.md)** - Complete reference for all 16 metrics

## 🔍 Advanced Features

```bash- **[Statistical Power Analysis](docs/statistical_power_analysis.md)** - Sample size calculations

### Template-based Generation

python scripts/new_experiment.py \- **[API Usage Reconciliation](docs/reconcile_usage_guide.md)** - Accurate cost tracking

Generate experiments based on existing ones:

  --name <experiment_name> \

```bash

python scripts/new_experiment.py  --model <model> \### Legacy Documentation

# Select "yes" when asked about templates

# Choose an existing experiment as base  --frameworks <framework1,framework2> \- [Original Quickstart](docs/quickstart.md) - Legacy single-run quickstart (deprecated)

```

  --runs <max_runs_per_framework> \- [Feature Specification](specs/001-baes-experiment-framework/spec.md) - Requirements and user stories

### Custom Output Directories

  [--output-dir <custom_directory>]- [Implementation Plan](specs/001-baes-experiment-framework/plan.md) - Technical design

Generate experiments outside the default `experiments/` folder:

```- [Research Decisions](specs/001-baes-experiment-framework/research.md) - Design rationale

```bash

python scripts/new_experiment.py \

  --name external_eval \

  --model gpt-4o \**Examples:**### Testing

  --frameworks baes \

  --runs 20 \- **Test Suite:** Comprehensive unit tests with 100% pass rate

  --experiments-dir /path/to/custom/location

``````bash- **Run Tests:** `pytest tests/unit/test_report_generation.py -v`



### Distributed Experiments# Single framework evaluation- **Test Coverage:** 26 tests covering validation, dynamics, and edge cases



Generated experiments are portable:python scripts/new_experiment.py \- **Execution Time:** < 2 seconds for full suite



```bash  --name baes_eval \

# Generate locally

python scripts/new_experiment.py --name dist_eval --model gpt-4o --frameworks baes --runs 50  --model gpt-4o-mini \## Architecture



# Archive  --frameworks baes \

tar -czf dist_eval.tar.gz experiments/dist_eval/

  --runs 10```

# Transfer to remote machine

scp dist_eval.tar.gz remote:/experiments/genai-devbench/



# Extract and run remotely# Compare all frameworks├── experiments/                 # 🆕 Multi-experiment storage (isolated)

ssh remote

cd /experimentspython scripts/new_experiment.py \│   ├── <experiment_name>/       # Individual experiment directory

tar -xzf dist_eval.tar.gz

cd dist_eval  --name full_comparison \│   │   ├── config.yaml          # Experiment configuration (immutable)

./setup.sh

# Configure .env  --model gpt-4o \│   │   ├── README.md            # Auto-generated documentation

./run.sh

```  --frameworks baes,chatdev,ghspec \│   │   ├── runs/                # Run outputs for this experiment



---  --runs 50│   │   │   ├── manifest.json    # Run tracking and metadata



## 📚 Documentation│   │   │   ├── baes/            # Framework-specific runs



- **[Generator Architecture](docs/STANDALONE_EXPERIMENT_DESIGN.md)** - Design overview# Custom output directory│   │   │   ├── chatdev/

- **[Transformation Plan](docs/GENERATOR_TRANSFORMATION_PLAN.md)** - Implementation details

- **[Validation Report](docs/TRANSFORMATION_VALIDATION.md)** - Testing and validationpython scripts/new_experiment.py \│   │   │   └── ghspec/

- **[Metrics Documentation](docs/metrics.md)** - Comprehensive metric definitions

- **[Configuration Reference](docs/configuration_reference.md)** - All config options  --name remote_eval \│   │   ├── analysis/            # Analysis outputs



---  --model gpt-4o \│   │   │   ├── report.md        # Statistical report



## 🤝 Contributing  --frameworks chatdev \│   │   │   └── visualizations/  # Charts and graphs



Contributions welcome! Areas for improvement:  --runs 25 \│   │   └── .meta/               # Metadata



- Additional framework adapters  --output-dir /mnt/experiments│   │       └── config.hash      # Config integrity verification

- New metrics implementations

- Enhanced analysis visualizations```│   └── .experiment_registry.json # 🆕 Global experiment registry

- Documentation improvements

- Bug fixes and optimizations├── scripts/                     # 🆕 Experiment management



------│   ├── new_experiment.py        # Create experiments (interactive/CLI)



## 📄 License│   └── run_experiment.py        # Run experiments (high-level wrapper)



See [LICENSE](LICENSE) file.## 🚀 Using Generated Experiments├── config/                      # Experiment configuration



---│   ├── experiment.yaml          # Framework configs, timeouts, seeds



## 🆘 Troubleshooting### Setup│   ├── prompts/                 # 6-step CRUD evolution scenario



### Generator Issues│   └── hitl/                    # HITL clarification templates



**Problem:** Import errors when running generator  ```bash├── src/

**Solution:** Ensure you're in the project root: `cd genai-devbench`

cd experiments/<experiment_name>│   ├── orchestrator/            # Execution coordination

**Problem:** Generation fails  

**Solution:** Check Python version (3.9+ required), verify all dependencies installed./setup.sh│   │   ├── runner.py            # OrchestratorRunner (single/multi)



### Generated Experiment Issues```│   │   ├── metrics_collector.py # 16 metrics computation



**Problem:** `./setup.sh` fails  │   │   ├── validator.py         # CRUD/UI/downtime validation

**Solution:** Check Python 3.9+ installed, verify internet connection for package downloads

This creates a virtual environment, installs dependencies, and clones framework repositories.│   │   └── archiver.py          # tar.gz with SHA-256

**Problem:** `./run.sh` fails with API key errors  

**Solution:** Verify `.env` file exists and contains valid API keys starting with `sk-`│   ├── adapters/                # Framework integrations



**Problem:** Framework cloning fails  ### Configure API Keys│   │   ├── base_adapter.py      # Abstract BaseAdapter

**Solution:** Check internet connection, verify git installed

│   │   ├── baes_adapter.py      # BAEs implementation

**Problem:** Import errors in generated experiment  

**Solution:** Run `./setup.sh` again, activate venv: `source venv/bin/activate````bash│   │   ├── chatdev_adapter.py   # ChatDev implementation



---nano .env  # or your preferred editor│   │   └── ghspec_adapter.py    # GitHub Spec-kit implementation



## 🗺️ Roadmap```│   ├── analysis/                # Statistical analysis



### Planned Features│   │   ├── statistics.py        # Tests, effect sizes, reports



- [ ] Docker containerization option for generated experimentsAdd your OpenAI API keys (one per framework for separate usage tracking):│   │   ├── stopping_rule.py     # Bootstrap CI convergence

- [ ] CI/CD workflow generation (GitHub Actions, GitLab CI)

- [ ] Additional framework support (AutoGen, CrewAI, etc.)│   │   └── visualizations.py    # Radar, Pareto, timeline charts

- [ ] Web UI for experiment generation

- [ ] Experiment result aggregation across multiple runs```bash│   └── utils/                   # Utilities

- [ ] Cloud deployment templates (AWS, Azure, GCP)

OPENAI_API_KEY_BAES=sk-...│       ├── logger.py            # Structured JSON logging

---

OPENAI_API_KEY_CHATDEV=sk-...│       ├── config_loader.py     # YAML validation

**Ready to start?** Run `python scripts/new_experiment.py` and create your first experiment! 🚀

OPENAI_API_KEY_GHSPEC=sk-...│       ├── isolation.py         # Workspace management

```│       ├── api_client.py        # OpenAI Usage API verification

│       ├── experiment_paths.py  # 🆕 Path resolution for experiments

### Run│       └── experiment_registry.py # 🆕 Global experiment tracking

├── runners/                     # Entry point scripts

```bash│   ├── run_experiment.sh        # ⚠️ DEPRECATED (use scripts/run_experiment.py)

./run.sh│   ├── analyze_results.sh       # Analysis pipeline (experiment-aware)

```│   └── reconcile_usage.sh       # 🆕 API usage reconciliation

├── runs/                        # 🗄️ Legacy run outputs (gitignored, deprecated)

Experiment executes automatically. Results saved to `./runs/<framework>/<run_id>/`├── tests/                       # Test suite

│   ├── unit/                    # Unit tests

### Analyze│   ├── integration/             # Integration tests

│   └── contract/                # Contract tests

```bash└── docs/                        # Documentation

source venv/bin/activate    ├── QUICKSTART.md            # 🆕 5-minute getting started guide

python -m src.analysis.report_generator    ├── WORKFLOWS.md             # 🆕 Common usage patterns

```    ├── COMPARISON_GUIDE.md      # 🆕 Statistical comparison guide

    ├── BEST_PRACTICES.md        # 🆕 Recommendations and tips

Generates comprehensive analysis report with visualizations.    └── ...                      # Other documentation

```

---

**Key Changes:**

## 🏗️ Architecture- 🆕 **experiments/** directory: All experiments organized by name

- 🆕 **Experiment Registry**: Global tracking of all experiments

### Generator Components- 🆕 **Config Hash**: Immutable configuration with integrity verification

- 🆕 **Isolated Runs**: Each experiment has its own runs and analysis

1. **Artifact Collector** - Determines which files to include based on config- ⚠️ **Deprecated**: Legacy `runners/run_experiment.sh` (use `scripts/run_experiment.py`)

2. **Import Rewriter** - Adapts Python imports for standalone operation- 🗄️ **Legacy**: Old `runs/` directory maintained for backward compatibility

3. **Script Generator** - Creates setup.sh, run.sh, README.md

4. **Dependency Analyzer** - Generates minimal requirements.txt## Metrics Reference

5. **Standalone Generator** - Orchestrates the entire generation process

### Quality Metrics (5)

### Generated Experiment Flow- **AUTR** (Automated User Testing Rate): Autonomy rate = 1 - (HIT/UTT), measuring independence from human intervention [0-1]

- **Q\*** (Quality Star): Composite score = 0.4·ESR + 0.3·(CRUDe/12) + 0.3·MC [0-1]

```- **ESR** (Emerging State Rate): Successful incremental evolution steps [0-1]

Config → Artifact Collection → File Copying → Script Generation- **CRUDe** (CRUD Evolution): CRUD operation coverage [0-12]

    → Git Init → Standalone Project- **MC** (Model Call Efficiency): Inverse normalized API calls [0-1]

```

### Efficiency Metrics (6)

### Experiment Execution Flow- **TOK_IN** (Input Tokens): Total tokens sent to LLM APIs

- **TOK_OUT** (Output Tokens): Total tokens received from LLM APIs

```- **T_WALL** (Wall-Clock Time): Total elapsed time (seconds)

Setup → Framework Cloning → Configuration → Run Loop- **T_USER** (User Time): CPU time in user mode (seconds)

    → Per-Framework Execution → Metrics Collection- **T_CPU** (System Time): CPU time in kernel mode (seconds)

    → Statistical Analysis → Visualization- **AEI** (API Efficiency Index): AUTR / log(1 + TOK_IN)

```

### Reliability Metrics (3)

---- **ZDI** (Zero-Downtime Incidents): Count of availability failures

- **RTE** (Runtime Errors): Proportion of steps with crashes [0-1]

## 📊 Metrics & Analysis- **MCI** (Model Call Interruptions): Count of API retry events



Generated experiments collect comprehensive metrics:### Process Metrics (2)

- **ITR** (Iterations): Number of framework invocations

### Autonomy Metrics- **DPL** (Deployment Success): Binary deployment outcome {0, 1}

- **AUTR** - Autonomous Task Resolution

- **HITL** - Human-in-the-Loop interactionsSee [Metrics Guide](docs/metrics.md) for detailed definitions, formulas, and interpretation.

- **MCI** - Manual Code Interventions

## Statistical Analysis

### Quality Metrics

- **Q*** - Quality star ratingThe framework automatically performs:

- **ESR** - External Service Readiness

- **CRUDe** - CRUD endpoint coverage1. **Aggregate Statistics**: Mean, median, std, 95% bootstrap CI for each metric

- **MC** - McCabe complexity2. **Hypothesis Testing**: Kruskal-Wallis H-test for group differences (p < 0.05)

3. **Pairwise Comparisons**: Dunn-Šidák corrected Mann-Whitney U tests

### Efficiency Metrics4. **Effect Sizes**: Cliff's delta (non-parametric effect magnitude)

- **TOK_IN/TOK_OUT** - Token consumption5. **Outlier Detection**: Values >3σ from median

- **T_WALL** - Wall clock time6. **Convergence Detection**: Stop when CI half-width < 10% of mean

- **COST_USD** - API costs

- **AEI** - Autonomy Efficiency IndexResults output as markdown report with publication-ready SVG visualizations.



### Reliability Metrics## Reproducibility

- **ZDI** - Zero-defect installations

- **RTE** - Runtime errorsThe framework ensures deterministic execution through:

- **BR** - Build success rate

- **Fixed Seeds**: Python random, NumPy seeds set per run

### Analysis Features- **Commit Verification**: SHA verification of framework repository state

- Bootstrap confidence intervals (95%)- **HITL Logging**: SHA-1 hashing of human-in-the-loop responses

- Kruskal-Wallis non-parametric tests- **Byte-for-Byte Validation**: Reproducibility test script for output comparison

- Dunn-Šidák pairwise comparisons- **Temperature Zero**: LLM sampling with temperature=0.0, top_p=1.0

- Cliff's δ effect sizes

- Automatic convergence detectionSee [Architecture Guide](docs/architecture.md#reproducibility) for details.



---## License



## 🔍 Advanced FeaturesThis project is licensed under CC BY 4.0 - see [LICENSE](LICENSE) file for details.



### Template-based Generation## Citation



Generate experiments based on existing ones:If you use this framework in your research, please cite:



```bash```bibtex

python scripts/new_experiment.py@software{genai-devbench_2025,

# Select "yes" when asked about templates  title={GenAI-DevBench},

# Choose an existing experiment as base  author={GESAD Lab},

```  year={2025},

  url={https://github.com/nosredna123/genai-devbench}

### Custom Output Directories}

```

Generate experiments outside the default `experiments/` folder:

## Contributing

```bash

python scripts/new_experiment.py \This is a research artifact. For questions or issues, please open a GitHub issue.

  --name external_eval \
  --model gpt-4o \
  --frameworks baes \
  --runs 20 \
  --output-dir /path/to/custom/location
```

### Distributed Experiments

Generated experiments are portable:

```bash
# Generate locally
python scripts/new_experiment.py --name dist_eval --model gpt-4o --frameworks baes --runs 50

# Archive
tar -czf dist_eval.tar.gz experiments/dist_eval/

# Transfer to remote machine
scp dist_eval.tar.gz remote:/experiments/

# Extract and run remotely
ssh remote
cd /experiments
tar -xzf dist_eval.tar.gz
cd dist_eval
./setup.sh
# Configure .env
./run.sh
```

---

## 📚 Documentation

- **[Generator Architecture](docs/STANDALONE_EXPERIMENT_DESIGN.md)** - Design overview
- **[Transformation Plan](docs/GENERATOR_TRANSFORMATION_PLAN.md)** - Implementation details
- **[Metrics Documentation](docs/metrics.md)** - Comprehensive metric definitions
- **[Configuration Reference](docs/configuration_reference.md)** - All config options

---

## 🤝 Contributing

Contributions welcome! Areas for improvement:

- Additional framework adapters
- New metrics implementations
- Enhanced analysis visualizations
- Documentation improvements
- Bug fixes and optimizations

---

## 📄 License

See [LICENSE](LICENSE) file.

---

## 🆘 Troubleshooting

### Generator Issues

**Problem:** Import errors when running generator  
**Solution:** Ensure you're in the project root: `cd genai-devbench`

**Problem:** Generation fails  
**Solution:** Check Python version (3.9+ required), verify all dependencies installed

### Generated Experiment Issues

**Problem:** `./setup.sh` fails  
**Solution:** Check Python 3.9+ installed, verify internet connection for package downloads

**Problem:** `./run.sh` fails with API key errors  
**Solution:** Verify `.env` file exists and contains valid API keys starting with `sk-`

**Problem:** Framework cloning fails  
**Solution:** Check internet connection, verify git installed

**Problem:** Import errors in generated experiment  
**Solution:** Run `./setup.sh` again, activate venv: `source venv/bin/activate`

---

## 🗺️ Roadmap

### Planned Features

- [ ] Docker containerization option for generated experiments
- [ ] CI/CD workflow generation (GitHub Actions, GitLab CI)
- [ ] Additional framework support (AutoGen, CrewAI, etc.)
- [ ] Web UI for experiment generation
- [ ] Experiment result aggregation across multiple runs
- [ ] Cloud deployment templates (AWS, Azure, GCP)

---

**Ready to start?** Run `python scripts/new_experiment.py` and create your first experiment! 🚀
