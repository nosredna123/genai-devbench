# Experiment Template for 'default' Config Set
# This file is used as the base for generated config.yaml

# Steps configuration (all enabled by default)
steps:
  - id: 1
    enabled: true
    name: "Create Student Entity"
    prompt_file: "config/prompts/01_create_student_entity.txt"
  
  - id: 2
    enabled: true
    name: "Add Email to Student"
    prompt_file: "config/prompts/02_add_email_to_student.txt"
  
  - id: 3
    enabled: true
    name: "Create Course Entity"
    prompt_file: "config/prompts/03_create_course_entity.txt"
  
  - id: 4
    enabled: true
    name: "Add Course to Student"
    prompt_file: "config/prompts/04_add_course_to_student.txt"
  
  - id: 5
    enabled: true
    name: "Create Teacher Entity"
    prompt_file: "config/prompts/05_create_teacher_entity.txt"
  
  - id: 6
    enabled: true
    name: "Add Teacher to Course"
    prompt_file: "config/prompts/06_add_teacher_to_course.txt"

# Timeouts (can be overridden by researcher post-generation)
timeouts:
  step_timeout_seconds: 600
  health_check_interval_seconds: 5
  api_retry_attempts: 3

# Stopping rule configuration
stopping_rule:
  min_runs: 5
  max_runs: 50
  confidence_level: 0.95
  max_half_width_pct: 10
  metrics:
    - TOK_IN
    - T_WALL_seconds
    - COST_USD

# Statistical analysis configuration
analysis:
  bootstrap_samples: 10000
  significance_level: 0.05
  confidence_level: 0.95
  effect_size_thresholds:
    negligible: 0.147
    small: 0.330
    medium: 0.474

# OpenAI model pricing (as of 2025-10-25)
# Prices are per million tokens (USD) - Standard tier
pricing:
  models:
    gpt-4o-mini:
      input_price: 0.150        # $0.150 per 1M input tokens
      cached_price: 0.075       # $0.075 per 1M cached input tokens (50% discount)
      output_price: 0.600       # $0.600 per 1M output tokens
      
    gpt-4o:
      input_price: 2.500        # $2.50 per 1M input tokens
      cached_price: 1.250       # $1.25 per 1M cached input tokens (50% discount)
      output_price: 10.000      # $10.00 per 1M output tokens
      
    o1-mini:
      input_price: 1.100        # $1.10 per 1M input tokens (CORRECTED from $3.00)
      cached_price: 0.550       # $0.55 per 1M cached input tokens (CORRECTED from $1.50)
      output_price: 4.400       # $4.40 per 1M output tokens (CORRECTED from $12.00)
      
    o1-preview:
      input_price: 15.000       # $15.00 per 1M input tokens
      cached_price: 7.500       # $7.50 per 1M cached input tokens (50% discount)
      output_price: 60.000      # $60.00 per 1M output tokens

# Framework configurations
# Note: All frameworks MUST run in isolated venvs (frameworks/<name>/.venv)
# The use_venv setting is kept for backward compatibility but should always be true
frameworks:
  baes:
    enabled: true
    repo_url: "https://github.com/gesad-lab/baes_demo"
    commit_hash: "1285967748549b62c2e3af7993fa8e81f79eae45"
    api_port: 8100
    ui_port: 8600
    api_key_env: "OPENAI_API_KEY_BAES"
    use_venv: true
  
  chatdev:
    enabled: true
    repo_url: "https://github.com/OpenBMB/ChatDev.git"
    commit_hash: "52edb89997b4312ad27d8c54584d0a6c59940135"
    api_port: 8200
    ui_port: 8700
    api_key_env: "OPENAI_API_KEY_CHATDEV"
    use_venv: true
  
  ghspec:
    enabled: true
    repo_url: "https://github.com/github/spec-kit.git"
    commit_hash: "e6d6f3cdee99752baee578896797400a72430ec0"
    api_port: 8300
    ui_port: 8800
    api_key_env: "OPENAI_API_KEY_GHSPEC"
    use_venv: true

# Metrics Configuration (Unified Format - Feature 009)
# See docs/CONFIG_MIGRATION_GUIDE.md for migration from old 3-subsection format
metrics:
  # === Token Metrics (Measured) ===
  TOK_IN:
    name: "Input Tokens"
    key: "TOK_IN"
    category: "tokens"
    unit: "tokens"
    display_format: "{:,.0f}"
    description: "Total input tokens sent to LLM across all API calls"
  
  TOK_OUT:
    name: "Output Tokens"
    key: "TOK_OUT"
    category: "tokens"
    unit: "tokens"
    display_format: "{:,.0f}"
    description: "Total output tokens generated by LLM across all API calls"
  
  CACHED_TOKENS:
    name: "Cached Tokens"
    key: "CACHED_TOKENS"
    category: "tokens"
    unit: "tokens"
    display_format: "{:,.0f}"
    description: "Total cached input tokens (prompt caching discount applied)"
  
  # === Performance Metrics (Measured) ===
  T_WALL_seconds:
    name: "Wall-Clock Time"
    key: "T_WALL_seconds"
    category: "performance"
    unit: "seconds"
    display_format: "{:.1f}"
    description: "Total execution time from start to finish"
  
  API_CALLS:
    name: "API Calls"
    key: "API_CALLS"
    category: "performance"
    unit: "calls"
    display_format: "{:,.0f}"
    description: "Total number of LLM API calls made"
    status: "derived"
    reason: "Counted from API interaction logs during execution"
  
  # === Cost Metrics (Derived) ===
  COST_USD:
    name: "Total Cost (USD)"
    key: "COST_USD"
    category: "cost"
    unit: "$"
    display_format: "${:.2f}"
    description: "Total API cost based on token usage and model pricing"
    status: "derived"
    reason: "Calculated from TOK_IN, TOK_OUT, CACHED_TOKENS using pricing config"
  
  # === Quality Metrics (Measured) ===
  ZDI:
    name: "Zero-Defect Index"
    key: "ZDI"
    category: "quality"
    unit: "score"
    display_format: "{:.3f}"
    description: "Composite code quality score (0-1 scale)"
  
  # === Quality Metrics (Partial Measurement) ===
  AUTR:
    name: "Automation Rate"
    key: "AUTR"
    category: "quality"
    unit: "%"
    display_format: "{:.1f}%"
    description: "Percentage of steps completed without human intervention"
    status: "partial"
    reason: "Only measured for frameworks with automation tracking (BAEs)"
  
  HIT:
    name: "Hit Rate"
    key: "HIT"
    category: "quality"
    unit: "%"
    display_format: "{:.1f}%"
    description: "Test pass rate for generated code"
    status: "partial"
    reason: "Only measured for frameworks with test suite integration"
  
  AEI:
    name: "Average Error Impact"
    key: "AEI"
    category: "quality"
    unit: "score"
    display_format: "{:.2f}"
    description: "Average severity of errors encountered"
    status: "partial"
    reason: "Requires error classification - only available for some frameworks"
  
  HEU:
    name: "Human Effort Units"
    key: "HEU"
    category: "quality"
    unit: "units"
    display_format: "{:.1f}"
    description: "Estimated human effort required"
    status: "partial"
    reason: "Requires manual logging - not all frameworks track this"
  
  # === Quality Metrics (Unmeasured) ===
  ESR:
    name: "Error-Step Ratio"
    key: "ESR"
    category: "quality"
    unit: "ratio"
    display_format: "{:.3f}"
    description: "Ratio of errors to total steps"
    status: "unmeasured"
    reason: "Requires comprehensive error tracking - not yet implemented"
  
  CRUDe:
    name: "CRUD Effectiveness"
    key: "CRUDe"
    category: "quality"
    unit: "%"
    display_format: "{:.1f}%"
    description: "CRUD operation success rate"
    status: "unmeasured"
    reason: "Requires database operation validation - not automated"
  
  MC:
    name: "Maintainability Compliance"
    key: "MC"
    category: "quality"
    unit: "%"
    display_format: "{:.1f}%"
    description: "Code maintainability score based on static analysis"
    status: "unmeasured"
    reason: "Requires static analysis tool integration (pylint/radon) - not implemented"
  
  Q_STAR:
    name: "Q* Composite Quality"
    key: "Q_STAR"
    category: "quality"
    unit: "score"
    display_format: "{:.3f}"
    description: "Weighted composite of all quality metrics"
    status: "unmeasured"
    reason: "Depends on other quality metrics - formula under development"
