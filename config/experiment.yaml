# BAEs Experiment Configuration
# Version: 1.0.0

# Fixed random seed for deterministic execution
random_seed: 42

# Model configuration - ALL frameworks must use the same model for fair comparison
# Supported models: gpt-4o-mini, o1-mini, o1-preview
# - gpt-4o-mini: Native ChatDev support (commit 52edb89)
# - o1-mini, o1-preview: Added via runtime patch (see src/adapters/chatdev_adapter.py::_patch_o1_model_support)
# Note: O1 models have better reasoning; GPT-4o-mini is faster and cheaper
model: "gpt-4o-mini"

# Framework configurations
frameworks:
  baes:
    enabled: true
    repo_url: "https://github.com/gesad-lab/baes_demo"
    commit_hash: "1dd573633a98b8baa636c200bc1684cec7a8179f"
    api_port: 8100
    ui_port: 8600
    max_retries: 3
    auto_restart_servers: false
    use_venv: true
    api_key_env: "OPENAI_API_KEY_BAES"
    
  chatdev:
    repo_url: "https://github.com/OpenBMB/ChatDev.git"
    commit_hash: "52edb89997b4312ad27d8c54584d0a6c59940135"  # Has 'refusal' + 'audio' fixes; we patch 'annotations' at runtime (see .copilot/WHY_VITORFOL_FORK.md)
    api_port: 8001
    ui_port: 3001
    api_key_env: "OPENAI_API_KEY_CHATDEV"
    
  ghspec:
    repo_url: "https://github.com/github/spec-kit.git"
    commit_hash: "89f4b0b38a42996376c0f083d47281a4c9196761"
    api_port: 8002
    ui_port: 3002
    api_key_env: "OPENAI_API_KEY_GHSPEC"

# Directory containing prompt files (step_1.txt through step_6.txt)
prompts_dir: "config/prompts"

# Path to fixed HITL clarification text
hitl_path: "config/hitl/expanded_spec.txt"

# Stopping rule configuration
# NOTE: The metrics listed below will be replaced by dynamic references to
# metrics.reliable_metrics and metrics.derived_metrics where stopping_rule_eligible=true
# See metrics configuration section for details on which metrics are eligible.
stopping_rule:
  min_runs: 5              # Minimum runs per framework
  max_runs: 50             # Maximum runs per framework
  confidence_level: 0.95   # 95% confidence intervals
  max_half_width_pct: 10   # Stop when CI half-width ≤10% of mean
  # Legacy metric list (to be replaced by config-driven approach):
  metrics:
    - TOK_IN
    - T_WALL_seconds
    - COST_USD

# Timeout configurations
timeouts:
  step_timeout_seconds: 600           # 10 minutes per step
  health_check_interval_seconds: 5    # Health check every 5 seconds
  api_retry_attempts: 3               # Retry API calls up to 3 times

# Statistical analysis configuration
analysis:
  bootstrap_samples: 10000        # Number of bootstrap resamples for CI estimation
  significance_level: 0.05        # Alpha level for statistical tests (p < 0.05)
  confidence_level: 0.95          # Confidence level for intervals (mirrors stopping_rule)
  effect_size_thresholds:         # Cliff's delta interpretation thresholds
    negligible: 0.147             # |δ| < 0.147: negligible effect
    small: 0.330                  # 0.147 ≤ |δ| < 0.330: small effect
    medium: 0.474                 # 0.330 ≤ |δ| < 0.474: medium effect
                                  # |δ| ≥ 0.474: large effect

# =============================================================================
# METRICS CONFIGURATION
# =============================================================================
# Centralized metrics definitions for consistent reporting and analysis.
# All metrics have: name, description, unit, ideal_direction, data_source, 
# aggregation, display_format, and flags for statistical tests and stopping rules.

metrics:
  # Metric categories for organization
  categories:
    - name: efficiency
      description: "Token usage, API calls, and execution time"
      color: "#4A90E2"
    - name: interaction
      description: "Automation and human intervention patterns"
      color: "#E27B4A"
    - name: cost
      description: "Actual USD costs based on OpenAI pricing"
      color: "#50C878"
  
  # Reliable metrics with complete data capture (7 metrics)
  reliable_metrics:
    TOK_IN:
      name: "Input Tokens"
      description: "Total prompt tokens sent to OpenAI API"
      unit: "tokens"
      category: efficiency
      ideal_direction: minimize
      data_source: "openai_usage_api"
      aggregation: sum
      display_format: "{:,.0f}"
      statistical_test: true
      stopping_rule_eligible: true
      visualization_types:
        - radar
        - timeline
        - boxplot
        - scatter
    
    TOK_OUT:
      name: "Output Tokens"
      description: "Total completion tokens generated by OpenAI API"
      unit: "tokens"
      category: efficiency
      ideal_direction: minimize
      data_source: "openai_usage_api"
      aggregation: sum
      display_format: "{:,.0f}"
      statistical_test: true
      stopping_rule_eligible: true
      visualization_types:
        - radar
        - timeline
        - boxplot
        - scatter
    
    API_CALLS:
      name: "API Calls"
      description: "Total number of requests to OpenAI API"
      unit: "calls"
      category: efficiency
      ideal_direction: minimize
      data_source: "openai_usage_api"
      aggregation: sum
      display_format: "{:,.0f}"
      statistical_test: true
      stopping_rule_eligible: true
      visualization_types:
        - radar
        - timeline
        - boxplot
        - evolution
    
    CACHED_TOKENS:
      name: "Cached Tokens"
      description: "Input tokens served from cache (50% cost discount)"
      unit: "tokens"
      category: efficiency
      ideal_direction: maximize
      data_source: "openai_usage_api"
      aggregation: sum
      display_format: "{:,.0f}"
      statistical_test: true
      stopping_rule_eligible: false
      visualization_types:
        - timeline
        - boxplot
    
    T_WALL_seconds:
      name: "Wall-Clock Time"
      description: "Total execution time from start to finish"
      unit: "seconds"
      category: efficiency
      ideal_direction: minimize
      data_source: "orchestrator_timer"
      aggregation: direct
      display_format: "{:.1f}"
      statistical_test: true
      stopping_rule_eligible: true
      visualization_types:
        - radar
        - boxplot
    
    ZDI:
      name: "Zero-Downtime Idle Time"
      description: "Total idle time between steps (seconds)"
      unit: "seconds"
      category: efficiency
      ideal_direction: minimize
      data_source: "step_timing_analysis"
      aggregation: sum
      display_format: "{:.1f}"
      statistical_test: true
      stopping_rule_eligible: false
      visualization_types:
        - boxplot
        - timeline
    
    UTT:
      name: "Utterance Count"
      description: "Total number of steps executed (max 6)"
      unit: "steps"
      category: interaction
      ideal_direction: minimize
      data_source: "orchestrator_step_count"
      aggregation: count
      display_format: "{:.0f}"
      statistical_test: true
      stopping_rule_eligible: false
      visualization_types:
        - boxplot
  
  # Derived metrics calculated from reliable metrics (1 metric)
  derived_metrics:
    COST_USD:
      name: "Total Cost"
      description: "Total USD cost with cache discount applied"
      unit: "USD"
      category: cost
      ideal_direction: minimize
      data_source: "calculated"
      aggregation: calculated
      display_format: "${:.4f}"
      statistical_test: true
      stopping_rule_eligible: true
      visualization_types:
        - radar
        - boxplot
        - timeline
      calculation:
        formula: "(TOK_IN - CACHED_TOKENS) * input_price + CACHED_TOKENS * cached_price + TOK_OUT * output_price"
        dependencies:
          - TOK_IN
          - TOK_OUT
          - CACHED_TOKENS
        pricing_source: "pricing.models"
  
  # Excluded metrics (unreliable or unmeasured - 8 metrics)
  excluded_metrics:
    AUTR:
      name: "Autonomy Rate"
      reason: "Hardcoded HITL detection always returns 0"
      original_formula: "1 - (HIT / 6)"
      status: partial_measurement
      
    AEI:
      name: "Autonomy Efficiency Index"
      reason: "Depends on unreliable AUTR metric"
      original_formula: "AUTR / log(1 + TOK_IN)"
      status: partial_measurement
      
    HIT:
      name: "Human Interventions"
      reason: "HITL detection not implemented in adapters"
      status: not_measured
      
    HEU:
      name: "Human Effort Units"
      reason: "Depends on unmeasured HIT metric"
      original_formula: "HIT * 3"
      status: not_measured
      
    Q_star:
      name: "Quality Score"
      reason: "Quality servers not started (CRUDe, ESR, MC all zero)"
      original_formula: "0.4·ESR + 0.3·(CRUDe/12) + 0.3·MC"
      status: not_measured
      
    ESR:
      name: "Endpoint Success Rate"
      reason: "Quality verification server not running"
      status: not_measured
      
    CRUDe:
      name: "CRUD Coverage"
      reason: "Quality verification server not running"
      status: not_measured
      
    MC:
      name: "Migration Continuity"
      reason: "Quality verification server not running"
      status: not_measured

# =============================================================================
# PRICING CONFIGURATION
# =============================================================================
# OpenAI model pricing (as of 2024-10-19)
# Prices are per million tokens (USD)

pricing:
  models:
    gpt-4o-mini:
      input_price: 0.150        # $0.150 per 1M input tokens
      cached_price: 0.075       # $0.075 per 1M cached input tokens (50% discount)
      output_price: 0.600       # $0.600 per 1M output tokens
      
    gpt-4o:
      input_price: 2.500        # $2.50 per 1M input tokens
      cached_price: 1.250       # $1.25 per 1M cached input tokens (50% discount)
      output_price: 10.000      # $10.00 per 1M output tokens
      
    o1-mini:
      input_price: 3.000        # $3.00 per 1M input tokens
      cached_price: 1.500       # $1.50 per 1M cached input tokens (50% discount)
      output_price: 12.000      # $12.00 per 1M output tokens
      
    o1-preview:
      input_price: 15.000       # $15.00 per 1M input tokens
      cached_price: 7.500       # $7.50 per 1M cached input tokens (50% discount)
      output_price: 60.000      # $60.00 per 1M output tokens

# =============================================================================
# VISUALIZATIONS CONFIGURATION
# =============================================================================
# Chart definitions with metrics, types, and output settings

visualizations:
  radar_chart:
    enabled: true
    title: "Framework Performance Profile"
    metrics:
      - TOK_IN
      - TOK_OUT
      - API_CALLS
      - T_WALL_seconds
      - COST_USD
    scale: normalized       # normalized (0-1) or percentage (0-100%)
    filename: "radar_framework_profile.png"
    
  token_efficiency_scatter:
    enabled: true
    title: "Token Efficiency: Input vs Output"
    x_metric: TOK_IN
    y_metric: TOK_OUT
    filename: "scatter_token_efficiency.png"
    
  api_calls_timeline:
    enabled: true
    title: "API Calls Timeline Across Steps"
    metric: API_CALLS
    x_axis: step_number
    aggregation: mean       # mean, median, or last
    filename: "timeline_api_calls.png"
    
  cost_boxplot:
    enabled: true
    title: "Cost Distribution by Framework"
    metric: COST_USD
    filename: "boxplot_cost_distribution.png"
    
  api_calls_evolution:
    enabled: true
    title: "API Calls Evolution Across Steps"
    metric: API_CALLS
    steps: [1, 2, 3, 4, 5, 6]
    filename: "evolution_api_calls.png"

# =============================================================================
# REPORT CONFIGURATION
# =============================================================================
# Statistical report sections and their order

report:
  title: "BAEs Framework Comparison: Statistical Analysis Report"
  sections:
    - name: overview
      enabled: true
      title: "1. Overview"
      
    - name: data_quality
      enabled: true
      title: "2. Data Quality Assessment"
      
    - name: descriptive_statistics
      enabled: true
      title: "3. Descriptive Statistics"
      metrics:
        - TOK_IN
        - TOK_OUT
        - API_CALLS
        - CACHED_TOKENS
        - T_WALL_seconds
        - ZDI
        - UTT
        - COST_USD
      
    - name: statistical_tests
      enabled: true
      title: "4. Statistical Tests"
      subsections:
        - name: kruskal_wallis
          title: "4.1 Kruskal-Wallis H Test"
          description: "Tests if distributions differ across frameworks"
          
        - name: pairwise_comparisons
          title: "4.2 Pairwise Comparisons (Dunn-Šidák)"
          description: "Post-hoc tests with family-wise error correction"
          
        - name: effect_sizes
          title: "4.3 Effect Sizes (Cliff's Delta)"
          description: "Magnitude of differences between frameworks"
      
    - name: visualizations
      enabled: true
      title: "5. Visualizations"
      
    - name: cost_analysis
      enabled: true
      title: "6. Cost Analysis"
      
    - name: conclusions
      enabled: true
      title: "7. Conclusions and Recommendations"

