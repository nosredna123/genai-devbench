# BAEs Experiment Configuration
# Version: 1.0.0

# Fixed random seed for deterministic execution
random_seed: 42

# Model configuration - ALL frameworks must use the same model for fair comparison
# Supported models: gpt-4o-mini, o1-mini, o1-preview
# - gpt-4o-mini: Native ChatDev support (commit 52edb89)
# - o1-mini, o1-preview: Added via runtime patch (see src/adapters/chatdev_adapter.py::_patch_o1_model_support)
# Note: O1 models have better reasoning; GPT-4o-mini is faster and cheaper
model: "gpt-4o-mini"

# Framework configurations
frameworks:
  baes:
    enabled: true
    repo_url: "https://github.com/gesad-lab/baes_demo"
    commit_hash: "553c68a2aa35c353688b885f6a83c678caf05466"
    api_port: 8100
    ui_port: 8600
    max_retries: 3
    auto_restart_servers: false
    use_venv: true
    api_key_env: "OPENAI_API_KEY_BAES"
    
  chatdev:
    repo_url: "https://github.com/OpenBMB/ChatDev.git"
    commit_hash: "52edb89997b4312ad27d8c54584d0a6c59940135"  # Has 'refusal' + 'audio' fixes; we patch 'annotations' at runtime (see .copilot/WHY_VITORFOL_FORK.md)
    api_port: 8001
    ui_port: 3001
    use_venv: true
    api_key_env: "OPENAI_API_KEY_CHATDEV"
    
  ghspec:
    repo_url: "https://github.com/github/spec-kit.git"
    commit_hash: "89f4b0b38a42996376c0f083d47281a4c9196761"
    api_port: 8002
    ui_port: 3002
    use_venv: false  # Node.js-based framework, no Python venv needed
    api_key_env: "OPENAI_API_KEY_GHSPEC"

# Directory containing prompt files (step_1.txt through step_6.txt)
prompts_dir: "config/prompts"

# Path to fixed HITL clarification text
hitl_path: "config/hitl/expanded_spec.txt"

# Stopping rule configuration
# NOTE: The metrics listed below will be replaced by dynamic references to
# metrics.reliable_metrics and metrics.derived_metrics where stopping_rule_eligible=true
# See metrics configuration section for details on which metrics are eligible.
stopping_rule:
  min_runs: 5              # Minimum runs per framework
  max_runs: 50             # Maximum runs per framework
  confidence_level: 0.95   # 95% confidence intervals
  max_half_width_pct: 10   # Stop when CI half-width ≤10% of mean
  # Legacy metric list (to be replaced by config-driven approach):
  metrics:
    - TOK_IN
    - T_WALL_seconds
    - COST_USD

# Timeout configurations
timeouts:
  step_timeout_seconds: 600           # 10 minutes per step
  health_check_interval_seconds: 5    # Health check every 5 seconds
  api_retry_attempts: 3               # Retry API calls up to 3 times

# Statistical analysis configuration
analysis:
  bootstrap_samples: 10000        # Number of bootstrap resamples for CI estimation
  significance_level: 0.05        # Alpha level for statistical tests (p < 0.05)
  confidence_level: 0.95          # Confidence level for intervals (mirrors stopping_rule)
  effect_size_thresholds:         # Cliff's delta interpretation thresholds
    negligible: 0.147             # |δ| < 0.147: negligible effect
    small: 0.330                  # 0.147 ≤ |δ| < 0.330: small effect
    medium: 0.474                 # 0.330 ≤ |δ| < 0.474: medium effect
                                  # |δ| ≥ 0.474: large effect

# =============================================================================
# METRICS CONFIGURATION
# =============================================================================
# Centralized metrics definitions for consistent reporting and analysis.
# All metrics have: name, description, unit, ideal_direction, data_source, 
# aggregation, display_format, and flags for statistical tests and stopping rules.

metrics:
  # Metric categories for organization
  categories:
    - name: efficiency
      description: "Token usage, API calls, and execution time"
      color: "#4A90E2"
    - name: interaction
      description: "Automation and human intervention patterns"
      color: "#E27B4A"
    - name: cost
      description: "Actual USD costs based on OpenAI pricing"
      color: "#50C878"
  
  # Reliable metrics with complete data capture (7 metrics)
  reliable_metrics:
    TOK_IN:
      name: "Input Tokens"
      description: "Total prompt tokens sent to OpenAI API"
      unit: "tokens"
      category: efficiency
      ideal_direction: minimize
      data_source: "openai_usage_api"
      aggregation: sum
      display_format: "{:,.0f}"
      statistical_test: true
      stopping_rule_eligible: true
      visualization_types:
        - radar
        - timeline
        - boxplot
        - scatter
    
    TOK_OUT:
      name: "Output Tokens"
      description: "Total completion tokens generated by OpenAI API"
      unit: "tokens"
      category: efficiency
      ideal_direction: minimize
      data_source: "openai_usage_api"
      aggregation: sum
      display_format: "{:,.0f}"
      statistical_test: true
      stopping_rule_eligible: true
      visualization_types:
        - radar
        - timeline
        - boxplot
        - scatter
    
    API_CALLS:
      name: "API Calls"
      description: "Total number of requests to OpenAI API"
      unit: "calls"
      category: efficiency
      ideal_direction: minimize
      data_source: "openai_usage_api"
      aggregation: sum
      display_format: "{:,.0f}"
      statistical_test: true
      stopping_rule_eligible: true
      visualization_types:
        - radar
        - timeline
        - boxplot
        - evolution
    
    CACHED_TOKENS:
      name: "Cached Tokens"
      description: "Input tokens served from cache (50% cost discount)"
      unit: "tokens"
      category: efficiency
      ideal_direction: maximize
      data_source: "openai_usage_api"
      aggregation: sum
      display_format: "{:,.0f}"
      statistical_test: true
      stopping_rule_eligible: false
      visualization_types:
        - timeline
        - boxplot
    
    T_WALL_seconds:
      name: "Wall-Clock Time"
      description: "Total execution time from start to finish"
      unit: "seconds"
      category: efficiency
      ideal_direction: minimize
      data_source: "orchestrator_timer"
      aggregation: direct
      display_format: "{:.1f}"
      statistical_test: true
      stopping_rule_eligible: true
      visualization_types:
        - radar
        - boxplot
    
    ZDI:
      name: "Zero-Downtime Idle Time"
      description: "Total idle time between steps (seconds)"
      unit: "seconds"
      category: efficiency
      ideal_direction: minimize
      data_source: "step_timing_analysis"
      aggregation: sum
      display_format: "{:.1f}"
      statistical_test: true
      stopping_rule_eligible: false
      visualization_types:
        - boxplot
        - timeline
    
    UTT:
      name: "Utterance Count"
      description: "Total number of steps executed (max 6)"
      unit: "steps"
      category: interaction
      ideal_direction: minimize
      data_source: "orchestrator_step_count"
      aggregation: count
      display_format: "{:.0f}"
      statistical_test: true
      stopping_rule_eligible: false
      visualization_types:
        - boxplot
  
  # Derived metrics calculated from reliable metrics (1 metric)
  derived_metrics:
    COST_USD:
      name: "Total Cost"
      description: "Total USD cost with cache discount applied"
      unit: "USD"
      category: cost
      ideal_direction: minimize
      data_source: "calculated"
      aggregation: calculated
      display_format: "${:.4f}"
      statistical_test: true
      stopping_rule_eligible: true
      visualization_types:
        - radar
        - boxplot
        - timeline
      calculation:
        formula: "(TOK_IN - CACHED_TOKENS) * input_price + CACHED_TOKENS * cached_price + TOK_OUT * output_price"
        dependencies:
          - TOK_IN
          - TOK_OUT
          - CACHED_TOKENS
        pricing_source: "pricing.models"
  
  # Excluded metrics (unreliable or unmeasured - 8 metrics)
  excluded_metrics:
    AUTR:
      name: "Autonomy Rate"
      reason: "Hardcoded HITL detection always returns 0"
      original_formula: "1 - (HIT / 6)"
      status: partial_measurement
      
    AEI:
      name: "Autonomy Efficiency Index"
      reason: "Depends on unreliable AUTR metric"
      original_formula: "AUTR / log(1 + TOK_IN)"
      status: partial_measurement
      
    HIT:
      name: "Human Interventions"
      reason: "HITL detection not implemented in adapters"
      status: not_measured
      
    HEU:
      name: "Human Effort Units"
      reason: "Depends on unmeasured HIT metric"
      original_formula: "HIT * 3"
      status: not_measured
      
    Q_star:
      name: "Quality Score"
      reason: "Quality servers not started (CRUDe, ESR, MC all zero)"
      original_formula: "0.4·ESR + 0.3·(CRUDe/12) + 0.3·MC"
      status: not_measured
      
    ESR:
      name: "Endpoint Success Rate"
      reason: "Quality verification server not running"
      status: not_measured
      
    CRUDe:
      name: "CRUD Coverage"
      reason: "Quality verification server not running"
      status: not_measured
      
    MC:
      name: "Migration Continuity"
      reason: "Quality verification server not running"
      status: not_measured

# =============================================================================
# PRICING CONFIGURATION
# =============================================================================
# OpenAI model pricing (as of 2024-10-19)
# Prices are per million tokens (USD)

pricing:
  models:
    gpt-4o-mini:
      input_price: 0.150        # $0.150 per 1M input tokens
      cached_price: 0.075       # $0.075 per 1M cached input tokens (50% discount)
      output_price: 0.600       # $0.600 per 1M output tokens
      
    gpt-4o:
      input_price: 2.500        # $2.50 per 1M input tokens
      cached_price: 1.250       # $1.25 per 1M cached input tokens (50% discount)
      output_price: 10.000      # $10.00 per 1M output tokens
      
    o1-mini:
      input_price: 3.000        # $3.00 per 1M input tokens
      cached_price: 1.500       # $1.50 per 1M cached input tokens (50% discount)
      output_price: 12.000      # $12.00 per 1M output tokens
      
    o1-preview:
      input_price: 15.000       # $15.00 per 1M input tokens
      cached_price: 7.500       # $7.50 per 1M cached input tokens (50% discount)
      output_price: 60.000      # $60.00 per 1M output tokens

# =============================================================================
# VISUALIZATIONS CONFIGURATION
# =============================================================================
# Chart definitions with metrics, types, and output settings

visualizations:
  radar_chart:
    enabled: true
    title: "Framework Performance Profile"
    metrics:
      - TOK_IN
      - TOK_OUT
      - API_CALLS
      - T_WALL_seconds
      - COST_USD
    scale: normalized       # normalized (0-1) or percentage (0-100%)
    filename: "radar_framework_profile.png"
    
  token_efficiency_scatter:
    enabled: true
    title: "Token Efficiency: Input vs Output"
    x_metric: TOK_IN
    y_metric: TOK_OUT
    filename: "scatter_token_efficiency.png"
    
  api_calls_timeline:
    enabled: true
    title: "API Calls Timeline Across Steps"
    metric: API_CALLS
    x_axis: step_number
    aggregation: mean       # mean, median, or last
    filename: "timeline_api_calls.png"
    
  cost_boxplot:
    enabled: true
    title: "Execution Time Distribution by Framework"
    metric: T_WALL_seconds  # Changed from COST_USD (not available for all frameworks)
    filename: "boxplot_execution_time.png"
    
  api_calls_evolution:
    enabled: true
    title: "API Calls Evolution Across Steps"
    metric: API_CALLS
    steps: [1, 2, 3, 4, 5, 6]
    filename: "evolution_api_calls.png"

# =============================================================================
# REPORT CONFIGURATION
# =============================================================================
# Statistical report sections and their order

report:
  title: "Statistical Analysis Report"
  subtitle: "BAEs Framework Comparison"
  sections:
    # Educational sections providing context
    - name: foundational_concepts
      enabled: true
      order: 1
      title: "📚 Foundational Concepts"
      description: "Essential background knowledge for understanding experiment design and findings"
      
    - name: experimental_methodology
      enabled: true
      order: 2
      title: "Experimental Methodology"
      description: "Research design, protocols, and controlled variables"
      
    - name: metric_definitions
      enabled: true
      order: 3
      title: "Metric Definitions"
      description: "Complete glossary of all metrics with measurement status"
      subsections:
        - name: reliable_metrics
          title: "✅ Reliably Measured Metrics"
        - name: partially_measured
          title: "⚠️ Partially Measured Metrics"
        - name: unmeasured_metrics
          title: "❌ Unmeasured Metrics"
        - name: hitl_detection
          title: "🔍 HITL Detection Implementation Notes"
      
    - name: statistical_methods
      enabled: true
      order: 4
      title: "Statistical Methods Guide"
      description: "Explanation of non-parametric methods and how to interpret results"
      
    - name: executive_summary
      enabled: true
      order: 5
      title: "Executive Summary"
      description: "High-level findings and key takeaways"
      
    # Analysis sections with data
    - name: aggregate_statistics
      enabled: true
      order: 6
      title: "1. Aggregate Statistics (Reliable Metrics Only)"
      description: "Mean values with 95% bootstrap confidence intervals"
      metrics:
        - TOK_IN
        - TOK_OUT
        - API_CALLS
        - CACHED_TOKENS
        - T_WALL_seconds
        - ZDI
        - UTT
      show_performance_indicators: true
      
    - name: relative_performance
      enabled: true
      order: 7
      title: "2. Relative Performance (Reliable Metrics Only)"
      description: "Framework comparisons as ratios and percentages"
      
    - name: cost_analysis
      enabled: false
      order: 8
      title: "Cost Analysis"
      description: "Detailed USD cost breakdown with cache savings"
      show_breakdown: true
      show_per_run: true
      show_cache_efficiency: true
      
    - name: kruskal_wallis
      enabled: true
      order: 9
      title: "3. Kruskal-Wallis H-Tests (Reliable Metrics Only)"
      description: "Testing for significant differences across all frameworks"
      skip_zero_variance: true
      significance_level: 0.05
      
    - name: pairwise_comparisons
      enabled: true
      order: 10
      title: "4. Pairwise Comparisons (Reliable Metrics Only)"
      description: "Dunn-Šidák corrected pairwise tests with Cliff's delta effect sizes"
      skip_zero_variance: true
      significance_level: 0.05
      correction_method: "dunn_sidak"
      
    - name: outlier_detection
      enabled: true
      order: 11
      title: "5. Outlier Detection (Reliable Metrics Only)"
      description: "Values > 3σ from median per framework, per metric"
      threshold_std: 3.0
      
    - name: visual_summary
      enabled: true
      order: 12
      title: "6. Visual Summary (Reliable Metrics Only)"
      description: "Key visualizations for multi-dimensional comparison"
      
    - name: recommendations
      enabled: true
      order: 13
      title: "7. Recommendations"
      description: "Framework selection guidance and decision matrix"
      subsections:
        - name: framework_selection
          title: "🎯 Framework Selection Guidance"
        - name: decision_matrix
          title: "📋 Decision Matrix"
      
    - name: limitations
      enabled: true
      order: 14
      title: "8. Limitations and Future Work"
      description: "Scientific honesty statement and future work roadmap"
      subsections:
        - name: unmeasured_metrics
          title: "❌ Unmeasured Metrics"
        - name: partially_measured
          title: "⚠️ Partially Measured Metrics"
        - name: future_work
          title: "🚀 Future Work Roadmap"
        - name: conclusions
          title: "📊 What We Can Conclude"

# Sprint-based archiving configuration (for multi-sprint runs)
archiving:
  mode: "final_only"  # or "all_sprints"
  compress_intermediate: true
