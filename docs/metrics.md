# Metrics Guide

Comprehensive reference for all 16 metrics collected by the BAEs Experiment Framework.

## Overview

The framework collects **16 standardized metrics** across 4 categories:

1. **Quality Metrics** (5): AUTR, Q*, ESR, CRUDe, MC
2. **Efficiency Metrics** (6): TOK_IN, TOK_OUT, T_WALL, T_USER, T_CPU, AEI
3. **Reliability Metrics** (3): ZDI, RTE, MCI
4. **Process Metrics** (2): ITR, DPL

All metrics are computed automatically after each run and saved to `metrics.json`.

---

## Quality Metrics

### 1. AUTR (Automated User Testing Rate)

**Definition**: Proportion of test cases automatically generated by the framework.

**Formula**:
```
AUTR = (Number of auto-generated tests) / (Total tests)
```

**Range**: [0, 1]
- 0 = No automated testing
- 1 = Fully automated testing

**Interpretation**:
- **High (>0.8)**: Framework generates comprehensive test suites
- **Medium (0.5-0.8)**: Partial test automation
- **Low (<0.5)**: Minimal or no test generation

**Example**:
```json
{
  "auto_tests": 17,
  "manual_tests": 3,
  "AUTR": 0.85  // 17 / (17 + 3)
}
```

**Research Significance**: Indicates framework's ability to reduce manual QA effort.

---

### 2. Q* (Quality Star)

**Definition**: Composite quality score combining emergence, CRUD coverage, and model calls.

**Formula**:
```
Q* = 0.4 · ESR + 0.3 · (CRUDe / 12) + 0.3 · MC
```

**Components**:
- **ESR**: Emerging State Rate (0-1)
- **CRUDe**: CRUD evolution coverage (0-12, normalized)
- **MC**: Model Call efficiency (0-1)

**Weights**:
- ESR (40%): Prioritizes incremental evolution capability
- CRUDe (30%): Values CRUD operation coverage
- MC (30%): Rewards efficient model usage

**Range**: [0, 1]
- 0 = Poor quality
- 1 = Exceptional quality

**Interpretation**:
- **Excellent (>0.8)**: High emergence, full CRUD, efficient calls
- **Good (0.6-0.8)**: Solid quality across dimensions
- **Fair (0.4-0.6)**: Mixed quality results
- **Poor (<0.4)**: Quality concerns

**Example**:
```json
{
  "ESR": 0.83,
  "CRUDe": 10,
  "MC": 0.15,
  "Q*": 0.72  // 0.4·0.83 + 0.3·(10/12) + 0.3·0.15
}
```

**Research Significance**: Single metric for overall framework quality comparison.

---

### 3. ESR (Emerging State Rate)

**Definition**: Proportion of evolution steps that successfully build on previous states.

**Formula**:
```
ESR = (Steps with successful emergence) / (Total steps)
```

**Emergence Criteria**:
- New functionality added without breaking existing features
- Database schema evolved incrementally
- UI components enhanced progressively

**Range**: [0, 1]
- 0 = No successful emergence
- 1 = Perfect incremental evolution

**Interpretation**:
- **High (>0.8)**: Strong incremental development
- **Medium (0.5-0.8)**: Moderate emergence capability
- **Low (<0.5)**: Struggles with state continuity

**Example**:
```json
{
  "successful_steps": 5,
  "total_steps": 6,
  "ESR": 0.83  // 5 / 6
}
```

**Research Significance**: Measures framework's ability to handle evolving requirements without restarts.

---

### 4. CRUDe (CRUD Evolution Coverage)

**Definition**: Count of successfully implemented CRUD operations across evolution steps.

**Operations**:
- **Create**: Insert new entities (e.g., POST /tasks)
- **Read**: Retrieve entities (e.g., GET /tasks, GET /tasks/:id)
- **Update**: Modify entities (e.g., PUT /tasks/:id, PATCH /tasks/:id)
- **Delete**: Remove entities (e.g., DELETE /tasks/:id)

**Multiplier**: × 3 entities (Tasks, Users, Projects) = 12 total operations

**Range**: [0, 12]
- 0 = No CRUD operations
- 12 = Complete CRUD for all entities

**Interpretation**:
- **Complete (12)**: Full CRUD coverage
- **High (9-11)**: Most operations implemented
- **Medium (6-8)**: Partial coverage
- **Low (<6)**: Minimal CRUD support

**Example**:
```json
{
  "tasks_crud": 4,     // C, R, U, D
  "users_crud": 3,     // C, R, U (no delete)
  "projects_crud": 3,  // C, R, U (no delete)
  "CRUDe": 10          // 4 + 3 + 3
}
```

**Research Significance**: Quantifies feature completeness for typical web applications.

---

### 5. MC (Model Call Efficiency)

**Definition**: Inverse of normalized model API calls (lower is better, inverted for scoring).

**Formula**:
```
MC = 1 - (Actual calls / Max expected calls)
```

**Max Expected**: 100 calls (empirically determined baseline)

**Range**: [0, 1]
- 0 = Excessive model calls (>100)
- 1 = Minimal model calls (0, theoretical)

**Interpretation**:
- **Excellent (>0.8)**: <20 calls (very efficient)
- **Good (0.6-0.8)**: 20-40 calls (efficient)
- **Fair (0.4-0.6)**: 40-60 calls (moderate)
- **Poor (<0.4)**: >60 calls (inefficient)

**Example**:
```json
{
  "actual_calls": 15,
  "max_expected": 100,
  "MC": 0.85  // 1 - (15 / 100)
}
```

**Research Significance**: Balances quality with LLM API cost and latency.

---

## Efficiency Metrics

### 6. TOK_IN (Input Tokens)

**Definition**: Total tokens sent to LLM APIs across all steps.

**Formula**:
```
TOK_IN = Σ (input tokens per API call)
```

**Source**: OpenAI Usage API verification

**Range**: [0, ∞)
- Typical: 5,000 - 50,000 tokens per run

**Interpretation**:
- **Low (<10,000)**: Efficient prompting
- **Medium (10,000-30,000)**: Moderate token usage
- **High (>30,000)**: Verbose prompting or multiple iterations

**Example**:
```json
{
  "step_1_input": 2500,
  "step_2_input": 3200,
  "step_3_input": 2800,
  "step_4_input": 2100,
  "step_5_input": 1900,
  "step_6_input": 2000,
  "TOK_IN": 14500
}
```

**Cost Calculation** (GPT-4):
```
Cost = TOK_IN × $0.03 / 1000
     = 14,500 × $0.03 / 1000
     = $0.435
```

**Research Significance**: Primary cost driver for LLM-based frameworks.

---

### 7. TOK_OUT (Output Tokens)

**Definition**: Total tokens received from LLM APIs across all steps.

**Formula**:
```
TOK_OUT = Σ (output tokens per API call)
```

**Source**: OpenAI Usage API verification

**Range**: [0, ∞)
- Typical: 2,000 - 15,000 tokens per run

**Interpretation**:
- **Low (<5,000)**: Concise code generation
- **Medium (5,000-10,000)**: Moderate verbosity
- **High (>10,000)**: Verbose or comprehensive outputs

**Example**:
```json
{
  "step_1_output": 800,
  "step_2_output": 950,
  "step_3_output": 720,
  "step_4_output": 680,
  "step_5_output": 600,
  "step_6_output": 650,
  "TOK_OUT": 4400
}
```

**Cost Calculation** (GPT-4):
```
Cost = TOK_OUT × $0.06 / 1000
     = 4,400 × $0.06 / 1000
     = $0.264
```

**Research Significance**: Secondary cost driver; indicates output verbosity.

---

### 8. T_WALL (Wall-Clock Time)

**Definition**: Total elapsed time from run start to completion.

**Formula**:
```
T_WALL = end_timestamp - start_timestamp
```

**Unit**: Seconds

**Range**: [0, ∞)
- Typical: 900 - 3,600 seconds (15-60 minutes)

**Interpretation**:
- **Fast (<1,200s)**: <20 minutes
- **Medium (1,200-2,400s)**: 20-40 minutes
- **Slow (>2,400s)**: >40 minutes

**Example**:
```json
{
  "start": "2025-10-08T14:23:15Z",
  "end": "2025-10-08T15:03:52Z",
  "T_WALL": 2437  // 40 minutes 37 seconds
}
```

**Research Significance**: User-perceived latency; includes API wait time, I/O, framework overhead.

---

### 9. T_USER (User Time)

**Definition**: CPU time spent in user mode (application code execution).

**Formula**:
```
T_USER = Σ (user CPU time per process)
```

**Source**: `time` command or `psutil`

**Unit**: Seconds

**Range**: [0, T_WALL]
- Typical: 50 - 300 seconds

**Interpretation**:
- **Low (<100s)**: Lightweight processing
- **Medium (100-200s)**: Moderate computation
- **High (>200s)**: CPU-intensive operations

**Example**:
```json
{
  "T_USER": 125.3,
  "T_WALL": 2437,
  "user_ratio": 0.051  // 5.1% of wall time
}
```

**Research Significance**: Actual CPU work done (excludes I/O wait, API latency).

---

### 10. T_CPU (System Time)

**Definition**: CPU time spent in kernel mode (system calls, I/O).

**Formula**:
```
T_CPU = Σ (system CPU time per process)
```

**Source**: `time` command or `psutil`

**Unit**: Seconds

**Range**: [0, T_WALL]
- Typical: 5 - 50 seconds

**Interpretation**:
- **Low (<10s)**: Minimal system overhead
- **Medium (10-30s)**: Moderate I/O activity
- **High (>30s)**: Heavy file operations or network I/O

**Example**:
```json
{
  "T_CPU": 12.8,
  "T_WALL": 2437,
  "cpu_ratio": 0.005  // 0.5% of wall time
}
```

**Research Significance**: Overhead from file I/O, network calls, OS operations.

---

### 11. AEI (API Efficiency Index)

**Definition**: Ratio of automated testing to logarithmic token consumption.

**Formula**:
```
AEI = AUTR / log(1 + TOK_IN)
```

**Components**:
- **AUTR**: Automated testing rate (0-1)
- **log(1 + TOK_IN)**: Diminishing returns on token usage

**Range**: [0, ∞)
- Typical: 0.02 - 0.10

**Interpretation**:
- **Excellent (>0.08)**: High testing with low token cost
- **Good (0.05-0.08)**: Balanced efficiency
- **Fair (0.03-0.05)**: Moderate efficiency
- **Poor (<0.03)**: Low testing or high token consumption

**Example**:
```json
{
  "AUTR": 0.85,
  "TOK_IN": 14500,
  "AEI": 0.088  // 0.85 / log(1 + 14500) = 0.85 / 9.58
}
```

**Research Significance**: Quality-per-token metric; identifies cost-effective frameworks.

---

## Reliability Metrics

### 12. ZDI (Zero-Downtime Incidents)

**Definition**: Count of incidents where application became unavailable during evolution.

**Detection**:
- HTTP 500 errors during step transitions
- Database connection failures
- Crashed processes

**Formula**:
```
ZDI = Count of downtime incidents
```

**Range**: [0, ∞)
- Target: 0 (zero downtime)
- Acceptable: 1-2 (minor incidents)
- Poor: >2 (unreliable evolution)

**Interpretation**:
- **Excellent (0)**: Seamless evolution
- **Good (1)**: Minor hiccup, quick recovery
- **Poor (>2)**: Frequent availability issues

**Example**:
```json
{
  "step_3_downtime": 1,  // 30-second outage during migration
  "step_5_downtime": 0,
  "ZDI": 1
}
```

**Research Significance**: Critical for production-safe evolution; zero-downtime deployment capability.

---

### 13. RTE (Runtime Errors)

**Definition**: Proportion of evolution steps with uncaught exceptions or crashes.

**Formula**:
```
RTE = (Steps with runtime errors) / (Total steps)
```

**Error Types**:
- Uncaught exceptions
- Segmentation faults
- Out-of-memory errors

**Range**: [0, 1]
- 0 = No runtime errors
- 1 = Errors in every step

**Interpretation**:
- **Excellent (0)**: Robust code generation
- **Good (<0.2)**: Minor issues, mostly stable
- **Poor (>0.2)**: Unreliable framework

**Example**:
```json
{
  "step_2_error": true,  // NullPointerException
  "step_4_error": false,
  "error_count": 1,
  "total_steps": 6,
  "RTE": 0.167  // 1 / 6
}
```

**Research Significance**: Code quality indicator; lower is better.

---

### 14. MCI (Model Call Interruptions)

**Definition**: Count of API call failures requiring retries.

**Failure Types**:
- Network timeouts
- Rate limit errors (429)
- API server errors (500, 502, 503)

**Formula**:
```
MCI = Count of interrupted API calls
```

**Range**: [0, ∞)
- Ideal: 0
- Acceptable: 1-3 (transient issues)
- Poor: >3 (persistent problems)

**Interpretation**:
- **Excellent (0)**: Reliable API connectivity
- **Good (1-2)**: Minor transient failures
- **Poor (>3)**: Network or rate limit issues

**Example**:
```json
{
  "step_3_retry": 1,  // 429 rate limit, retried successfully
  "step_5_retry": 1,  // Timeout, retried successfully
  "MCI": 2
}
```

**Research Significance**: External dependency reliability; may indicate need for retry strategies.

---

## Process Metrics

### 15. ITR (Iterations)

**Definition**: Number of framework invocations required to complete all steps.

**Scenarios**:
- **Ideal**: 6 iterations (1 per step, no retries)
- **Retries**: >6 iterations (failures requiring re-execution)
- **Incremental**: Framework may batch multiple steps

**Formula**:
```
ITR = Total framework invocations
```

**Range**: [1, ∞)
- Ideal: 6 (one-shot success)
- Acceptable: 7-10 (minor retries)
- Poor: >10 (frequent failures)

**Interpretation**:
- **Excellent (6)**: Perfect execution
- **Good (7-9)**: Minor failures, successful retries
- **Poor (>10)**: Unreliable, many retries

**Example**:
```json
{
  "step_1_attempts": 1,
  "step_2_attempts": 2,  // Failed once, succeeded on retry
  "step_3_attempts": 1,
  "step_4_attempts": 1,
  "step_5_attempts": 1,
  "step_6_attempts": 1,
  "ITR": 7  // 1+2+1+1+1+1
}
```

**Research Significance**: Robustness indicator; fewer iterations = more reliable framework.

---

### 16. DPL (Deployment Success)

**Definition**: Binary indicator of successful final deployment.

**Success Criteria**:
- Application starts without errors
- All endpoints respond (HTTP 200)
- Database connections established
- UI accessible

**Formula**:
```
DPL = 1 if deployment successful, else 0
```

**Range**: {0, 1}
- 0 = Deployment failed
- 1 = Deployment succeeded

**Interpretation**:
- **Success (1)**: Production-ready artifact
- **Failure (0)**: Unusable output

**Example**:
```json
{
  "deployment_status": "success",
  "endpoints_tested": 12,
  "endpoints_passing": 12,
  "DPL": 1
}
```

**Research Significance**: Ultimate framework success criterion; binary go/no-go metric.

---

## Composite Scores

### Q* (Quality Star)

Already covered in Quality Metrics section.

**Summary**:
```
Q* = 0.4 · ESR + 0.3 · (CRUDe / 12) + 0.3 · MC
```

### AEI (API Efficiency Index)

Already covered in Efficiency Metrics section.

**Summary**:
```
AEI = AUTR / log(1 + TOK_IN)
```

---

## Metric Relationships

### Quality vs Efficiency Trade-off

**Pareto Frontier**: Frameworks balancing high Q* with low TOK_IN

```
High Q*, Low TOK_IN → Pareto Optimal
High Q*, High TOK_IN → Quality at cost
Low Q*, Low TOK_IN → Efficient but incomplete
Low Q*, High TOK_IN → Worst case
```

### Reliability vs Speed Trade-off

**Negative Correlation**: Faster frameworks (low T_WALL) may have higher RTE

```
Low T_WALL, Low RTE → Ideal (fast and reliable)
Low T_WALL, High RTE → Rushed, error-prone
High T_WALL, Low RTE → Slow but careful
High T_WALL, High RTE → Inefficient and unreliable
```

---

## Interpretation Guidelines

### Individual Metrics

**For each metric, consider:**
1. **Absolute Value**: Is it within acceptable range?
2. **Relative Comparison**: How does it compare to other frameworks?
3. **Trend**: Improving or degrading over runs?
4. **Outliers**: Are there anomalous values?

### Aggregate Analysis

**When comparing frameworks:**
1. **Statistical Significance**: Use Kruskal-Wallis test (p < 0.05)
2. **Effect Size**: Compute Cliff's delta (|δ| > 0.147 = meaningful)
3. **Confidence**: Bootstrap 95% CI for uncertainty quantification
4. **Convergence**: Check if additional runs needed

---

## Metric Collection

### Automated Collection

All metrics collected automatically by `MetricsCollector`:

```python
from src.orchestrator.metrics_collector import MetricsCollector

collector = MetricsCollector(workspace_dir)
metrics = collector.collect_metrics(
    adapter_results,
    validation_results,
    wall_time=2437.5,
    api_usage={"input": 14500, "output": 4400}
)
```

### Manual Verification

Verify specific metrics:

```bash
# Check token usage via OpenAI API
cat runs/baes/<run-id>/metrics.json | jq '.TOK_IN, .TOK_OUT'

# Check timing
cat runs/baes/<run-id>/event_log.jsonl | head -1  # Start
cat runs/baes/<run-id>/event_log.jsonl | tail -1  # End

# Check CRUD coverage
cat runs/baes/<run-id>/validation_results.json | jq '.crud_coverage'
```

---

## Further Reading

- **[Architecture Guide](./architecture.md)**: How metrics are collected
- **[Quickstart Guide](./quickstart.md)**: Running experiments to collect metrics
- **Statistical Analysis**: `src/analysis/statistics.py` for metric aggregation

---

## References

1. **AUTR**: Adapted from automated testing coverage metrics
2. **Q***: Novel composite score for framework quality
3. **ESR**: Based on incremental development research
4. **CRUDe**: Standard CRUD operation taxonomy
5. **Token Metrics**: OpenAI Usage API standards
6. **AEI**: Original efficiency metric for LLM frameworks
