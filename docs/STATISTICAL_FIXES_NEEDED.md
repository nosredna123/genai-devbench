# Statistical Analysis Fixes Required

Based on expert review of statistical_report_full.md (2025-10-30), the following critical issues must be addressed:

## Phase 1: Critical Issues (Immediate)

### 1. Cliff's Delta Confidence Intervals [1, 1] ‚úÖ RESOLVED

**Status**: Investigation complete - bootstrap implementation is correct

**Issue**: Expert review flagged CIs of [1.0, 1.0] as "mathematically impossible"

**Investigation Results**:
- ‚úÖ Bootstrap implementation is **CORRECT** (uses proper independent group resampling)
- ‚úÖ Tested with separated groups ‚Üí CI = [1.0, 1.0] (mathematically valid)
- ‚úÖ Tested with overlapping groups ‚Üí CI = [0.574, 1.0] (proper variation)

**Root Cause**: BAES framework has **zero variance** on some metrics:
- `cached_tokens`: All 66 runs produce identical values (std ‚âà 0)
- `tokens_total`: Near-zero variance
- Creates complete separation from other frameworks
- Bootstrap correctly maintains this ‚Üí deterministic CIs

**Solution Implemented**: ‚úÖ **COMPLETE**
- Added zero-variance detection (SD < 0.01 OR IQR < 0.01)
- Skip Cohen's d when zero variance detected (would be invalid)
- Add warning for Cliff's Delta deterministic CIs
- Log variance diagnostics (SD and IQR for both groups)
- 5 unit tests added and passing

**Files Modified**:
- `src/paper_generation/statistical_analyzer.py` (lines 1793-1865)
- `tests/test_zero_variance_detection.py` (new file)

### 2. Cohen's d > 5 ‚úÖ FIXED

**Status**: Fixed via zero-variance detection

###3. Post-hoc Power Analysis ‚úÖ FIXED

**Status**: Post-hoc power reporting disabled in report generation

**Problem**:
- "Achieved power = 0.995-1.000" for all metrics is meaningless
- Post-hoc power always mirrors p-values (circular logic)
- Power for Kruskal-Wallis cannot be derived from ANOVA functions

**Root Cause**: Using observed effect sizes to calculate power (tautological)

**Expert Guidance**:
> "Post-hoc power is generally discouraged because it's directly related to p-value"
> "When p < 0.05, power will be high; when p > 0.05, it will be low"
> "This adds no information beyond what the p-value tells us"

**Solution Implemented**: ‚úÖ **COMPLETE**
- Disabled Power Analysis section in statistical_report_full.md
- Section 5 (Power Analysis) excluded from report output
- Table of Contents updated (removed Power Analysis entry)
- Section numbers renumbered (Methodology: 6‚Üí5, Glossary: 7‚Üí6)
- All power calculation code preserved for backward compatibility
- Added detailed comments explaining rationale

**Implementation Details**:
```python
# In experiment_analyzer.py:
# Power Analysis section commented out with explanatory note:
# "Post-hoc power (calculated from observed data) is directly 
#  correlated with p-values and provides no independent information 
#  about study adequacy."
```

**Files Modified**:
- `src/paper_generation/experiment_analyzer.py` (lines 687-693, 847-857, 860, 874)
  - Removed Power Analysis from Table of Contents
  - Disabled _generate_power_analysis_section() call
  - Renumbered subsequent sections
  - Added detailed comments explaining why power analysis is disabled

**Recommendation**: For prospective studies, use *a priori* power analysis with expected effect sizes

## ‚ö†Ô∏è Moderate Issues (Should Fix)

### 4. Multiple Comparison Correction
**Problem**: 21 pairwise comparisons without Bonferroni/Holm correction

**Fix Required**:
```python
from statsmodels.stats.multitest import multipletests

# After all pairwise tests
p_values = [comp['p_value'] for comp in comparisons]
rejected, p_adjusted, _, _ = multipletests(p_values, alpha=0.05, method='holm')

# Update comparisons with adjusted p-values
for comp, p_adj in zip(comparisons, p_adjusted):
    comp['p_value_adjusted'] = p_adj
```

**Files to Fix**:
- `src/paper_generation/statistical_analyzer.py` - Add multiple testing correction

### 5. Kruskal-Wallis vs Negligible Pairwise Effects
**Problem**: Significant omnibus test but Œ¥ = 0.069 (negligible) pairwise

**Explanation**: 
- Omnibus test detects *any* difference among groups
- Pairwise tests may show small individual effects
- This is expected but should be explained

**Fix Required**: Add interpretation note in report generation

### 6. Zero-Variance BAES Metrics
**Problem**: BAES variance ‚âà 0 for cached_tokens, tokens_total

**Investigation Needed**:
- Check if BAES truly has identical values across all runs
- If so, this is a data characteristic, not a bug
- But standardized effect sizes are inappropriate

**Fix Required**: 
- Detect and flag zero-variance groups
- Skip or replace effect size calculations
- Report as "categorical difference" instead

## ‚ö†Ô∏è Minor Issues (Nice to Have)

### 7. Over-Confident Language
**Problem**: "systematically higher (99.2%)" ‚Üí false certainty

**Fix Required**: 
```python
# Change wording in templates
"ghspec systematically outperforms" 
‚Üí "ghspec tended to show higher values"

"99.2% probability"
‚Üí "‚âà99% probability based on sample ranks"
```

**Files to Fix**:
- `src/paper_generation/educational_content.py` - Tone down certainty

### 8. Missing Outlier Handling Documentation
**Problem**: Outliers identified but no mention of winsorization/trimming

**Fix Required**: Add section documenting outlier treatment policy

### 9. Inconsistent Numeric Formatting
**Problem**: Mix of 2 and 3 decimal places

**Fix Required**: Standardize to 3 decimals for effect sizes, p-values

## üìã Implementation Priority

### Phase 1 (Critical - Must Fix Before Publication)
1. ‚úÖ Fix Cliff's Delta bootstrap implementation
2. ‚úÖ Add zero-variance detection and skip Cohen's d when invalid
3. ‚úÖ Remove or replace post-hoc power analysis

### Phase 2 (Important - Should Fix)
4. ‚úÖ Add multiple comparison correction (Holm-Bonferroni)
5. ‚úÖ Document zero-variance BAES issue
6. ‚úÖ Add practical significance interpretation

### Phase 3 (Polish - Nice to Have)
7. ‚úÖ Moderate language for academic tone
8. ‚úÖ Document outlier handling
9. ‚úÖ Standardize numeric formatting

## üîß Testing Strategy

### Unit Tests Needed
- [ ] Test Cliff's Delta bootstrap produces varying CIs
- [ ] Test zero-variance detection
- [ ] Test multiple comparison correction
- [ ] Test Cohen's d validation

### Integration Tests Needed
- [ ] Generate report with fixed code on real data
- [ ] Verify no [1.000, 1.000] CIs
- [ ] Verify adjusted p-values < raw p-values
- [ ] Verify warnings for zero-variance metrics

### Validation
- [ ] Compare bootstrap CIs against scipy.stats implementation
- [ ] Verify power calculations match statsmodels
- [ ] Cross-check effect sizes with published benchmarks

## üìä Expected Outcomes

After fixes:
- Cliff's Delta CIs will vary: e.g., [0.94, 1.00] instead of [1.00, 1.00]
- Cohen's d will be marked as "invalid" for zero-variance metrics
- No more "power = 1.000" claims
- p-values adjusted for multiple comparisons
- More conservative, academically appropriate language

## üéØ Success Criteria

1. All Cliff's Delta CIs have non-zero width
2. No Cohen's d > 5 without "variance violation" warning
3. Power analysis removed or replaced with a priori calculations
4. Multiple comparison correction applied (p_adjusted ‚â• p_raw)
5. Expert review confirms methodological soundness
