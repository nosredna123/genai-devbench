# Quickstart Guide

Get up and running with the BAEs Experiment Framework in minutes.

## Prerequisites

### Required Software

- **Python 3.11+**: Core runtime environment
- **Git**: Version control for framework repositories
- **Bash**: Shell scripting support (Linux/macOS)

### Optional Software

- **Docker**: For containerized framework isolation (recommended)
- **Virtual Environment**: `venv` or `conda` for Python dependency isolation

### API Keys

You'll need API keys for the frameworks you want to test:

- **BAEs Framework**: OpenAI API key
- **ChatDev Framework**: OpenAI API key
- **GitHub Spec-kit**: GitHub token (optional, for rate limiting)

## Installation

### 1. Clone the Repository

```bash
git clone https://github.com/gesad-lab/baes_experiment.git
cd baes_experiment
```

### 2. Create Virtual Environment (Recommended)

```bash
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

### 4. Configure Environment

Copy the example environment file and add your API keys:

```bash
cp .env.example .env
```

Edit `.env` and add your keys:

```bash
BAES_API_KEY=sk-your-openai-key-here
CHATDEV_API_KEY=sk-your-openai-key-here
GHSPEC_API_KEY=ghp_your-github-token-here  # Optional
```

### 5. Verify Configuration

Check that your configuration is valid:

```bash
python3 -m src.orchestrator --help
```

You should see the CLI usage information.

## First Run

### Run a Single Framework Test

Execute a single run with the BAEs framework:

```bash
./runners/run_experiment.sh baes
```

**What happens:**
1. Creates an isolated workspace in `runs/baes/<run-id>/`
2. Clones the BAEs framework repository
3. Executes all 6 CRUD evolution steps
4. Validates outputs at each step
5. Collects 16 metrics (AUTR, Q*, TOK_IN, etc.)
6. Archives results with SHA-256 hash
7. Logs all events to `event_log.jsonl`

**Expected duration:** 15-30 minutes (depending on framework and API latency)

### View Results

After the run completes, check the outputs:

```bash
# View metrics
cat runs/baes/<run-id>/metrics.json

# View event log
cat runs/baes/<run-id>/event_log.jsonl

# View HITL events (for reproducibility)
cat runs/baes/<run-id>/hitl_events.jsonl

# View archive
ls -lh runs/baes/<run-id>/archive.tar.gz
```

### Run Multi-Framework Comparison

To compare multiple frameworks until convergence:

```bash
./runners/run_experiment.sh --multi baes chatdev ghspec
```

**What happens:**
1. Runs all 3 frameworks in parallel (default: 5 runs minimum)
2. Checks convergence after each run using bootstrap CI
3. Stops when 95% CI half-width < 10% of mean
4. Maximum 25 runs per framework
5. Results saved in `runs/<framework>/<run-id>/`

**Expected duration:** 1-3 hours (depending on convergence rate)

## Analyze Results

After collecting run data, generate analysis and visualizations:

```bash
./runners/analyze_results.sh ./analysis_output
```

**Outputs:**
- `radar_chart.svg`: Multi-framework comparison across 6 metrics
- `pareto_plot.svg`: Quality vs cost trade-off visualization
- `timeline_chart.svg`: CRUD coverage evolution over steps
- `report.md`: Comprehensive statistical report with:
  - Aggregate statistics (mean, median, 95% CI)
  - Kruskal-Wallis tests
  - Pairwise comparisons with Cliff's delta
  - Outlier detection

**View the report:**

```bash
cat analysis_output/report.md
```

**View visualizations:**

Open the SVG files in a web browser or image viewer:

```bash
firefox analysis_output/radar_chart.svg  # Or your preferred browser
```

## Expected Outputs

### Metrics File (`metrics.json`)

```json
{
  "AUTR": 0.85,
  "Q*": 0.72,
  "AEI": 0.043,
  "TOK_IN": 12500,
  "TOK_OUT": 3200,
  "T_WALL": 1847.5,
  "T_USER": 245.3,
  "T_CPU": 12.8,
  "CRUDe": 10,
  "ESR": 0.83,
  "MC": 0.15,
  "ZDI": 0,
  "RTE": 0.08,
  "MCI": 0,
  "ITR": 3,
  "DPL": 1
}
```

### Event Log (`event_log.jsonl`)

Each line is a JSON event:

```json
{"timestamp": "2025-10-08T14:23:15.123456Z", "level": "INFO", "message": "Starting BAEs run", "run_id": "baes-20251008-142315-abc123"}
{"timestamp": "2025-10-08T14:23:18.456789Z", "level": "INFO", "message": "Executing step 1", "step": 1, "prompt": "..."}
```

### HITL Events (`hitl_events.jsonl`)

SHA-1 hashed HITL clarifications for reproducibility:

```json
{"timestamp": "2025-10-08T14:25:30.123456Z", "step": 2, "hitl_response": "Use PostgreSQL 14 with JSON support", "sha1": "a1b2c3d4e5f6..."}
```

### Archive (`archive.tar.gz`)

Compressed tarball with SHA-256 checksum containing:
- All source code generated by framework
- Logs and intermediate outputs
- Configuration snapshots

## Next Steps

- **Reproducibility**: See [Reproducibility Guide](./reproducibility.md) for deterministic runs
- **Configuration**: See [Configuration Guide](./configuration.md) for advanced options
- **Metrics**: See [Metrics Guide](./metrics.md) for detailed metric definitions
- **Architecture**: See [Architecture Guide](./architecture.md) for system design
- **Troubleshooting**: See [Troubleshooting Guide](./troubleshooting.md) for common issues

## Common Issues

### API Rate Limits

If you encounter rate limit errors:

1. Add delays between steps in `config/experiment.yaml`:
   ```yaml
   timeout_seconds: 3600
   retry_attempts: 3
   retry_delay: 60  # Wait 60 seconds between retries
   ```

2. Use a paid OpenAI API key with higher rate limits

### Framework Clone Failures

If Git clone fails:

1. Check internet connectivity
2. Verify framework repository URL in `config/experiment.yaml`
3. Ensure Git is installed and accessible

### Disk Space

Each run consumes ~100-500 MB. Ensure sufficient disk space:

```bash
df -h .  # Check available space
```

Clean old runs if needed:

```bash
rm -rf runs/*/  # WARNING: Deletes all run data
```

## Support

For additional help:

- **Issues**: [GitHub Issues](https://github.com/gesad-lab/baes_experiment/issues)
- **Documentation**: Browse the `docs/` directory
- **Logs**: Check `event_log.jsonl` for detailed execution traces
