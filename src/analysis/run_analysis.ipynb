{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ba8a3d5",
   "metadata": {},
   "source": [
    "# Run Analysis & Data Quality Checks\n",
    "\n",
    "This notebook validates experimental runs according to the **008-fix-zero-tokens** data model.\n",
    "\n",
    "## Key Changes (v2.0.0):\n",
    "- Token metrics (`TOK_IN`, `TOK_OUT`, `API_CALLS`, `CACHED_TOKENS`) are **only** at run-level in `aggregate_metrics`\n",
    "- Steps array contains **only** timing and status data (no tokens)\n",
    "- Reconciliation status tracked in `usage_api_reconciliation` section\n",
    "\n",
    "## What We Check:\n",
    "1. **Run completeness**: All expected files present\n",
    "2. **Data model compliance**: No token fields in steps array\n",
    "3. **Reconciliation status**: Verification progress tracking\n",
    "4. **Token accuracy**: Run-level totals validation\n",
    "5. **Step integrity**: Timing and status consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da49310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Configuration\n",
    "RUNS_DIR = '../runs/'  # Relative path from analysis directory\n",
    "\n",
    "print(\"üìä Run Analysis Notebook\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Runs directory: {RUNS_DIR}\")\n",
    "print(f\"Analysis started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ff81a9",
   "metadata": {},
   "source": [
    "## 1. Load and Parse All Runs\n",
    "\n",
    "Scan the runs directory and load all `metrics.json` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9f4c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_runs(runs_directory=RUNS_DIR):\n",
    "    \"\"\"Load all metrics.json files from the runs directory.\"\"\"\n",
    "    runs_data = []\n",
    "    missing_metrics = []\n",
    "    parse_errors = []\n",
    "    \n",
    "    runs_path = Path(runs_directory)\n",
    "    if not runs_path.exists():\n",
    "        print(f\"‚ö†Ô∏è  Runs directory not found: {runs_directory}\")\n",
    "        return [], []\n",
    "    \n",
    "    # Iterate through framework directories\n",
    "    for framework_dir in runs_path.iterdir():\n",
    "        if not framework_dir.is_dir() or framework_dir.name.startswith('.'):\n",
    "            continue\n",
    "            \n",
    "        framework_name = framework_dir.name\n",
    "        \n",
    "        # Iterate through run directories\n",
    "        for run_dir in framework_dir.iterdir():\n",
    "            if not run_dir.is_dir() or run_dir.name.startswith('.'):\n",
    "                continue\n",
    "                \n",
    "            run_id = run_dir.name\n",
    "            metrics_file = run_dir / 'metrics.json'\n",
    "            \n",
    "            if not metrics_file.exists():\n",
    "                missing_metrics.append({\n",
    "                    'framework': framework_name,\n",
    "                    'run_id': run_id,\n",
    "                    'issue': 'metrics.json not found'\n",
    "                })\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                with open(metrics_file, 'r') as f:\n",
    "                    metrics = json.load(f)\n",
    "                \n",
    "                runs_data.append({\n",
    "                    'framework': framework_name,\n",
    "                    'run_id': run_id,\n",
    "                    'metrics_file': str(metrics_file),\n",
    "                    'data': metrics\n",
    "                })\n",
    "            except json.JSONDecodeError as e:\n",
    "                parse_errors.append({\n",
    "                    'framework': framework_name,\n",
    "                    'run_id': run_id,\n",
    "                    'issue': f'JSON parse error: {str(e)}'\n",
    "                })\n",
    "            except Exception as e:\n",
    "                parse_errors.append({\n",
    "                    'framework': framework_name,\n",
    "                    'run_id': run_id,\n",
    "                    'issue': f'Unexpected error: {str(e)}'\n",
    "                })\n",
    "    \n",
    "    print(f\"‚úì Found {len(runs_data)} valid runs\")\n",
    "    if missing_metrics:\n",
    "        print(f\"‚ö†Ô∏è  {len(missing_metrics)} runs missing metrics.json\")\n",
    "    if parse_errors:\n",
    "        print(f\"‚ùå {len(parse_errors)} runs with parse errors\")\n",
    "    \n",
    "    return runs_data, missing_metrics + parse_errors\n",
    "\n",
    "# Load all runs\n",
    "runs_data, issues = load_all_runs()\n",
    "print(f\"\\nüìä Total runs loaded: {len(runs_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569decf5",
   "metadata": {},
   "source": [
    "## 2. Data Model Compliance Check\n",
    "\n",
    "Verify that runs follow the v2.0.0 data model (no token fields in steps array)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a19972",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_model_compliance(runs_data):\n",
    "    \"\"\"Check that runs follow v2.0.0 data model (no tokens in steps).\"\"\"\n",
    "    violations = []\n",
    "    \n",
    "    # Token fields that should NOT be in steps\n",
    "    forbidden_fields = ['tokens_in', 'tokens_out', 'api_calls', 'cached_tokens']\n",
    "    \n",
    "    for run in runs_data:\n",
    "        framework = run['framework']\n",
    "        run_id = run['run_id']\n",
    "        metrics = run['data']\n",
    "        \n",
    "        # Check steps array\n",
    "        steps = metrics.get('steps', [])\n",
    "        for step_idx, step in enumerate(steps):\n",
    "            for field in forbidden_fields:\n",
    "                if field in step:\n",
    "                    violations.append({\n",
    "                        'framework': framework,\n",
    "                        'run_id': run_id,\n",
    "                        'step': step.get('step', step_idx + 1),\n",
    "                        'violation': f'Forbidden field \"{field}\" in steps array',\n",
    "                        'value': step[field]\n",
    "                    })\n",
    "    \n",
    "    if violations:\n",
    "        print(f\"‚ùå Found {len(violations)} data model violations\")\n",
    "        violations_df = pd.DataFrame(violations)\n",
    "        return violations_df\n",
    "    else:\n",
    "        print(\"‚úÖ All runs comply with v2.0.0 data model\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Check compliance\n",
    "violations_df = check_data_model_compliance(runs_data)\n",
    "if not violations_df.empty:\n",
    "    print(\"\\nViolations by framework:\")\n",
    "    print(violations_df.groupby('framework').size())\n",
    "    display(violations_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe71671",
   "metadata": {},
   "source": [
    "## 3. Reconciliation Status Overview\n",
    "\n",
    "Check the reconciliation status for all runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb3ac2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_reconciliation_status(runs_data):\n",
    "    \"\"\"Analyze reconciliation status across all runs.\"\"\"\n",
    "    status_data = []\n",
    "    \n",
    "    for run in runs_data:\n",
    "        framework = run['framework']\n",
    "        run_id = run['run_id']\n",
    "        metrics = run['data']\n",
    "        \n",
    "        reconciliation = metrics.get('usage_api_reconciliation', {})\n",
    "        status = reconciliation.get('verification_status', 'unknown')\n",
    "        attempts = reconciliation.get('attempts', [])\n",
    "        verified_at = reconciliation.get('verified_at')\n",
    "        \n",
    "        # Get latest attempt details\n",
    "        latest_attempt = attempts[-1] if attempts else {}\n",
    "        \n",
    "        # Handle verified_at - could be timestamp, string, or None\n",
    "        verified_at_str = None\n",
    "        if verified_at:\n",
    "            if isinstance(verified_at, str):\n",
    "                # Already a string, use as-is\n",
    "                verified_at_str = verified_at\n",
    "            elif isinstance(verified_at, (int, float)):\n",
    "                # It's a timestamp, convert it\n",
    "                try:\n",
    "                    verified_at_str = datetime.fromtimestamp(verified_at).strftime('%Y-%m-%d %H:%M:%S')\n",
    "                except (ValueError, OSError):\n",
    "                    # Invalid timestamp\n",
    "                    verified_at_str = f\"Invalid timestamp: {verified_at}\"\n",
    "        \n",
    "        status_data.append({\n",
    "            'framework': framework,\n",
    "            'run_id': run_id,\n",
    "            'status': status,\n",
    "            'attempt_count': len(attempts),\n",
    "            'verified_at': verified_at_str,\n",
    "            'latest_tokens_in': latest_attempt.get('total_tokens_in', 0),\n",
    "            'latest_tokens_out': latest_attempt.get('total_tokens_out', 0),\n",
    "            'latest_api_calls': latest_attempt.get('total_api_calls', 0)\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(status_data)\n",
    "    \n",
    "    print(\"üìä Reconciliation Status Summary\")\n",
    "    print(\"=\" * 60)\n",
    "    if not df.empty:\n",
    "        print(df['status'].value_counts())\n",
    "        print(f\"\\nTotal runs: {len(df)}\")\n",
    "        print(f\"Verified: {len(df[df['status'] == 'verified'])}\")\n",
    "        print(f\"Pending: {len(df[df['status'] == 'pending'])}\")\n",
    "        print(f\"Failed: {len(df[df['status'] == 'failed'])}\")\n",
    "    else:\n",
    "        print(\"No runs found\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Analyze reconciliation\n",
    "reconciliation_df = analyze_reconciliation_status(runs_data)\n",
    "if not reconciliation_df.empty:\n",
    "    display(reconciliation_df.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336aea43",
   "metadata": {},
   "source": [
    "## 4. Run-Level Token Analysis\n",
    "\n",
    "Validate run-level token metrics from `aggregate_metrics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3ac397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_token_metrics(runs_data):\n",
    "    \"\"\"Analyze run-level token metrics from aggregate_metrics.\"\"\"\n",
    "    token_data = []\n",
    "    issues = []\n",
    "    \n",
    "    for run in runs_data:\n",
    "        framework = run['framework']\n",
    "        run_id = run['run_id']\n",
    "        metrics = run['data']\n",
    "        \n",
    "        agg = metrics.get('aggregate_metrics', {})\n",
    "        reconciliation = metrics.get('usage_api_reconciliation', {})\n",
    "        \n",
    "        tok_in = agg.get('TOK_IN', 0)\n",
    "        tok_out = agg.get('TOK_OUT', 0)\n",
    "        api_calls = agg.get('API_CALLS', 0)\n",
    "        cached = agg.get('CACHED_TOKENS', 0)\n",
    "        cost = agg.get('COST_USD', 0)\n",
    "        status = reconciliation.get('verification_status', 'unknown')\n",
    "        \n",
    "        # Check for zero tokens in verified runs\n",
    "        if status == 'verified' and (tok_in == 0 or tok_out == 0):\n",
    "            issues.append({\n",
    "                'framework': framework,\n",
    "                'run_id': run_id,\n",
    "                'issue': 'Zero tokens in verified run',\n",
    "                'tok_in': tok_in,\n",
    "                'tok_out': tok_out\n",
    "            })\n",
    "        \n",
    "        token_data.append({\n",
    "            'framework': framework,\n",
    "            'run_id': run_id,\n",
    "            'status': status,\n",
    "            'TOK_IN': tok_in,\n",
    "            'TOK_OUT': tok_out,\n",
    "            'API_CALLS': api_calls,\n",
    "            'CACHED_TOKENS': cached,\n",
    "            'COST_USD': cost,\n",
    "            'total_tokens': tok_in + tok_out\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(token_data)\n",
    "    \n",
    "    print(\"üìä Token Metrics Summary\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total runs: {len(df)}\")\n",
    "    \n",
    "    if not df.empty:\n",
    "        print(f\"\\nToken statistics (all runs):\")\n",
    "        print(df[['TOK_IN', 'TOK_OUT', 'total_tokens', 'COST_USD']].describe())\n",
    "        print(f\"\\nRuns with zero input tokens: {len(df[df['TOK_IN'] == 0])}\")\n",
    "        print(f\"Runs with zero output tokens: {len(df[df['TOK_OUT'] == 0])}\")\n",
    "    \n",
    "    if issues:\n",
    "        print(f\"\\n‚ö†Ô∏è  {len(issues)} verified runs with zero tokens!\")\n",
    "        issues_df = pd.DataFrame(issues)\n",
    "        return df, issues_df\n",
    "    \n",
    "    return df, pd.DataFrame()\n",
    "\n",
    "# Analyze tokens\n",
    "token_df, token_issues_df = analyze_token_metrics(runs_data)\n",
    "if not token_df.empty:\n",
    "    display(token_df.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a67705",
   "metadata": {},
   "source": [
    "## 5. Step Integrity Check\n",
    "\n",
    "Validate step-level data (timing, status, counts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4a99f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_step_integrity(runs_data):\n",
    "    \"\"\"Check step-level data for inconsistencies.\"\"\"\n",
    "    step_issues = []\n",
    "    step_summary = []\n",
    "    \n",
    "    for run in runs_data:\n",
    "        framework = run['framework']\n",
    "        run_id = run['run_id']\n",
    "        metrics = run['data']\n",
    "        \n",
    "        steps = metrics.get('steps', [])\n",
    "        \n",
    "        for step in steps:\n",
    "            step_num = step.get('step', -1)\n",
    "            duration = step.get('duration_seconds', 0)\n",
    "            start_ts = step.get('start_timestamp', 0)\n",
    "            end_ts = step.get('end_timestamp', 0)\n",
    "            success = step.get('success', None)\n",
    "            \n",
    "            # Check for missing required fields\n",
    "            if step_num < 0:\n",
    "                step_issues.append({\n",
    "                    'framework': framework,\n",
    "                    'run_id': run_id,\n",
    "                    'step': step_num,\n",
    "                    'issue': 'Missing step number'\n",
    "                })\n",
    "            \n",
    "            # Check for negative or zero duration\n",
    "            if duration <= 0:\n",
    "                step_issues.append({\n",
    "                    'framework': framework,\n",
    "                    'run_id': run_id,\n",
    "                    'step': step_num,\n",
    "                    'issue': f'Invalid duration: {duration}'\n",
    "                })\n",
    "            \n",
    "            # Check timestamp consistency\n",
    "            if start_ts > 0 and end_ts > 0:\n",
    "                computed_duration = end_ts - start_ts\n",
    "                if abs(computed_duration - duration) > 1:  # Allow 1 second tolerance\n",
    "                    step_issues.append({\n",
    "                        'framework': framework,\n",
    "                        'run_id': run_id,\n",
    "                        'step': step_num,\n",
    "                        'issue': f'Duration mismatch: stored={duration}, computed={computed_duration}'\n",
    "                    })\n",
    "            \n",
    "            # Check success field\n",
    "            if success is None:\n",
    "                step_issues.append({\n",
    "                    'framework': framework,\n",
    "                    'run_id': run_id,\n",
    "                    'step': step_num,\n",
    "                    'issue': 'Missing success status'\n",
    "                })\n",
    "            \n",
    "            step_summary.append({\n",
    "                'framework': framework,\n",
    "                'run_id': run_id,\n",
    "                'step': step_num,\n",
    "                'duration': duration,\n",
    "                'success': success,\n",
    "                'hitl_count': step.get('hitl_count', 0),\n",
    "                'retry_count': step.get('retry_count', 0)\n",
    "            })\n",
    "    \n",
    "    summary_df = pd.DataFrame(step_summary)\n",
    "    \n",
    "    print(\"üìä Step Integrity Summary\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total steps analyzed: {len(summary_df)}\")\n",
    "    if not summary_df.empty:\n",
    "        print(f\"Successful steps: {len(summary_df[summary_df['success'] == True])}\")\n",
    "        print(f\"Failed steps: {len(summary_df[summary_df['success'] == False])}\")\n",
    "        print(f\"\\nDuration statistics:\")\n",
    "        print(summary_df['duration'].describe())\n",
    "    \n",
    "    if step_issues:\n",
    "        print(f\"\\n‚ö†Ô∏è  {len(step_issues)} step integrity issues found!\")\n",
    "        issues_df = pd.DataFrame(step_issues)\n",
    "        return summary_df, issues_df\n",
    "    else:\n",
    "        print(\"\\n‚úÖ All steps have valid data\")\n",
    "        return summary_df, pd.DataFrame()\n",
    "\n",
    "# Check step integrity\n",
    "steps_df, step_issues_df = check_step_integrity(runs_data)\n",
    "if not steps_df.empty:\n",
    "    print(f\"\\nSteps per run statistics:\")\n",
    "    print(steps_df.groupby(['framework', 'run_id']).size().describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c56dea6",
   "metadata": {},
   "source": [
    "## 6. Framework Comparison\n",
    "\n",
    "Compare metrics across different frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aceda930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Framework comparison\n",
    "print(\"üìä Framework Comparison\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if not token_df.empty:\n",
    "    # Count runs per framework\n",
    "    print(\"\\nüî¢ Run counts:\")\n",
    "    print(token_df['framework'].value_counts())\n",
    "    \n",
    "    # Average metrics by framework\n",
    "    print(\"\\nüìà Average metrics by framework:\")\n",
    "    framework_summary = token_df.groupby('framework')[['TOK_IN', 'TOK_OUT', 'total_tokens', 'API_CALLS', 'COST_USD']].agg(['mean', 'median', 'std'])\n",
    "    print(framework_summary)\n",
    "    \n",
    "    # Reconciliation status by framework\n",
    "    if not reconciliation_df.empty:\n",
    "        print(\"\\n‚úÖ Reconciliation status by framework:\")\n",
    "        status_by_framework = reconciliation_df.groupby(['framework', 'status']).size().unstack(fill_value=0)\n",
    "        print(status_by_framework)\n",
    "else:\n",
    "    print(\"No token data available for comparison\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9780fb0c",
   "metadata": {},
   "source": [
    "## 7. Export Issues to CSV\n",
    "\n",
    "Save all identified issues for further investigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7461a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export issues to CSV\n",
    "export_count = 0\n",
    "\n",
    "if issues:\n",
    "    issues_df = pd.DataFrame(issues)\n",
    "    issues_df.to_csv('issues_missing_metrics.csv', index=False)\n",
    "    print(f\"‚úì Exported {len(issues_df)} missing metrics issues to issues_missing_metrics.csv\")\n",
    "    export_count += 1\n",
    "\n",
    "if not violations_df.empty:\n",
    "    violations_df.to_csv('issues_data_model_violations.csv', index=False)\n",
    "    print(f\"‚úì Exported {len(violations_df)} data model violations to issues_data_model_violations.csv\")\n",
    "    export_count += 1\n",
    "\n",
    "if not token_issues_df.empty:\n",
    "    token_issues_df.to_csv('issues_zero_tokens_verified.csv', index=False)\n",
    "    print(f\"‚úì Exported {len(token_issues_df)} zero token issues to issues_zero_tokens_verified.csv\")\n",
    "    export_count += 1\n",
    "\n",
    "if not step_issues_df.empty:\n",
    "    step_issues_df.to_csv('issues_step_integrity.csv', index=False)\n",
    "    print(f\"‚úì Exported {len(step_issues_df)} step integrity issues to issues_step_integrity.csv\")\n",
    "    export_count += 1\n",
    "\n",
    "# Export pending reconciliations\n",
    "if not reconciliation_df.empty:\n",
    "    pending_df = reconciliation_df[reconciliation_df['status'] == 'pending']\n",
    "    if not pending_df.empty:\n",
    "        pending_df.to_csv('runs_pending_reconciliation.csv', index=False)\n",
    "        print(f\"‚úì Exported {len(pending_df)} pending reconciliations to runs_pending_reconciliation.csv\")\n",
    "        export_count += 1\n",
    "\n",
    "if export_count > 0:\n",
    "    print(f\"\\n‚úÖ Export complete! {export_count} file(s) created.\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ No issues to export - all data is clean!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81486632",
   "metadata": {},
   "source": [
    "## 8. Visualizations\n",
    "\n",
    "Generate charts for token usage and reconciliation status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61eb651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install visualization libraries if needed\n",
    "%pip install matplotlib seaborn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35792b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Create visualizations directory\n",
    "os.makedirs('visualizations', exist_ok=True)\n",
    "\n",
    "if not token_df.empty and len(token_df) > 0:\n",
    "    # 1. Token distribution by framework\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Total tokens by framework\n",
    "    token_df.groupby('framework')['total_tokens'].mean().plot(kind='bar', ax=axes[0], color='steelblue')\n",
    "    axes[0].set_title('Average Total Tokens by Framework')\n",
    "    axes[0].set_ylabel('Total Tokens')\n",
    "    axes[0].set_xlabel('Framework')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Cost by framework\n",
    "    token_df.groupby('framework')['COST_USD'].mean().plot(kind='bar', ax=axes[1], color='coral')\n",
    "    axes[1].set_title('Average Cost (USD) by Framework')\n",
    "    axes[1].set_ylabel('Cost (USD)')\n",
    "    axes[1].set_xlabel('Framework')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('visualizations/token_analysis.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úì Saved visualization to visualizations/token_analysis.png\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No token data available for visualization\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f95de9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Reconciliation status distribution\n",
    "if not reconciliation_df.empty and len(reconciliation_df) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    status_counts = reconciliation_df['status'].value_counts()\n",
    "    colors = {'verified': 'green', 'pending': 'orange', 'failed': 'red', 'data_not_available': 'gray'}\n",
    "    bar_colors = [colors.get(status, 'blue') for status in status_counts.index]\n",
    "    \n",
    "    status_counts.plot(kind='bar', ax=ax, color=bar_colors)\n",
    "    ax.set_title('Reconciliation Status Distribution', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('Number of Runs')\n",
    "    ax.set_xlabel('Status')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, v in enumerate(status_counts):\n",
    "        ax.text(i, v + 0.1, str(v), ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('visualizations/reconciliation_status.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úì Saved visualization to visualizations/reconciliation_status.png\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No reconciliation data available for visualization\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00337540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Token distribution histogram\n",
    "if not token_df.empty and len(token_df[token_df['TOK_IN'] > 0]) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Input tokens\n",
    "    token_df[token_df['TOK_IN'] > 0]['TOK_IN'].hist(bins=30, ax=axes[0, 0], color='skyblue', edgecolor='black')\n",
    "    axes[0, 0].set_title('Distribution of Input Tokens (excluding zeros)')\n",
    "    axes[0, 0].set_xlabel('Input Tokens')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    \n",
    "    # Output tokens\n",
    "    token_df[token_df['TOK_OUT'] > 0]['TOK_OUT'].hist(bins=30, ax=axes[0, 1], color='lightcoral', edgecolor='black')\n",
    "    axes[0, 1].set_title('Distribution of Output Tokens (excluding zeros)')\n",
    "    axes[0, 1].set_xlabel('Output Tokens')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    \n",
    "    # API calls\n",
    "    token_df[token_df['API_CALLS'] > 0]['API_CALLS'].hist(bins=30, ax=axes[1, 0], color='lightgreen', edgecolor='black')\n",
    "    axes[1, 0].set_title('Distribution of API Calls (excluding zeros)')\n",
    "    axes[1, 0].set_xlabel('API Calls')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    \n",
    "    # Cost\n",
    "    token_df[token_df['COST_USD'] > 0]['COST_USD'].hist(bins=30, ax=axes[1, 1], color='gold', edgecolor='black')\n",
    "    axes[1, 1].set_title('Distribution of Cost in USD (excluding zeros)')\n",
    "    axes[1, 1].set_xlabel('Cost (USD)')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('visualizations/token_distributions.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úì Saved visualization to visualizations/token_distributions.png\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No token data available for distribution histograms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311176a5",
   "metadata": {},
   "source": [
    "## 9. Summary Report\n",
    "\n",
    "Generate a comprehensive summary of the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407f9c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"EXPERIMENT RUN ANALYSIS SUMMARY REPORT\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Runs Directory: {RUNS_DIR}\")\n",
    "print()\n",
    "\n",
    "print(\"üìä DATA QUALITY OVERVIEW\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Total runs analyzed: {len(runs_data)}\")\n",
    "print(f\"Runs with missing metrics: {len([i for i in issues if 'metrics.json not found' in i.get('issue', '')])}\")\n",
    "print(f\"Runs with parse errors: {len([i for i in issues if 'parse error' in i.get('issue', '').lower()])}\")\n",
    "print()\n",
    "\n",
    "print(\"‚úÖ DATA MODEL COMPLIANCE (v2.0.0)\")\n",
    "print(\"-\" * 80)\n",
    "if violations_df.empty:\n",
    "    print(\"‚úì All runs comply with v2.0.0 data model\")\n",
    "    print(\"  - No token fields in steps array\")\n",
    "    print(\"  - Token metrics stored only in aggregate_metrics\")\n",
    "else:\n",
    "    print(f\"‚ùå {len(violations_df)} data model violations found\")\n",
    "    print(f\"  - Runs with violations: {violations_df['run_id'].nunique()}\")\n",
    "    print(f\"  - See: issues_data_model_violations.csv\")\n",
    "print()\n",
    "\n",
    "print(\"üîÑ RECONCILIATION STATUS\")\n",
    "print(\"-\" * 80)\n",
    "if not reconciliation_df.empty:\n",
    "    for status, count in reconciliation_df['status'].value_counts().items():\n",
    "        percentage = (count / len(reconciliation_df)) * 100\n",
    "        print(f\"  {status}: {count} runs ({percentage:.1f}%)\")\n",
    "else:\n",
    "    print(\"  No reconciliation data available\")\n",
    "print()\n",
    "\n",
    "print(\"üí∞ TOKEN & COST METRICS\")\n",
    "print(\"-\" * 80)\n",
    "if not token_df.empty:\n",
    "    verified_runs = token_df[token_df['status'] == 'verified']\n",
    "    if not verified_runs.empty:\n",
    "        print(f\"Verified runs: {len(verified_runs)}\")\n",
    "        print(f\"  Average input tokens: {verified_runs['TOK_IN'].mean():.0f}\")\n",
    "        print(f\"  Average output tokens: {verified_runs['TOK_OUT'].mean():.0f}\")\n",
    "        print(f\"  Average API calls: {verified_runs['API_CALLS'].mean():.1f}\")\n",
    "        print(f\"  Average cost: ${verified_runs['COST_USD'].mean():.4f}\")\n",
    "        print(f\"  Total cost (all verified): ${verified_runs['COST_USD'].sum():.2f}\")\n",
    "    else:\n",
    "        print(\"  No verified runs found\")\n",
    "else:\n",
    "    print(\"  No token data available\")\n",
    "print()\n",
    "\n",
    "if not token_issues_df.empty:\n",
    "    print(f\"‚ö†Ô∏è  WARNING: {len(token_issues_df)} verified runs with zero tokens\")\n",
    "    print(f\"  - See: issues_zero_tokens_verified.csv\")\n",
    "    print()\n",
    "\n",
    "print(\"üìù STEP ANALYSIS\")\n",
    "print(\"-\" * 80)\n",
    "if not steps_df.empty:\n",
    "    print(f\"Total steps: {len(steps_df)}\")\n",
    "    print(f\"Average steps per run: {steps_df.groupby('run_id').size().mean():.1f}\")\n",
    "    print(f\"Successful steps: {len(steps_df[steps_df['success'] == True])}\")\n",
    "    print(f\"Failed steps: {len(steps_df[steps_df['success'] == False])}\")\n",
    "    print(f\"Average step duration: {steps_df['duration'].mean():.2f} seconds\")\n",
    "else:\n",
    "    print(\"  No step data found\")\n",
    "print()\n",
    "\n",
    "if not step_issues_df.empty:\n",
    "    print(f\"‚ö†Ô∏è  {len(step_issues_df)} step integrity issues found\")\n",
    "    print(f\"  - See: issues_step_integrity.csv\")\n",
    "    print()\n",
    "\n",
    "print(\"üéØ FRAMEWORK BREAKDOWN\")\n",
    "print(\"-\" * 80)\n",
    "if not token_df.empty:\n",
    "    for framework in token_df['framework'].unique():\n",
    "        fw_runs = token_df[token_df['framework'] == framework]\n",
    "        fw_verified = fw_runs[fw_runs['status'] == 'verified']\n",
    "        print(f\"{framework}:\")\n",
    "        print(f\"  Total runs: {len(fw_runs)}\")\n",
    "        print(f\"  Verified: {len(fw_verified)}\")\n",
    "        if not fw_verified.empty:\n",
    "            print(f\"  Avg tokens: {fw_verified['total_tokens'].mean():.0f}\")\n",
    "            print(f\"  Avg cost: ${fw_verified['COST_USD'].mean():.4f}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"  No framework data available\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"‚úÖ Analysis complete! Check the 'visualizations/' directory for charts.\")\n",
    "print(\"=\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
